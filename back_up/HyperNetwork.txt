* HyperNetwork
** 概要
HyperNetworkとは、Stable Diffusionのファインチューニング方式の一つ。NovelAIリークが基になっているらしい。
学習が遅く設定も難しいし大した成果が無いためか、現在は5ch本スレで話題を全く見かけないほどに廃れた。本当にこの界隈の進化は速い。
強いて言えば画風調整に有用。


2023年1月21日現在、
学習やなくて使用する方やけど、アップデートでhypernetworkを標準UIからの指定が変わったが、現状正常に動作していない模様。
治るまでの間はMultiple Hypernetworks Extensionで指定してやってや
https://github.com/antis0007/sd-webui-multiple-hypernetworks

2022年12月14日現在、hypernetworkは以下のExtensionを使用するのがオススメ。
https://github.com/aria1th/Hypernetwork-MonkeyPatch-Extension

このExtensionはWEB UI(AUTOMATIC1111)に含まれているものに機能追加やバグの修正を施したもの。

とりあえず変更点を書きます。

** バグ修正
このExtensionを使う事により以下のバグを回避することが可能
+ 学習中にHypernetwork strengthを1未満にした場合に悪影響がある現象の修正 (学習時は常に1になるようになる)
+ 学習中にプレビューを生成した際に学習に悪影響がある現象の修正

** 拡張機能
- 複数のhypernetworkをマージし、それを使い画像を生成する機能の追加
説明は[[ここ>https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/4334]]。
ただし、モデルのマージと違い｢画像を生成する時に指定したhypernetworkをロードしながら処理を行う｣ために画像1枚生成する時間がもの凄く長くなりあまり実用的ではない。
- 学習画像のサイズの変更やクロップをしなくても学習出来る機能
内部的には長辺のサイズを学習の設定で指定したサイズにし学習を行う。%%''注意点としてはbatch sizeは1にすること''%%。 (2023-01-12現在、この制限はなくなりました)
- Optimizerをアンロードしてメモリ不足エラーが発生する可能性を減少。 (VRAM8GBでも学習可能になるっぽい)

** Beta hypernetwork
*** 可変Dropout rateの追加
純正では2番目の層にしかdropoutが指定されていなかったため、深い階層のhypernetworkを作成すると3番目以降の層に適応されない問題があった。
また、dropoutの値が0.3に固定されており、これが大きすぎるのではないかという話があった。
可変Dropout rateはこれらの問題を解決するもの。
｢Use dropout. Might improve training when dataset is small / limited.｣のチェックをOnにし、その下のtext入力欄にdropoutの構造を記載する。
これはhypernetwork layer structureの構造と一致していなければならない。
例えばlayer structureを ｢1, 2, 2, 1｣で作成した場合、可変dropoutは｢0, 0.3, 0.3, 0｣のようにする。
推奨値は0〜0.35で最初は0固定、前の方の層ではあまり大きい数字を使わないようにすると安定しやすい。

*** Show advanced optionsの中にWeight initialization seedの固定、レイヤーウェイトの初期化にNormalを選んだ際の初期ウェイトを0.01以外に指定出来る機能の追加
通常は触る必要はなし。
hypernetworkは学習する際にランダムなシードが使われることで全く同じ設定・学習素材を使っても同じ結果にならないがそれを固定化する機能。

 なお、この機能で作られたhypernetworkファイルはextension無しでも動作するはずだが上記の機能は適応されてないものとして扱われる(はず。未検証。)

** Train Gamma
*** Gradient accumulation
割と最近、本体のHypernetworkにも搭載された機能。
簡単にいうと｢処理速度や品質への影響を抑えつつ省メモリで動かす｣仕組み。
4〜8あたりが推奨値。
この値を変更すると1stepで処理する数が変わるため、学習stepの指定が変わることに注意。
 1stepで処理する画像数 = batch size × gradient accumulation。
 学習素材が64枚、batch size = 1、 gradient accmulation = 8なら 1stepで8枚の処理が行われ1epochは8stepとなる(端数切り捨て)

*** Cosine annealing learning rate scheduler
指定したStepの間でlearning rateを上下させる機能。Hypernetwork Learning rateで指定した値を最大値、Minimum learning rate for beta schedulerで指定した値を最小値として上下するようになる。
- Step for cycle: 1cycleの長さをstepで指定する。目安としては10epochくらい。(学習素材数 ÷  batch size ÷ Gradient accumulation × 10epoch を切り捨てで入力)
- Step multiplier per cycle: 1のままでOK
- Warmup step per cycle: Step for cycleの値に合わせ上下させる。どの位が良いのかは調査中。デフォルト値のままでとりあえず問題無い。
- Decays learning rate every cycle: cycleが進むごとに最大値を減らすようにする。1なら毎回最大値になる。1のままでも問題ないが、学習ステップ数を多くする予定なら0.998など少し減らすと良い。
- Saves when every cycle finisher, Generates image when every cycle finishes: それぞれ、学習率が最小値に達した時にその時点のハイパーネットワークを保存する、プレビュー画像を保存する設定。

* HNの適用方法
生成したptをフォルダ/Models/hypernetworksに置く。SettingsのHypernetwork欄で選択。
Hypernetwork strengthで影響の強弱を変えられる。過学習してしまったptでも数値を低くすれば汎用性が戻るかも。

* HN+DB
画風を学習させたHNとキャラを学習させたDBなどで版権再現度を高めるという手法。
あまり再現度が高くないキャラDBでもキャラHNと組み合わせると丁度良くなったりする。

* 学習について
ここでは、Hypernetworkによる学習を始めてみたい、という人向けに目安となるよう簡単に説明します。
キャラクターや再現度を高める方法は様々あり、設定パラメータも多いため｢この通りにやればまずはそれっぽく出来る｣ということをゴールにします。
** 使用Extension
- https://github.com/aria1th/Hypernetwork-MonkeyPatch-Extension
最初に書いた通り
- https://github.com/toriato/stable-diffusion-webui-wd14-tagger
wd14のモデル作成のために作られたtagger。deepdanbooruを使用するよりも高い精度でタグ付けが出来るため楽をしたいなら必須。
- https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor
学習素材を見ながらタグ一覧の確認が出来る。導入必須ではないがあると便利。
** 注意すべき設定
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/k6cFiaMt4t.38.14.png)
まず、WEB UI本体の設定の学習にある｢Saves Optimizer state as...｣にチェックを入れ設定を適応してください。
この設定を入れない場合、学習を中断し再開した場合にHypernetworkが破壊される可能性があります。(問題無く再開出来る事もあります)
これはhypernetworkのファイル内にoptimizerの状態を保存していないから発生する問題です。
Saves Optimizer...の設定を有効にすることで .optim ファイルにOptimizerの状態を保存し、安全に再開できるようになります。
 なぜ設定として別れているかというと、Optimizerを保存したファイルの容量がhypernetwork本体の容量の2倍であり、ストレージの圧迫の原因になりうるからです。
 なお.optimファイルは学習再開時やさらに追加で学習を行う場合にのみ使われるものなので、hypernetworkが完成したなら削除してもかまいません。

** 学習素材の準備
学習素材を用意します。
キャラクターなど新しい概念を学習する場合も画風を学習する場合も出来るだけ絵の傾向が似ているものを選びます。
素材の枚数は筆者は24〜80枚程度で行う事が多いです(筆者は画風の学習をメインに行っています)
MonkeyPatchを使う限り、トリミングや画像の縮小で1:1の画像にする必要がありません。
ただし、画像の端の方にサインが入っているなど簡単な編集で学習の邪魔になるものを排除できるなら軽くトリミングくらいはしたほうが成功確率が上がります。
(イラストのメインとなる部分にサインが入っているような場合も自分で修正したほうが良いかもしれませんが、塗りむらが悪影響を出すこともありその手間と見合っているかというと難しいと考えています。
まずはHNを使う一歩として、ここでは無編集のまま進むことを前提とします)

学習素材は適当な1フォルダにまとめて保存してください。 /path/to/images/ に保存している、と仮定して話を進めます。

** 学習するイラストのタグファイルの作成
学習する絵にタグをつけます。
タグ付けにはwd14 taggerを使用します。
該当のExtensionを正しく導入出来ていれば｢Tagger｣タブが増えているはずです。
Taggerタブの下の｢Batch from directory｣タブから指定したフォルダ内の画像全てにタグを付けることが可能なので、これを利用します。
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/4RvZK9XFue.03.02.png)
まず、上部ですが入力フォルダ、出力フォルダに学習素材が入っているフォルダを指定してください。

&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xBwsd63eL6.03.30.png)
続いて下段ですが、Interrogatorに wd14-convnext を指定します。この選択肢が無い場合はExtensionが古い可能性があるので更新しましょう。
以前のもの (現在は wd14-vit という名前になっている) より高い精度でタグ付けをおこなってくれます。

しきい値は0.35のままで大丈夫です。

除外するタグですが、自分は breasts, blurry を常に除外するようにしています。
danbooruを見ればわかりますが、小さい胸の場合は｢breasts, small breasts｣、大きな胸の場合は｢breasts, large breasts｣と胸があったらとりあえずbreastsと入っていることが多く、学習によってbreastsの意味が変動してしまうことがあるためです。
blurryも同様で前ボケ、後ろボケの場合それぞれ｢blurry foreground｣｢blurry background｣となっているため、単体のblurryは削除するようにしています。

その下のチェックボックスは｢アンダースコアの代わりにスペースを使用する｣｢括弧をエスケープする｣にチェックをいれます。
準備ができたら Interrogate ボタンを押してください。
しばらく待つと学習画像と同じ名前のテキストファイルが指定したフォルダにできあがっているはずです。

** タグの確認
確認して手で直した方がいいのですが、とりあえず後日かきます。
とりあえず気にせずそのまま次の項目へ。

** textual_inversion_templates の作成
textual_inversion_templates/ フォルダの下にテンプレートファイルを作成します。
以下のテキストファイルを作ってください。

- キャラクターを学習する場合
 character is ''キャラクターの名前'', [filewords]
- 画風の学習をする場合
 artist is ''画風の名前'', illustration drawn by style of ''画風の名前'', [filewords]
このとき、作成するhypernetworkの名前がキャラクターの名前や画風の名前と一致しているなら [name] とすることも出来ます。
一致しない場合はそれぞれ太字になっている箇所を編集し、わかりやすい名前で textual_inversion_templates/ フォルダの下に保存してください。

** Hypernetworkの作成
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/MqGXLpxh3H.44.39.png)
Create Beta hypernetworkタブからhypernetworkを作成します。
- 名称
hypernetworkの名前です。わかりやすい名前を付けましょう。
- モジュール
全て選択済みのままにします
- Hypernetworkのレイヤー構造を入力
一般的に｢キャラクターを学習する場合は1,4,1のような広いネットワーク｣、｢画風を学習する場合は1,2,2,1のような深いネットワーク｣を作成すると効果的と言われています。
1,2,4,2,1のような｢広くて深いネットワーク｣は効果的ではなく学習時間や容量が大きくなるのでオススメしません。
またデフォルトの値である1,2,1でも十分効果的なハイパーネットワークを作ることは可能なため、VRAMに余裕がない人は1,2,1のままでかまいません。
- 活性化関数
筆者はmishを選択しています。mishは優秀ですが学習時間が伸びる傾向にあるようです。SwishやLinearでも成功事例はあるので、分かってきたら色々試して見るのもいいでしょう。
- レイヤーウェイトの初期化
KaimingNormalを指定します。
ここでの注意として、KaimingNormalを指定した場合その下の｢レイヤーの正規化(Layer Normalization)を追加｣にチェックをいれないと学習結果がもの凄い勢いで壊れます。
- Show advanced option
とりあえず無視でOK
- レイヤーの正規化を追加
チェックを入れてください。レイヤーウェイトの初期化にNormalやXaierを使う場合は使用しなくてかまいません。
また、レイヤーの正規化を使用する場合、学習率を高くしないと学習が進まなくなります。
- Use dropout.
使用する場合、レイヤー構造にあわせてdrop outする割合を指定します。dropoutを使うことで過学習を防ぐ効果があるようです。
前の方のレイヤーでは小さめの値を、後ろのほうの値はやや大きめの数値を設定するといいようです。
WEB UIでのデフォルト値は0.3のため、ここでは0.3を指定しています。

以上の設定ができたら｢Hypernetworkを作成｣ボタンを押し、ファイルを作成します。

** 学習を行う
*** サンプル画像出力用のプロンプトの設定
まず、txt2imgタブでサンプル画像を出力するための設定をします。
- 学習素材フォルダにあるテキストから適当なものを選び、そのままPromptに入力
- Negative Promptには普段使っているものを入力
- Seedを固定
- サンプリング回数やサンプリングアルゴリズム、サイズなどは特に指定はないが、画像の出力に時間がかかる設定をしてしまうとトータルの学習時間が延びるので控えめに

*** 学習の開始
Train_Gammaタブで設定を行い、学習を開始します。
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/YyjLFLJHWf.45.23.png)
注意するのは赤枠で囲った部分の設定です。
その他の部分は画像通りに設定してください。
Gradient蓄積ステップは4または8を設定します。この設定は学習がやや遅くなる欠点の代わりにVRAMの消費を抑える効果があります。
Steps for cycleの値は｢学習素材数 ÷ Gradient蓄積ステップ｣した値を10倍にした数値を入力します。端数は切り捨てです。

&ref(https://image01.seesaawiki.jp/n/h/nai_ch/sN0UbW_CQ9.00.02.png)

残りの設定です。
データセットフォルダ、プロンプトのテンプレートファイルにはそれぞれ学習素材を入れているフォルダ、textual_inversion_templatesの項目で作ったテンプレートを指定します。
幅と高さは学習時の画像のサイズです。
MonkeyPatchを使用する場合、学習画像のサイズとここで指定したサイズを合わせる必要はありませんが、学習自体はここで設定した値を基準に行われます。
サイズを大きくすれば細かい部分まで学習できますが使用VRAMが大きくなり、学習時間も延びます。
必ず幅と高さの値は同じにします。

最大ステップ数にはSteps for cycleで指定した値の40〜50倍の値を、指定したステップ数ごとにEmbeddingのコピーをログに保存にはSteps for cycleで指定した値の10倍を入力します。
Embeddingのコピーを保存の設定は指定したステップ数ごとにログファイルにその時のhypernetworkを保存する設定です。ストレージに余裕があるのならもっと細かく保存してもいいでしょう。
残りの設定は画像通りにしてください。

準備ができたら｢Hypernetworkの学習を開始｣ボタンを押します。

** この後は？
ひとまずこれで学習が終わるのを待てばHypernetworkが作成されるはずです。
この設定である程度の成果は得られますが、より適した学習素材の準備やタグの調整、学習率の調整や学習ステップ数など調整する項目は多く改善の余地は多くあると考えています。
より多くの成功事例・失敗事例が集まるきっかけになれば幸いです。

** メモ
=|BOX|
801 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ f31e-ZR1D)[sage] 投稿日：2022/12/17(土) 17:08:25.41 ID:03SdMsEd0 [7/8]
もっとHypernetworkの有用性理解する人が増えてほしいからＨＮ生成時の個人的なメモ置いとく。個人的な奴だから間違ってるかもしれん

既存の『stable-diffusion-webui』フォルダにこのMOD入れる。すると学習タブでCreate Beta hypernetworkとTrain_Gammaが増える
https://github.com/aria1th/Hypernetwork-MonkeyPatch-Extension

Create Beta hypernetworkタブ開く→名称は適当（日本語も行ける）、レイヤー構造は1, 1、Hypernetworkの活性化関数はrelu→Hypernetworkを作成
画像の前処理タブ開く→入力フォルダに学習したい画像を入れる※１→deepbooruで説明をつけるにチェック→前処理開始
Train_Gammaタブ開く→名称決めたHypernetwork選択→学習率は0.00004（デフォ）、最大ステップは2000で学習開始→終わったら『0.000004』(一桁減らす)で4000まで→終わったらさらに一桁減らして『0.0000004』で6000まで→これで大体できてる
2000の段階で一度画像をt2iで書き出してみて足りなければ3000〜4000まで追加学習してから次へ

※１　絵柄を学習したいなら顔だけ切り抜きでいい。（重要　体は邪魔）
体位なら体全部入れる。背景処理がいると見せかけて実はあんまりいらない？参考画像は40〜50枚は欲しいかもしれない。少なくても行けるかもしれないけどやってない。
MODのおかげで画像を四角形に切り取る必要はないが、デフォ設定だと強制的に真四角に切り出されるので横長画像、四角画像、縦長画像をフォルダ分けして3回720x512、512x512、512x720とかで書き出せばめんどくさくない。|
||=
学習率の指定は纏めて
最大ステップ　6000
学習率　4e-05:2000,4e-06:4000,4e-07:6000
でいいかも
=|BOX|
845 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ｱｳｱｳｳｰ Sa9f-fzMu)[] 投稿日：2022/12/17(土) 19:38:59.30 ID:6aA74DbMa
HNの前処理で512×512、512×768、768×512を分けて3フォルダ作ったけどこれは1つのデータセットフォルダにまとめて良えんかな

あとtrain_ganmaのwidhとhightは512×512のままで良えの？

質問ばっかですまん


849 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ｻｻｸｯﾃﾛﾗ Spb3-zkD9)[sage] 投稿日：2022/12/17(土) 19:42:19.24 ID:jvT5L/5rp
> >845
まとめていける

あと後者は512のままでいい
勝手に隙間埋めてくれるらしいから
||=
=|BOX|
874 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ｻｻｸｯﾃﾛﾗ Spb3-zkD9)[sage] 投稿日：2022/12/17(土) 20:18:41.08 ID:RAi7H65Kp [5/5]
顔の切り抜き方はハッキリとや
汎用性持たせようといろんなキャラとか混ぜると碌なことがないから止めるんや
あと絵柄学習も50枚くらいはあった方がええかもしれん
||=
=|BOX|
999 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ f31e-ZR1D)[sage] 投稿日：2022/12/18(日) 00:32:38.61 ID:hdK6/sqT0 [4/4]
�仝飢菫�を見つけて切り抜く→�∩綾萢�する→�Ａ綾萢�済み画像→��betaHypernetworkを作る→
��Train_Gammaで学習開始→�ε喘羞于瓩妊蹈哀侫�ルダにそれっぽいのが出来る→
�Т粟�したら設定でhypernetwork読み込んでガチャ
||=
[+]
��&ref(https://image02.seesaawiki.jp/n/h/nai_ch/BootHGQsMl.png,420,350){前処理する}
��&ref(https://image01.seesaawiki.jp/n/h/nai_ch/B8hq4RnCeB.png,420,350){betaHypernetworkを作る}
��&ref(https://image01.seesaawiki.jp/n/h/nai_ch/b_haoBdJ4v.png,420,750){Train_Gammaで学習開始}
[END]
=|BOX|
429 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ f31e-ZR1D)[sage] 投稿日：2022/12/18(日) 21:28:52.55 ID:hdK6/sqT0 [14/15]
前スレ801や
今のところ研究したHyperNetworkの絵柄学習についてをまとめる
・レイヤー構造について
【1, 1 】とにかく軽い。絵柄を寄せるならこれで普通に行ける。ただしガチャは必要。
【1, 2, 1】精度がより高い。検証中だがおすすめかも？
【1, 2, 2, 1】重い。小物とか髪型とか細かい要素を学習するっぽいけどMishの回す量が足りなかったり、そもそもLeruだと肝心の絵柄が似ない。繊細な絵なら合うかも

・活性化関数について
【Leru】今のところ無難な感じ。
【Mish】絵柄を似せるのはシビアな調整がいるが回してれば雰囲気は似る。これで1,2,2,1と合わせると結果的に一番似る。ただしなんかi2iとの相性がメチャクチャ悪い。
||=


