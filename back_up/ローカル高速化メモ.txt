#contents

* 高速化オプションの利用

stable diffusionに用意されているxformersというオプションを利用する。
理論上、このオプションを付けてるときと付けてないときで絵が変わるらしいが
少なくともアスカプロンプトでは明確な違いを確認できず、演算時間が2割程度早くなる。
(明確な違いが出る条件がわかる方がいたらここに新章作って記録していってほしい)

検証ページ作ったので記録はこちらへ
→「[[xformersの検証]]」

** 対応GPU

- NVIDIA
- GeForce 1000番台〜3000番台(4000番台は後述)
- GeForce 900番台以前でも動くけど逆効果だったという報告あり

** 方法

stable-diffusion-webuiディレクトリ直下にあるwebui-user.batを修正する。

6行目修正前
> set COMMANDLINE_ARGS=

6行目修正後
> set COMMANDLINE_ARGS= --xformers

** 利用するライブラリの更新
2023/01/23時点の最新版(7ff1ef77dd22f7b38612f91b389237a5dbef2474)で本体に取り込まれた
旧バージョンのtorchとxformersを使用している場合はアップグレードを促すアナウンスメッセージが表示される
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/1sa2Wev7W4.png)

コマンドラインオプションに再インストールオプションを追加して起動すると更新される
> COMMANDLINE_ARGS= --reinstall-torch --reinstall-xformers ~~

アップデート完了した場合、WebUIの下部の表示が以下のようになる
(torch・xformers欄)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xh_WFZyBHE.png)

***RTX4000シリーズを使っている場合
標準でインストールされるcudnn関係のDLLが古い(4000シリーズ未対応)為、対応版(cudnn8.7.0)に置き換える

+https://developer.nvidia.com/rdp/cudnn-download より Local Installer for Windows (Zip) をダウンロードする~~※メールアドレス登録が必要かもしれない
+ダウンロードしたzipファイル(cudnn-windows-x86_64-8.7.0.84_cuda11-archive.zip)を展開し、binフォルダ内の7ファイルを取り出す
+stable-diffusion-main\venv\Lib\site-packages\torch\lib 内に上書きコピーする


[+]以前の方法
4090で高速化する方法。3060でもほんのちょっとだけ早くなったとの報告あり。

- 更新手順(Windows用)
1.エクスプローラーでstable-diffusion-webui/venv/Scriptsフォルダを開き、右クリック→ターミナルで開く(多分Win10ではPowershellで開く)でPowershellを開く
2.以下のコマンドを実行
> .\Activate.ps1
3.以下のコマンドを実行
> pip install -U -I --no-deps torch==1.12.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
4.https://pomf2.lain.la/f/5u34v576.7zをDLして中身を「stable-diffusion-webui\venv\Lib\site-packages\torch\lib」にコピーする
5.以下のコマンドを実行
> pip install -U -I --no-deps torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
5.Powershellウィンドウは閉じて、webuiフォルダ直下のwebui-user.batを開き、set COMMANDLINE_ARGS=の後ろに--xformers --opt-channelslastを追加して保存する

ソース:なんJNVA部★84 
http://fate.5ch.net/test/read.cgi/liveuranus/1667387353/
[+]
> 265 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ fb72-iO6U)[sage] 投稿日：2022/11/02(水) 23:51:01.19 ID:n0RTOjsJ0 [5/6] ~~
> >>239>>240 ~~
> 俺も2週間ぐらい同じ感じだったけどxformersオプション入れるだけじゃ多分駄目なんよ ~~
> これから4090買う人のために書いておく こっちはドライバは最新 Win10 CPUは5900X MEM32G ~~
> webuiのvenv内に入ってから(stable-diffusion-webui\venv\Scripts\activateで入れる) ~~
> pip install -U -I --no-deps torch==1.12.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 ~~
> https://pomf2.lain.la/f/5u34v576.7z ~~
> ↑のファイルをstable-diffusion-webui\venv\Lib\site-packages\torch\lib ~~
> にコピペ ~~
> pip install -U -I --no-deps torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 ~~
>  ~~
> 266 名前：今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ fb72-iO6U)[sage] 投稿日：2022/11/02(水) 23:51:28.93 ID:n0RTOjsJ0 [6/6] ~~
> (続き) ~~
> 起動オプションに追加 ~~
> --xformers --opt-channelslast ~~
> これで18it/sぐらい出るようになった ~~
> Windows ディスプレイ設定→グラフィックの設定→GPUアクセラレータによるGPUスケジューリングをOFFで再起動する ~~
> これで24it/s出るようになった うまくいったら報告頼む 環境壊れたらすまんvenv消してくれ ~~

[END]
* torchのバージョンとcudaのdllを入れ替える
4090が主だけど、3090でも早くなった報告あり。
自分の環境では2倍速近く早くなった。
https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/2449#issuecomment-1320800317
[END]

* torchを2.0にする(非推奨)
生成が5%以上速くなるが、''いくつかの機能が動作しなくなる可能性があるため非推奨''
Linuxであればさらに高速化するが、モデル、解像度、Batch sizeの変更の度にコンパイルでかなり待たされるのでやる価値は低い。
(txt2img,img2img,Extra network, ControlNetは動作確認済)
↓に更新方法が書いてある。
https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/6932
※xformersはvenvでのビルドはエラーで動作しないかも

** 補足
- パッケージ名について
Installセクションステップ3のpip3はtritonのパッケージ名が変わっているため失敗する。代わりに以下のコマンドを実行する。
> pip3 install --pre torch torchvision torchaudio pytorch-triton --extra-index-url https://download.pytorch.org/whl/nightly/cu118 --force


- Windowsの補足事項
WindowsではIntallセクションのステップ5がうまくいかない。ステップ4まで進めたら、gitでxformersをクローンしてビルドする必要あり。
[[ここの手順>>https://michyo.net/sempetit/install-xformers-windows/]]の「xFormers のダウンロード」まで進めた後、
xformersフォルダ内で以下のコマンドを実行する。
> python setup.py build
ビルドされるのでしばらく待つ。その後xformersフォルダ内で以下のコマンドを実行する。
> python setup.py bdist_wheel
その後xfomers\dist\にwhlファイルができるので、それをvenv/Scriptsにコピーしてvenv側のpythonでpip install ファイル名.whlでインストールする。

- Linuxの補足事項
torch.compile()を利用するとdynamoのエラーで生成ができないため、stable-diffusion-webui\repositories\stable-diffusion-stability-ai\ldm\modules\diffusionmodules\util.pyの113-114行目を次のコードで置き換える。
> return CheckpointFunction.apply(func, len(inputs), *inputs, *params)

※Windowsは非対応なのでtorch.compile()があっても速くならない

* torch2.0+cu118+SDP(要検証)
https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/8691
torchが正式に2.0となったため使用できるようになった技術
上記の「torchを2.0にする」と「利用するライブラリの更新」を同時に適用し、
更にSDPというxformersと似た技術を適用することで高速化を図る
4000番台、3000番台にて効果を確認したが、1000番台ではxformers適用の方が速いらしい?
(txt2img,hires.fixで動作確認済)

1.venvフォルダ削除
2.git pullしてwebuiを最新版にする
3.webui-user.batを以下に書き換えて実行。--xformersはつけないこと!
=|BOX|
@echo off

set PYTHON=
set GIT=
set COMMANDLINE_ARGS=--opt-sdp-no-mem-attention --opt-channelslast
set TORCH_COMMAND=pip install torch==2.0.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu118

call webui.bat
||=
5.起動したらウェブブラウザでhttp://127.0.0.1:7860にアクセス。一番下に"torch: 2.0.0+cu118"と記載があれば成功

- SDPについて
2023年3月上旬にcommitされたらしい
xformersと特徴は同じで、--opt-sdp-no-mem-attentionをつけるとほんのちょっと絵が変化する代わりに2割程度速くなる
xformersと併用、共存はできない

* VAEをckptに内蔵する

1111に標準で入っているCheckpoint Mergerでもできるようになっている
マージ対象Aに内蔵先モデルを指定して、Interpolation Method を No interpolation 指定
Bake in VAEで内蔵するVAEを選択してマージする

[+]以前の方法
116スレ559

Checkpoint Mergerでマージすると色が薄くなる問題、Merge Blocvk Weightedで解決できそう

Checkpoint Merger のMultiplierで指定してた値をMerge Block Weightedの IN00〜OUT11まで全部同じ値にして、base_alphaとM00を0にする(=ModelA側に寄せる)と発色がよくなる 
[+]
https://i.imgur.com/80TODwE.png
[END]

出てくる絵は微妙に変わるけど発色よくなってVAEが必要なくなっただけでも満足や

↓のX/Yは上から
- Checkpoint Mergerでマージしたもの
- Merge Blocvk Weighted で全部↑のMultiplierと同じ値を設定 0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3
- Merge Blocvk Weighted でbase_alphaとM00に0を設定 0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3
[+]
https://i.imgur.com/7chOcgA.jpg
https://i.imgur.com/Q6p1jJI.jpg
[END]
[END]

* safetensors変換

1111に標準で入っているCheckpoint Mergerでもできるようになっている
マージ対象Aに変換したいモデルを指定して、Interpolation Method を No interpolation 指定
Checkpoint formatをsafetensorにしてマージする

[+]以前の方法
116スレ612

>>559法とsafetensors変換を併用したらハローアスカ5秒短縮できたやで！

1-1. 混ぜたいvaeを有効にしておく
1-2. Merge Block Weightedを開く
1-3. Model AとModel Bに同じckptを指定する
1-4. Output Model Nameも何か入れておくと便利
1-5. Run Merge

2. できたckptをsafetensorsに変換(方法は下記)

3-1. できたsafetensorsを選ぶ
3-2. vaeをNoneにする

ハローアスカ
ckpt+vae 42.89
safetensors 37.87
多用するモデルで作っとけば普段使いもいけるでー 
[END]

** 変換方法いろいろ

- Pythonを使う https://pastebin.com/ijApuS6b
- Colabを使う(今回はまわりくどいけど) [[ローカルの「ツール」]]
* Extensionsの退避

しばらく使わないときは Dreambooth Extension を削除する

起動時間が劇的に変わる
UIからオフにするだけではダメ
フォルダごと別の場所に退避させとくとか、いっそバッサリ消すとか

* Stepsを減らす

出来るだけ少ないStepsでも期待通りの絵が出る方法を模索する。
Euler a以外(収束するサンプラー)で再現できないか試す。
DPM++ 2M Karrasは15Stepsでもそれなりに映えるのでおすすめ。

* RamDiskを使う
1111 全部 RamDisk に突っ込んで高速化するのは、必要RAMのわりに恩恵がほとんど無いからおすすめしない。

以前はモデルをマージする際に RamDisk が有用であるとされてきたが、RamDisk にあずかる恩恵はさらに少なくなっているかもしれない。
2023年2月現在はモデルをメモリ上でマージしてくれる SuperMerger が公式で使えるようになっているので。

詳しくは [[ローカルのExtensions]] の SuperMerger を参照。

もし何らかの形で RamDisk を使うのであれば ImDisk Toolkit が使いやすい。
https://sourceforge.net/projects/imdisk-toolkit/

stable-diffusion-webui\models\Stable-diffusionにジャンクションをはるか、
コマンドラインオプションで--ckpt-dirを指定して使う(こっちのが便利かも)
*チェックポイントをRAMに保持する
Settings->Stable Diffusion->Checkpoints to cache in RAM
の値を変更すると、最後に読み込まれたチェックポイントをRAMに保持します。

ちなみに、ほとんどのOS(Android等も)は一度読み込まれたファイルをRAMに保持する
キャッシュ/スタンバイ領域というものがあり、
このキャッシュ領域はメモリの空き容量に応じて可変します。
(OSコアやプログラムで使用中のメモリ-総容量=キャッシュ/スタンバイメモリの最大値)
例えば、メモリが32GBで10GB使用中の場合のキャッシュメモリは20GBですが、
メモリにキャッシュされていないファイルを読み込み、総容量が20GBを超えると、
メモリにキャッシュされたファイルのうち一番古いファイルがキャッシュメモリから破棄されるので、
再度HDD/SSD等から読み込む作業が発生します。

* WSL2で使う

[[1111_WSL2]]

* Linuxに入れる

仮想化ではなく生で挿入れる
Windowsと比べて多少高速化するらしい・・・？
虎の子のゲーミングPCでは厳しいやね
サブ機とかお古になったら検討してもいいかも
なおインストールが簡単かは謎

* venvを作り直す

Windows再インストール並みの最終手段。
最新の1111に必要なものしか入らなくなるので軽くなる、かも。
Extension試しまくってると肥大化するので月イチくらいでやるといいかも。

* GPUを買い替える

{{{give me money}}}

















