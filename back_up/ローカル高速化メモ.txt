#contents

* 収束の高速化
いわゆる「蒸留」で少ないステップ数で収束させ、Guidance Scale/コンディショニングの計算を不要にすることで高速化する。
ただし品質が低下する。

*** LCM(Latent Consistency Model)
https://huggingface.co/latent-consistency/lcm-lora-sdxl

*** Hyper-SD
https://huggingface.co/ByteDance/Hyper-SD

*** DMD2(Distribution Matching Distillation)
https://huggingface.co/tianweiy/DMD2

* torch2
現在、torch2がデフォルトになりました。
WebUIの最適化が進んで速くなっている上Hyper-SDなどの改造不要の最適化やパラメータの調整でさらに速くできるのでtorch.compile改造はおすすめしません。

[+] 古い情報
torch2化、torch.compile改造ともにVRAMの消費量が増えます。
高解像度生成などでVRAMがカツカツな人は取捨選択が必要かも。
何倍も速くなるわけではない。xformersよりは良いかなってくらい。

torch.compile()でさらに高速化するが、以下の欠点があるのでお勧めしない。
- コンパイルそのものに時間がかかる
- モデル、解像度、Batch size変更の度にコンパイルされる ~~ &size(12){キャッシュされるので毎回コンパイルされるわけではない}
- WebUIにとって想定外の処理のため不安定。
- Linuxのみ対応
[END]

* CUDA使用率
画像生成速度には、CUDA使用率が大きく影響する。
アスカベンチで同じグラボの他人の結果より明らかに遅い場合、なんらかの原因でCUDAがフルに使われていない可能性もある。
（これを確認する場合、Win11なら、GPUスケジューリングをオフにしてタスクマネージャーで項目をクリックしてCudaに切り替えるのが簡単）
このページにある対策を行ってもアスカベンチで期待したパフォーマンスが出ない、という場合でも、高解像度やHires fixした場合は512x512単体生成よりもCUDAがフルに使われやすい傾向があるので、
もしそっちでCUDA使用率が100%近くなってて、そういう生成方法がメインだという場合は、アスカベンチで劣っていても気にしなくていい可能性もある。

ちなみに、速度が速いほどCPUのシングル性能の影響が大きくなる。旧世代CPUを使っているなら最新世代のCPUに買い替えるのもあり。

* Cross-attentionの最適化による高速化
** xformers
Meta Researchが開発した最適化ライブラリ。高速化とVRAM消費減少効果がある。
インストールは1111の標準機能で簡単にできる。
このオプションをつけるとわずかに出力内容が変化し、常に同じ結果にならない。
とはいえ体感できるか難しいほど小さい変化のため気にする必要はない。

変化の検証ページ作ったので記録はこちらへ
→「[[xformersの検証]]」

*** 対応GPU
- NVIDIA
-- GeForce GTX 1000(Pascal)、RTX 2000(Turing)シリーズ以降
-- GeForce 900番台以前でも動くけど逆効果だったという報告あり

- AMD(Linux/WSL2限定)
-- ついにAMDグラボも対応した。Torch 2.4.1+ROCm6.1対応のv0.0.28がリリースされた。
-- RDNA2以降のグラボが必要？

*** 導入方法
初めて導入する場合(NVIDIA)
 set COMMANDLINE_ARGS= --xformers

古い環境から更新する場合(NVIDIA)
 set COMMANDLINE_ARGS= --reinstall-torch --reinstall-xformers
venvを削除してからWebUIを起動する手もあり

Torchバージョンが1111のデフォルトではない場合/Radeonの場合
[[ここ>>https://github.com/facebookresearch/xformers/releases]]で対応するTorchバージョンを確認してから、「pip install xformers==x.x.x」でインストールする。
** SDP(Scaled dot product attention)
2023年3月上旬にcommitされたらしい
xformersと特徴は同じ。わずかに絵が変化する代わりに2割程度速くなる。
有効にするには、Settings->Optimizations->Cross attention optimizationでsdpかsdp-no-memを選ぶ。
sdpのほうは出力の揺らぎが大きいかもしれない。
xformersと併用、共存はできない。
xformersよりVRAM使用量が増えるため、学習や高解像度生成時には注意

* --opt-channelslast

これはつけるだけ。
 --xformers や --opt-sdp-no-mem-attention と併用できる。
環境によっては効果が無かったり逆に遅くなったりするらしいので比較検討しましょう。

*設定のOptimizationの項目
** Negative Guidance minimum sigma
生成の後半にNegative Promptの計算を省略することで高速化する。

** Token merging
トークンをマージして高速化する。
解像度が大きいほど効果が大きくなる。
数値が大きいほど効果が大きいが品質も低下する。
fp8では動作しない。

** Pad prompt/negative prompt
76トークン以上で出力がおかしくなることがあるので、オフ推奨

* ハードウェアアクセラレーションによるGPUスケジューリング を無効にする
VRAM消費を減らす効果がある。速度も向上するかも。
やり方は[[こちら>>https://win11lab.info/win11-gpu-scheduling/#:~:text=%E6%9C%89%E5%8A%B9%E3%80%81%E7%84%A1%E5%8A%B9%E3%81%AB%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95&text=%E5%B7%A6%E3%83%A1%E3%83%8B%E3%83%A5%E3%83%BC%E3%82%88%E3%82%8A%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%82%92,%E3%82%AA%E3%83%B3%E3%80%81%E3%81%BE%E3%81%9F%E3%81%AF%E3%82%AA%E3%83%95%E3%81%AB%E3%81%97%E3%81%BE%E3%81%99%E3%80%82]]
[[公式wiki>>https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations]]にも記載がある
[[redditによれば>>https://www.reddit.com/r/StableDiffusion/comments/y71q5k/4090_cudnn_performancespeed_fix_automatic1111/jcx2jx5/?context=8&depth=9]]速度が30%くらいかわることがあるとかないとか
副作用としてDLSS3のフレーム生成が使えなくなる。


* モデルを軽量化する
2024年現在、fp16,pruned,safetensorsでの配布(=すでに軽量化された状態)が主流になった。

[+] 古い情報

- ckptとvae.ptをsafetensorsに変換する
-- 最近はckptよりsafetensorsで配布するほうが主流になった。
-- vae.ptはWindows Defenderが誤検出を起こすので回避のためにも変換するのが良さそう
--- 変換すると場合によって容量が約半分になる(NAIはそうだった)
- pruneする
-- VRAMの節約になるかも?
- fp16化する

fp16化の副作用として、modules.devices.NansException が発生する。
特にNovelAI系のVAEで発生しやすい(NovelAI,ClearVAE1.0,Nenhanceなどが該当)。

対策としては、fp32で計算するように強制する。(高速化としては本末転倒だけど)
 --no-half や--no-half-vae を追加する。速度は落ちてVRAM消費が大幅に増加する。

たまに真っ黒画像が出てもいいから高速化したいのであれば、
disable-nan-checkを追加する手もある。
それと、黒画像は大体VAEが原因なのでNovelAI系VAEを使わない手もある。

** VAEをckptに内蔵する

1111に標準で入っているCheckpoint Mergerでもできるようになっている
マージ対象Aに内蔵先モデルを指定して、Interpolation Method を No interpolation 指定
Bake in VAEで内蔵するVAEを選択してマージする

fp16化したモデルにマージするとVAE部分もfp16になる。
それによる高速化も起こりうるかも。
高速化しないと言っている人もいるので検証できたら追記お願いします。

ということでお気持ち表明に反証するで
[+]
複数のモデル、VAEで速度を計測したが誤差の範囲でしか変わらんかった。
masterpiece, best quality,
Negative prompt: lowres, low quality, bad anatomy, signature
Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 11, Size: 512x512, Clip skip: 2, ENSD: 31337
Batch size 2, Batch count 5

-Counterfeit
None:22.69s,21.73s
kl-f8-anime2.pt:21.40s,21.72s
kl-f8-anime2(fp16化):21.38s,21.70s
novelai:21.58s,21.81s
kl-f8のFP16化を埋め込んで生成:21.65s,21.61s
kl-f8のFP16化を埋め込んでモデルをfp16化して生成:21.56s,21.68s

-PastelMixPrunedFp16
None:21.49s,21.54s
kl-f8-anime2.pt:21.35s,21.67s
kl-f8-anime2(fp16化):21.38s,21.70s
novelai:21.45s,21.80s
kl-f8のFP16化を埋め込んで生成:21.34s,21.68s

-NovalAI
None:21.66s,21.89s
kl-f8-anime2.pt:21.29s,21.88s
kl-f8-anime2(fp16化):21.51s,21.62s
novelai:21.45s,21.80s
kl-f8のFP16化を埋め込んで生成:21.55s,21.67s
kl-f8のFP16化を埋め込んでモデルをfp16化して生成:21.45s,21.86s

no-half系の引数つけてない限り''モデルもVAEも内部ではfloat16(half)で処理してる''から変わるはずがない。
[END]

[END]

* Extensionsの退避や削除
しばらく使わないときは Dreambooth Extension を削除する(もう使ってない?)
起動時間が劇的に変わる

Extensionにはinstall.pyが同梱されていて毎回起動するので、
不要なExtensionが沢山入ってるなら消すだけでだいぶ速くなるはず。

UIからオフにするだけではダメ
フォルダごと別の場所に退避させとくとか、いっそバッサリ消すとか

** LyCORIS Extensionを削除する
1111がLyCORISに標準で対応したためこのExtensionは不要。
このExtensionは今となっては生成を遅くするだけ。

* Stepsを減らす
出来るだけ少ないStepsでも期待通りの絵が出る方法を模索する。
末尾にa(ancestral)がつかないサンプラーで再現できないか試す。
DPM++ 2Mは15Stepsでもそれなりに映えるのでおすすめ。
%%UniPCはもっと良い感じ。%%実際はそうでもない。
SchedulerにSimpleを使うとさらに良くなる。
LCM、TurboやHyper-SDはさらに速い。出力を大きく変化させたり相性の良し悪しがあったりする代わりに10steps未満でまともな絵がでる。
ν-predictionモデルは↑無しでも10ステップ程度でまともな絵を出せる。

* WebUIを高速なストレージに入れる
シーケンシャル3GB以/s上のSSDに配置することで読み込みを高速化する。大容量なSDXLで効果的。

* メインメモリを使う

モデルのロード時間を短縮するために、あらかじめRAMに読みこんでおくことが出来る。

Settings->Stable Diffusion
- Maximum number of checkpoints loaded at the same time
一度に読み込んでおけるcheckpointの数を設定できる。値が1でも読み込んだことのあるcheckpointは読み込み時間が短縮される。

SuperMergeでもモデル3つのトリプルマージまでRamで高速化できるようになった。
詳しくは [[ローカルのExtensions]] の SuperMerger を参照。

新しい機能などのために RamDisk が使いたくなったら ImDisk Toolkit が使いやすい。
https://sourceforge.net/projects/imdisk-toolkit/

1111 全部 RamDisk に突っ込んで高速化するのは、必要RAMのわりに恩恵がほとんど無いからおすすめしない。

[+]
ちなみに、ほとんどのOS(Android等も)は一度読み込まれたファイルをRAMに保持する
キャッシュ/スタンバイ領域というものがあり、
このキャッシュ領域はメモリの空き容量に応じて可変します。
(OSコアやプログラムで使用中のメモリ-総容量=キャッシュ/スタンバイメモリの最大値)
例えば、メモリが32GBで10GB使用中の場合のキャッシュメモリは20GBですが、
メモリにキャッシュされていないファイルを読み込み、総容量が20GBを超えると、
メモリにキャッシュされたファイルのうち一番古いファイルがキャッシュメモリから破棄されるので、
再度HDD/SSD等から読み込む作業が発生します。
[END]

* WSL2で使う

- [[1111_WSL2]]

以前はLinuxにするだけで1〜2割速くなると言われていたが、ドライバやTorchの最適化が進んで速度やメモリ消費の差が小さくなった。
どっちが速いか比較検討してみるのが良さそう。

* Linuxに入れる(上級者向け)
Windows上の仮想化ではなく実際にインストールする。
Ubuntu24.04 LTS、Python 3.11.9、Torch2.3.1での環境では、推論(it/s)は2〜4%、学習は10%高速化した。VRAM消費は特筆すべき変化はなかった。
また、メモリ消費が減少し、medvram有効時の生成開始と終了のもたつきが軽減して結構速くなった。いいぞこれ
ただしメモリ不足になるとWindowsは頑張って耐えるもののLinuxは素直にフリーズしてから落ちる

Windowsが入っていない空っぽのSSDに入れるか、サブ機とかお古になったら検討してもいいかも
なお各種ライブラリ等のインストールは原則コマンド打ってやるし、途中でUbuntuのGUI(gdm3)が壊れてリカバリーモードでgdm3を再インストールしたりとかなり面倒

* TensorRTを使う
https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT
最適化で大幅に高速化するもので最大2倍速くなるらしい。NVIDIA専用。
1111のExtensionから導入できる。

けど以下の重大な欠点があるのでベンチ以外で使い道はないかな
-あらかじめ設定した解像度のみ使用可能
-TensorRTモデル変換に時間がかかるしVRAM消費が激しい(しかもOOMになるとWebUI本体がフリーズ)
-LoRAはTensorRT変換で埋め込んで使えるが1つしか使えない
-ControlNetのようなU-netに干渉するものは一切使えない
-Extensionのインストールが不安定で失敗しやすい


* venvをいったん消して作り直す
Windows再インストール並みの最終手段。
最新の1111に必要なものしか入らなくなるので軽くなる、かも。
Extension試しまくってると肥大化するので月イチくらいでやるといいかも。

* 別のUIを使う
1111のWebUIを速くする方法でこれを書くのは本末転倒だが、ほかのUIを使う手もある。
AUTOMATIC1111のWebUIは最近になって速度が改善されたが、Windows版のメモリ消費の多さは変わっていない。Forgeのほうが省VRAM、ComfyUIは速くて省メモリで省VRAM。

* GPUを買い替える
ソフトウェア側であれこれ必死にやっても劇的に速くならないのが現実。結局ハードウェアの強化が一番。
場合によっては他のパーツの買い替えも必要になるが、最も効果的。
(give me money:3000.0)
