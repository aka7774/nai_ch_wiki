#contents

* 注意

torch2化、torch.compile改造ともにVRAMの消費量が増えます。
高解像度生成などでVRAMがカツカツな人は取捨選択が必要かも。
何倍も速くなるわけではない。xformersよりは良いかなってくらい。

torch.compile()でさらに高速化するが、以下の欠点があるのでお勧めしない。
- コンパイルそのものに時間がかかる
- モデル、解像度、Batch size変更の度にコンパイルされる ~~ &size(12){キャッシュされるので毎回コンパイルされるわけではない}
- WebUIにとって想定外の処理のため不安定。
- Linuxのみ対応

* モジュールによる高速化

2023-03-19現在、たぶんこの順で速い。

1. torch2 cu118 --opt-sdp-no-mem-attention torch.compile(不安定)(Linux限定)
2. torch2 cu118 --opt-sdp-no-mem-attention
3. torch1 cu117 xformers cudnnのバージョンアップ

** torch2 cu118 --opt-sdp-no-mem-attention/--opt-sdp-attention

RTX(2000〜4000番台)で高速化するらしい。
環境によっては--opt-sdp-attentionのほうが高速?

手順
1.venv,repositoriesフォルダ削除 あるいはvenv上で pip uninstall torch torchvision -y
2.git pullしてwebuiを最新版にする
3.webui-user.batを以下に書き換えて実行。--xformersはつけないこと!
=|BOX|
@echo off

set PYTHON=
set GIT=
set COMMANDLINE_ARGS=--opt-sdp-no-mem-attention --opt-channelslast
set TORCH_COMMAND=pip install torch==2.0.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu118

call webui.bat
||=
5.起動したらウェブブラウザでhttp://127.0.0.1:7860にアクセス。一番下に"torch: 2.0.0+cu118"と記載があれば成功

*** プレビュー版のインストール方法
あらかじめ古いバージョンをアンインストールしてから、venv内で以下のコマンドを実行する。
> pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118

[+]詳細説明
* torch2.0+cu118+SDP(要検証)
https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/8691
torchが正式に2.0となったため使用できるようになった技術
上記の「torchを2.0にする」と「利用するライブラリの更新(cu118=cuDNN8.8.0)」を同時に適用し、
更にSDPというxformersと似た技術を適用することで高速化を図る。環境によっては変化が無いかも。
RTX30,40にて効果を確認したが、1000番台ではxformers適用の方が速い可能性あり。
(txt2img,hires.fixで動作確認済)

- SDPについて
2023年3月上旬にcommitされたらしい
xformersと特徴は同じで、--opt-sdp-no-mem-attentionまたは--opt-sdp-attentionをつけるとほんのちょっと絵が変化する代わりに2割程度速くなる
xformersと併用、共存はできない
xformersよりVRAM使用量が増えるため、学習や高解像度生成時には注意
[END]

[+]導入した人たちの感想（ある程度集まったらまとめます）
=||
297今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ d112-SwXs)2023/03/19(日) 20:04:41.62ID:pGBhW2el0
ワイも
torch: 2.0.0+cu118
にして、xformersじゃなくsdpっていうのにしたんやけど
30it/sが35it/sになったから効果はあったと思うわ

ただし生成結果が変わって前の絵を全て捨てる事になる諸刃の剣やな

336今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ a1c3-7LVk)2023/03/19(日) 20:55:05.01ID:Mipkannl0>>345
torch2.0にしたらなんかVRAMの消費量上がる？
なんかすぐにメモリアウトするし、生成後にメモリ使用率が下がらんくなった
当方3060TI　vram8Gや

459今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 1154-CSnB)2023/03/20(月) 01:02:56.60ID:fmUx+vru0
>>449
起動時のコマンドラインに--opt-sdp-no-mem-attention付けた？x-formerの代わりや
--opt-sdp-attentionの方が速いとか言うけどこっちは生成する度に結果が微妙に違うガバガバやし上とほぼ速度差なかったわ

467今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 13ba-2lPJ)2023/03/20(月) 01:26:42.72ID:0uuy3D7h0>>516
torch2にしたらddetailer使えなくなってた

488今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 1154-BQ3C)2023/03/20(月) 02:09:36.21ID:mGnI1SHN0>>493
>>449だが、ちゃんとアスカベンチ回して全部計測してきた
RTX4070ti
　38.1s torch1.13.1+cu117
　22.2s torch1.13.1+cu117+cudnn更新
　19.9s torch2.0.0.1+cu118
元々cudnnは更新済みだったけどそこから少しだけ早くなってたけど思ったほどではなかった
こんなもんかな？

583今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ e17b-CSnB)2023/03/20(月) 10:47:01.35ID:zgEqc5Jw0
torch2入れたけどlora使った時の生成速度上がって幸せになれた

379今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ c97b-DkPe)2023/03/21(火) 18:53:37.44ID:R54ixFAd0
GTX1080でtorch2+cu118にxformerビルドして入れてみたが
大体5秒くらいしか早くならんな
RTX4090ほちい
||=
[END]

[+]オプション:xformersをtorch2用にビルドする(赤ちゃんには難しい)
- ビルド方法(Windows)
1.以下のコマンドを実行する。
※venvは使うな! 終わった後はtorch,torchvision,torchaudio,pytorch-tritonのアンインストール推奨
> pip3 install torch==2.0.0 torchaudio pytorch-triton --extra-index-url https://download.pytorch.org/whl/cu118 --force

2.[[ここの手順>>https://michyo.net/sempetit/install-xformers-windows/]]の「xFormers のビルド」の「xFormers が依存しているライブラリをインストール」まで進めた後、
xformersフォルダ内で以下のコマンドを実行する。
> python setup.py build
ビルドされるのでしばらく待つ。その後xformersフォルダ内で以下のコマンドを実行する。
> python setup.py bdist
その後xfomers\dist\にzipファイルができるので、それの中身のxformersとxformers-〇〇-egg-infoを1111のvenv/Lib/site-packages/にコピーする。
[END]

** xformers

xformersのインストールは1111の標準機能で簡単にできる。

*** torch1の場合

初めて導入する場合
 set COMMANDLINE_ARGS= --xformers

古い環境から更新する場合
 set COMMANDLINE_ARGS= --reinstall-torch --reinstall-xformers

*** torch2の場合

 pip install --pre -U xformers
でコンパイル無しで導入できるようになったらしい。

=|BOX|
アスカテストのtime token
--xformers --opt-sdp-attention　18.50
--xformers　20.30
--opt-sdp-attention　18.47
--opt-sdp-no-mem-attention　18.40
--xformers --opt-sdp-no-mem-attention　18.60
両方指定の場合はsdpが有効になってる雰囲気です
||=

[+]以前の方法
* 高速化オプションの利用

stable diffusionに用意されているxformersというオプションを利用する。
理論上、このオプションを付けてるときと付けてないときで絵が変わるらしいが
少なくともアスカプロンプトでは明確な違いを確認できず、演算時間が2割程度早くなる。
(明確な違いが出る条件がわかる方がいたらここに新章作って記録していってほしい)

検証ページ作ったので記録はこちらへ
→「[[xformersの検証]]」

** 対応GPU

- NVIDIA
- GeForce 1000番台〜3000番台(4000番台は後述)
- GeForce 900番台以前でも動くけど逆効果だったという報告あり

** 方法

stable-diffusion-webuiディレクトリ直下にあるwebui-user.batを修正する。

6行目修正前
> set COMMANDLINE_ARGS=

6行目修正後
> set COMMANDLINE_ARGS= --xformers

** 利用するライブラリの更新
2023/01/23時点の最新版(7ff1ef77dd22f7b38612f91b389237a5dbef2474)で本体に取り込まれた
旧バージョンのtorchとxformersを使用している場合はアップグレードを促すアナウンスメッセージが表示される
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/1sa2Wev7W4.png)

コマンドラインオプションに再インストールオプションを追加して起動すると更新される
> COMMANDLINE_ARGS= --reinstall-torch --reinstall-xformers ~~

アップデート完了した場合、WebUIの下部の表示が以下のようになる
(torch・xformers欄)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xh_WFZyBHE.png)
[END]

* torch.compile

(TensorRTみたいな感じで)モデルの事前コンパイルを行うことによって、
生成処理を高速化する仕組みがtorch2には実装されている。
これを1111で使えるように改造する。

Windowsには対応していない(WSL2で動かすことになる)

参考
https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/6932

前提として、torch2で1111が動作できていること。

1. modules/sd_hijack.pyのself.optimization_method = apply_optimizations()の前に次のコードを入れる(L171-172)
インデントの位置が前後と揃うようにする。

=|BOX|
        try:
            import torch._dynamo as dynamo
            torch._dynamo.config.verbose = True
            torch.backends.cudnn.benchmark = True
            m.model = torch.compile(m.model, mode="reduce-overhead", fullgraph=False)
            print("Model compiled set")
        except Exception as err:
            print(f"Model compile not supported: {err}")
||=

modeはdefault, reduce-overhead, max-autotuneから選べます
max-autotuneは10分以上かかる割に効果が薄かった
defaultだとコンパイル半分くらいで済む。こっちでも良いかも？

なお、残念ながら8GBの環境ではcompile完了前にCUDA OOMで死にました・・・。
reduce-overheadだとVRAMを食っていくけどdefaultだと消費しないので完走できる。だけど速さは感じない。
3060でreduce-overheadを試したら9.6GBだったので10GBあれば使えるかも。速さは劇的ってほどではない。
samplerは変えても再コンパイルは走らなかった。

2. repositories/stable-diffusion-stability-ai/ldm/modules/diffusionmodules/util.pyのcheckpointメソッドを編集(L113-114)
(これをしないとエラーになる)

=|BOX|
- return CheckpointFunction.apply(func, len(inputs), *args)
+ return CheckpointFunction.apply(func, len(inputs), *inputs, *params)
||=

バッチサイズ等を変えた後にGenerateを押すと返ってこなくなることがあります。リロードすると直る。この辺動作が怪しいです。

xformersは必要ない。
以下の警告が出るけど無視しても動く。

=|BOX|
/home/user/stable-diffusion-webui/venv/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: 
UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. 
Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.
  warnings.warn(
||=

- torch2用にビルドされたxformersは現状存在しない(自前でビルドが必要)
- xformersのビルドは茨の道

必要なかったこと
- 1111標準のaccelerate==0.12.0を少なくとも0.15.0以上に更新する必要がある
-- この類の問題を克服するためにsd_dreambooth_extensionをpip更新ツールとして使う手がある

出来ればやりたいこと
- コンパイルしたいモデルとしたくないモデルを選択したくなる気がする
* --opt-channelslast

これはつけるだけ。
 --xformers や --opt-sdp-no-mem-attention と併用できる。
環境によっては効果が無かったり逆に遅くなったりするらしいので比較検討しましょう。

* cudnnのバージョンアップ(RTX4000シリーズでは必須)
標準でインストールされるcudnn関係のDLLが古い(4000シリーズ未対応)為、対応版(cudnn8.7.0)に置き換える
最新版(2023-03-19現在だと8.8.1)に置き換えることでさらなる高速化が期待できるかも。

+https://developer.nvidia.com/rdp/cudnn-download より Local Installer for Windows (Zip) をダウンロードする~~※NVIDIA開発者アカウントが必要
+ダウンロードしたzipファイル(cudnn-windows-x86_64-8.7.0.84_cuda11-archive.zip)を展開し、binフォルダ内の7ファイルを取り出す
+stable-diffusion-main\venv\Lib\site-packages\torch\lib 内に上書きコピーする

* ハードウェアアクセラレーションによるGPUスケジューリング を無効にする
VRAM消費を減らす効果がある。速度も向上するかも。
やり方は[[こちら>>https://win11lab.info/win11-gpu-scheduling/#:~:text=%E6%9C%89%E5%8A%B9%E3%80%81%E7%84%A1%E5%8A%B9%E3%81%AB%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95&text=%E5%B7%A6%E3%83%A1%E3%83%8B%E3%83%A5%E3%83%BC%E3%82%88%E3%82%8A%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%82%92,%E3%82%AA%E3%83%B3%E3%80%81%E3%81%BE%E3%81%9F%E3%81%AF%E3%82%AA%E3%83%95%E3%81%AB%E3%81%97%E3%81%BE%E3%81%99%E3%80%82]]
[[公式wiki>>https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations]]にも記載がある
[[redditによれば>>https://www.reddit.com/r/StableDiffusion/comments/y71q5k/4090_cudnn_performancespeed_fix_automatic1111/jcx2jx5/?context=8&depth=9]]速度が30%くらいかわることがあるとかないとか

* モデルを軽量化する

- ckptとvae.ptをsafetensorsに変換する
-- 最近はckptよりsafetensorsで配布するほうが主流になった。
-- vae.ptはWindows Defenderが誤検出を起こすので回避のためにも変換するのが良さそう
--- 変換すると場合によって容量が約半分になる(NAIはそうだった)
- pruneする
-- VRAMの節約になるかも?
- fp16化する

fp16化の副作用として、modules.devices.NansException が発生する。特にVAEで発生しやすい。

対策としては、fp32で計算するように強制する。(高速化としては本末転倒だけど)
 --no-half や--no-half-vae を追加する。速度は落ちる。

たまに真っ黒画像が出てもいいから高速化したいのであれば、
disable-nan-checkを追加する手もある。

** VAEをckptに内蔵する

1111に標準で入っているCheckpoint Mergerでもできるようになっている
マージ対象Aに内蔵先モデルを指定して、Interpolation Method を No interpolation 指定
Bake in VAEで内蔵するVAEを選択してマージする

fp16化したモデルにマージするとVAE部分もfp16になる。
それによる高速化も起こりうるかも。
高速化しないと言っている人もいるので検証できたら追記お願いします。

ということでお気持ち表明に反証するで
[+]
複数のモデル、VAEで速度を計測したが誤差の範囲でしか変わらんかった。
masterpiece, best quality,
Negative prompt: lowres, low quality, bad anatomy, signature
Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 11, Size: 512x512, Clip skip: 2, ENSD: 31337
Batch size 2, Batch count 5

-Counterfeit
None:22.69s,21.73s
kl-f8-anime2.pt:21.40s,21.72s
kl-f8-anime2(fp16化):21.38s,21.70s
novelai:21.58s,21.81s
kl-f8のFP16化を埋め込んで生成:21.65s,21.61s
kl-f8のFP16化を埋め込んでモデルをfp16化して生成:21.56s,21.68s

-PastelMixPrunedFp16
None:21.49s,21.54s
kl-f8-anime2.pt:21.35s,21.67s
kl-f8-anime2(fp16化):21.38s,21.70s
novelai:21.45s,21.80s
kl-f8のFP16化を埋め込んで生成:21.34s,21.68s

-NovalAI
None:21.66s,21.89s
kl-f8-anime2.pt:21.29s,21.88s
kl-f8-anime2(fp16化):21.51s,21.62s
novelai:21.45s,21.80s
kl-f8のFP16化を埋め込んで生成:21.55s,21.67s
kl-f8のFP16化を埋め込んでモデルをfp16化して生成:21.45s,21.86s

no-half系の引数つけてない限り''モデルもVAEも内部ではfloat16(half)で処理してる''から変わるはずがない。
[END]
* Extensionsの退避
しばらく使わないときは Dreambooth Extension を削除する(もう使ってない?)
起動時間が劇的に変わる

Extensionにはinstall.pyが同梱されていて毎回起動するので、
不要なExtensionが沢山入ってるなら消すだけでだいぶ速くなるはず。

UIからオフにするだけではダメ
フォルダごと別の場所に退避させとくとか、いっそバッサリ消すとか

* Stepsを減らす
出来るだけ少ないStepsでも期待通りの絵が出る方法を模索する。
Euler a以外(収束するサンプラー)で再現できないか試す。
DPM++ 2M Karrasは15Stepsでもそれなりに映えるのでおすすめ。
UniPCはもっと良い感じ。

* メインメモリを使う

モデルのロード時間を短縮するために、あらかじめRAMに読みこんでおくことが出来る。

Settings - Stable Diffusion
- Checkpoints to cache in RAM
- VAE Checkpoints to cache in RAM

SuperMergeでもモデル3つのトリプルマージまでRamで高速化できるようになった。
詳しくは [[ローカルのExtensions]] の SuperMerger を参照。

新しい機能などのために RamDisk が使いたくなったら ImDisk Toolkit が使いやすい。
https://sourceforge.net/projects/imdisk-toolkit/

1111 全部 RamDisk に突っ込んで高速化するのは、必要RAMのわりに恩恵がほとんど無いからおすすめしない。

[+]
ちなみに、ほとんどのOS(Android等も)は一度読み込まれたファイルをRAMに保持する
キャッシュ/スタンバイ領域というものがあり、
このキャッシュ領域はメモリの空き容量に応じて可変します。
(OSコアやプログラムで使用中のメモリ-総容量=キャッシュ/スタンバイメモリの最大値)
例えば、メモリが32GBで10GB使用中の場合のキャッシュメモリは20GBですが、
メモリにキャッシュされていないファイルを読み込み、総容量が20GBを超えると、
メモリにキャッシュされたファイルのうち一番古いファイルがキャッシュメモリから破棄されるので、
再度HDD/SSD等から読み込む作業が発生します。
[END]

* WSL2で使う

- [[1111_WSL2]]

以前はLinuxにするだけで1〜2割速くなると言われていたが、
cudnnのDLL差し替えや、
Windows上で新しいバージョンのxformers等の導入がしやすくなったこともあり、
どっちが速いか比較検討してみるのが良さそう。
* Linuxに入れる
仮想化ではなく生で挿入れる
Windowsと比べて多少高速化するらしい・・・？
虎の子のゲーミングPCでは厳しいやね
サブ機とかお古になったら検討してもいいかも
なおインストールが簡単かは謎

* TensorRT(Lsmith)を使う
- [[Lsmith]]
- 使いこなすのは大変

* venvをいったん消して作り直す
Windows再インストール並みの最終手段。
最新の1111に必要なものしか入らなくなるので軽くなる、かも。
Extension試しまくってると肥大化するので月イチくらいでやるといいかも。

* GPUを買い替える
ソフトウェア側であれこれ必死にやっても劇的に速くならないのが現実。結局ハードウェアの強化が一番。
場合によっては他のパーツの買い替えも必要になるが、最も効果的。
(give me money:3000.0)
