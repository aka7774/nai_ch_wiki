* 目次

#contents

* ControlNetについて
ポーズや構図等を指定して画像生成できるようにする拡張機能。
従来のプロンプトでの画像制御に加えて、輪郭線・奥行き・人のポーズなどの「画像情報」を追加で入力することで、より思い通りに画像を制御できる。
その制御方法（Model）は複数ある（詳しくはcannyやhedやsegなど、下記参照）。

** 詳細
実際の実装では、
+元画像Aを与える
+その画像を「必要ならば」処理して（Preprocessor）、新しい画像Bを生成する
+画像Bと、選んだモデル（Model）を使って、画像を生成する。
という処理が行われるので、「Preprocessor」と「Model」の2つを選んであげる必要がある。
2番目が不要な場合は、Preprocessorとしてnoneを選択すると、画像AがそのままModelに与えられる。
（例えば最初から棒人間ツールなどで作った画像を与えたり、本拡張機能より優秀なプロセッサーで既に画像Aを画像Bに変換している場合など。）

例えばイラストから線画を抽出して画像を生成する（canny）の場合の処理の流れは、PreprocessorとModelにともにcannyを指定し、
+イラストを与える
+Preprocessor（canny）が、イラストから線画の画像を作る
+その線画を用いて、Model（canny）が画像を生成する
という流れになる。

** i2iとの違い
i2iは元画像の全ての情報（とくに色込み）を読み込むので、線画からの着色はほぼ無理だったり、元画像に新たに適切な色を塗ってやる必要があった。
それと比べて、ControlNetは「元画像が持っている特定の情報のみ」を取り出し、その情報とプロンプトを使って画像を生成する。例えば：
- 色情報が無い白黒の線画からでも着色できる
- 画像から線画を作るプリプロセッサーを噛ませれば、画像→線画→再着色により、元画像から色だけを変えるなどができる。
- 線画以外にも、画像からポーズだけを抽出して、プロンプトでは作るのが困難なポーズを取らせることができる
- 構図や領域分けだけ抽出して、どこに何を描くかという構図を完全に固定することもできる
という利点がある。
もちろん、色などの情報が落ちる分、完全にi2iの上位互換というわけではないことに注意。

参考(英語)google翻訳でよんでな
[[https://github.com/lllyasviel/ControlNet>https://github.com/lllyasviel/ControlNet]]&aname(lllyasviel)

**基本
現在Mikubill版はすごい勢いで更新されているので、定期的にアプデするのがいいと思うで。
ちなみに入門自体は拡張機能インストールしたりしたことあるなら楽勝だからあんまり焦らんでもええで
難しそうに見えるのはたぶん「こんなポーズにしてや」と渡した画像を下処理する手法(Preprocessor)がたくさんあるせいや(棒人間化とか奥行き情報を白黒で表現とか)

- 爆速で実装されたときのディスカッション：ControlNet now available in the WebUI! &#183; Discussion #7784 &#183; AUTOMATIC1111/stable-diffusion-webui - https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/7784
- 現在メインの拡張機能リポジトリ：Mikubill/sd-webui-controlnet: ControlNet の WebUI 拡張機能 - https://github.com/Mikubill/sd-webui-controlnet

*インストール
**Mikubill版
-https://github.com/Mikubill/sd-webui-controlnet
上のurlをStable Diffusion Web UI の「拡張機能」タブ > 「URLからインストール」

▼モデル配置場所
%%-https://huggingface.co/webui/ControlNet-modules-safetensors/tree/main%%
%%8つの.safetensors ファイルをダウンロードして (わかるやつは右上の<>how to clone押してgit)%%
下記の「モデル」の項目から適宜ダウンロードして以下のフォルダに配置
=|PERL|
stable-diffusion-webui\extensions\sd-webui-controlnet\models
||=

** モデル
- 標準的な軽量モデル：https://huggingface.co/webui/ControlNet-modules-safetensors/tree/main
- kohya氏によるモデル改良版（何か応答がよくなるらしい？）：https://huggingface.co/kohya-ss/ControlNet-diff-modules/tree/main
上の軽量化とは違う方法での軽量化で（diff版）、原理的にはこっちのほうがいいらしいが、上の方がいい場合もあるらしい（[[ここ参照>https://github.com/Mikubill/sd-webui-controlnet#extraction]]）。検証求む。
- もともとのControlNetでのモデル（ただし1つ5.71GB）：https://huggingface.co/lllyasviel/ControlNet/tree/main/models
- %%Abyss Orange Mix2用のモデル（？？誰か検証してクレメンス）：https://civitai.com/models/9557/abyssorangemix2-controlnets%% ←これは上のkohya氏のものとハッシュ値が一致するので同一のようです。

** モデルの変換とか
[+]
-Controlnetのpthとかsafetensorsって各マージモデル用に最適化した方がええんか
モデル専用のそれらがうｐされとるな
-> 968
指示画像に生成画像が従うように後付けのネットワークが訓練されとるのがControlNetや
今の公式版はSD1.5の生成画像を操作するように学習されとるから別のモデルに適用すると当然ズレがあって最良の結果にはならん
なのでモデルにはそれぞれのControlNetが本来は必要で専用のがあるのはおかしい話やないんやけど
その専用のがどういう風に作られてるのかは知らん
↓
-> 980
それもう古い話やで
わざわざ専用版作らなくてもマージでそれっぽく動いたでという初期の報告や
それと同じことをミクビル版はその場でやっとる(やからモデルの切り替えがやけに重かったりする)
- [実験]他のSD1に制御を移します。X モデル &#183;ディスカッション #12 &#183;lllyasviel/ControlNet - [https://github.com/lllyasviel/ControlNet/discussions/12](https://github.com/lllyasviel/ControlNet/discussions/12)
[END]
** unpronpt版
-https://github.com/ThereforeGames/unprompted
試したけどうまくいかんかったんで誰か書いて・・・

*使い方
**Mikubill版
txt2imgタブの下にControlNetができてるので▼をクリック
-Enableを押して
-Preprocessor を選んで
-Preprocessor と同じ名前のモデルを選ぶ

詳しい例は[[上>#lllyasviel]]のやつ読んで

** Preprocessorの種類と特徴
|canny|輪郭線を検出して取り出す|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/NViOVZS5CD-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/NViOVZS5CD.png]]|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/4iq0nA0ANk-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/4iq0nA0ANk.png]]|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/AdyrhRy78X-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/AdyrhRy78X.png]]|
|depth|深度情報を引き継ぐ 立体的な位置関係を処理できる|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/N2r_v1rV9j-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/N2r_v1rV9j.png]]|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/UZU7n3ZOva-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/UZU7n3ZOva.png]]|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/QQNO3ImvDh-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/QQNO3ImvDh.png]]|
|hed|いわゆる「描き込み」的な部分を取り出す タッチを引き継ぎやすい||[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/m6Tlxhhuh7-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/m6Tlxhhuh7.png]]|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/COjrN0iWAw-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/COjrN0iWAw.png]]|
|mlsd|直線の輪郭だけ取り出す 曲線のない人工的な構図をノイズ無く取り出せる||||
|normal_map|法線情報を引き継ぐ　深度情報よりも細部の立体感に沿いやすい|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/iyHkyJEFHf-s.jpg)>https://image02.seesaawiki.jp/n/h/nai_ch/iyHkyJEFHf.jpg]]|
|openpose|おなじみの棒人間|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/WTbFDdTOT7-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/WTbFDdTOT7.png]]|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/5WaCsMlIST-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/5WaCsMlIST.png]]|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/RwFNDeU88I-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/RwFNDeU88I.png]]|
|openpose_hand|棒人間に手が追加されたやつ||||
|scribble|マウス入力の落書き　線を絶対的なものとしては扱わず、いい感じに仕上げる|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/5tsjNQTbfT-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/5tsjNQTbfT.png]]|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/qhknh2I97S-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/qhknh2I97S.png]]|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/v7ZV5VrtYc-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/v7ZV5VrtYc.png]]|
|fake_scribble|画像を一回落書き風に変換してscribbleに投入する|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/4uQiYf6rzq-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/4uQiYf6rzq.png]]|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/Zun19XTTDJ-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/Zun19XTTDJ.png]]|[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/9PeZWztrlG-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/9PeZWztrlG.png]]|
|segmentation|'ADE20K'という規格で塗り絵 領域に分類を持たせる||[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/EmAiydIWmc-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/EmAiydIWmc.png]]|[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/o_bVqhY228-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/o_bVqhY228.png]]|
とりあえずスレにある画像使ったんであかんかったら消すわ

元画像から加工されたcannyやdepthなどの画像はデフォルトでは 
=|PERL|
stable-diffusion-webui\extensions\sd-webui-controlnet\detected_maps
||=
に保存されているのでいいやつができたら取っておこう
**得意不得意

ちょっと試した感じこんな感じでした
- %%OpenPose: 奥行き苦手%%
- NomalMap: 奥行き得意 ただし背景が消失する(?)
- DepthMap: 奥行き得意 背景も死なない

Normal & Depth
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/pDOEL4rMst.2_comp001.png,600)

OpenPoseは実写/イラストで検出精度が変わるみたいです
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/M09ux9A9R1.3_comp001.png,600)

** segmentation
塗り絵から構図と描くもの（人や道や木など150種類の概念から選べる）をコントロール・・・できそうなやつや。

ADE20Kって規格で塗り絵するで
物と事で色分けが違うらしい。雑に言うと「人間」は数えられるので「物」、「空」は数えられないので「事」 
色分けの決まりは下参照
// -&ref(https://image01.seesaawiki.jp/n/h/nai_ch/edkUvJLYzQ.png)
// これは違う色分けや。下の2つとは違う。この画像の色が何準拠なのかは分からんで……
-https://github.com/CSAILVision/sceneparsing/tree/master/visualizationCode/color150
-ADE20Kのオブジェクト-RGB対応表スプレッドシート https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0

こんなん人力では無理やという場合は元画像（''実写が望ましい''）を用意してWebで
https://huggingface.co/spaces/shi-labs/OneFormer
the task is semantic, ADE20K, DiNAT-L（最後はSwin-Lでも可、たぶん変換のモデルの違い？）
にチェック入れて送信すると右下に塗り分けられて出力される。
あとは適当なペイントツールで好きなように修正する。
***めんどくさい！ようわからん！
とりあえず参考にしたい画像をドロップして
Preprocessor:segmentation, model:~_segにして出力してみる
出てきた塗り絵を適当なペイントツールで修正する。
ちなみに元画像はアニメ塗りより実写がいいらしい(って4chanで見た)→openposeの話やったすまん

***複数人を塗り分けるには
https://huggingface.co/spaces/shi-labs/OneFormer
%%%the task is panoptic%%%, ADE20K, DiNAT-L
にチェック入れて送信すると
判定がゆるくなる代わりに人数分だけpersonの色が増えるので
適当なペイントツールで色分けして塗る。(人間が重なっていなければ塗り分けなくていい)
[+]
基本
=|PERL|
229今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 8655-8lmq)2023/02/16(木) 15:33:50.21ID:g7tc6IYz0
>>226
まずsegに必要なのはADE20Kっていう画像内のオブジェクトをクラスごとに色分けしたやつや
各色とオブジェクトの紐づけはこのページとひとまとめにした画像を参照
基本は150色
https://github.com/CSAILVision/sceneparsing/tree/master/visualizationCode/color150
https://i.imgur.com/XXryPUX.png
すでに学習済みのデータセットやから残念ながら自分で色足したり設定変えたりはできん
ただしこのクラス分けもさらに細分化されたり混ぜ合わせたりが可能みたいや
詳しくは後述
||=
ADE20Kのオブジェクト-RGB対応表スプシ
https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0

=|PERL|
236今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 8655-8lmq)2023/02/16(木) 15:43:14.25ID:g7tc6IYz0>>239
>>226
ほんで写真やイラストを変換したいときはここ使う
https://huggingface.co/spaces/shi-labs/OneFormer
segで使うにはthe task is semantic、ADE20K、DiNAT-Lを選ぶ(ワイもADE20K以外は選ぶ意味が分かってない)
https://i.imgur.com/Sc2gMxV.png
で変換すると右に二枚画像が出てくる
https://i.imgur.com/Bh7dOLM.pngこっちは「この色分がこのオブジェクトなんやで〜」って説明
これを見ると分かると思うが明らかに基本の150色には無いオブジェクト名や色が出てくる
この辺がさっき話したほんとはもっと細かいという話やね
ADE20Kでググると細かいツリー画像出てくるからまぁそういうことなんやろ
https://i.imgur.com/nam6pNp.pngほんでこっちがwebUIに入れる色分け画像
プロンプトなしで色分けルールに沿ったもの大体出てくる

260今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 8655-8lmq)2023/02/16(木) 16:00:12.39ID:g7tc6IYz0>>363
>>226
後はちょっとした応用で、色分けで生成されるものが決まるので色の一覧表や欲しい要素がある画像を変換器に入れて出てきた色をスポイト抽出して
適当にペイントツールで塗り塗りしてその画像をsegに入れると好きな要素を好きな構図で出せるようになるわけ
これは前スレに貼ったskyとかmountainとかwaterの色塗って作った奥行きのあるなんか壮大そうな景色を出すやつ
landscapeとネガティブの(low quality,worst quality:1.4),(monochrome:1.1)のみ
https://i.imgur.com/Da7bWWv.png
https://i.imgur.com/mOB9khp.png
https://i.imgur.com/74r2uZx.png
欠点としては人の形もシルエットになるから腕や足が体に重なったポーズはうまく認識できなかったり、ありえんボリュームの髪のシルエットに無理やり引っ張られて奇形になったりすること
||=

複数人を塗り分けて描画する方法
=|PERL|
578今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 8655-8lmq)2023/02/16(木) 22:20:25.69ID:g7tc6IYz0>>580
まず複数人が登場する画像を用意する
今回はワイが以前AIで生成した画像やが相性的に実写寄りの絵か写真のほうが好ましい
>>236でsemanticを選べと言ったがこの方法だと何人いようが体が重なってようが離れていようがpersonは一色しか出てこないんや
https://i.imgur.com/q1n8yln.png
なので画像変換の時にpanopticを選ぶと色々と認識ガバガバになる代わりに人数分だけpersonの色を用意してくれるんや
https://i.imgur.com/nYCNpRB.png
その複数出てきたpersonの色で塗るとそれぞれ別人と認識してくれるんや
ただしこの通りのガバガバ色分けやからsemanticみたいに変換した画像を直にwebuiに入れるのはまず無理やね

580今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 8655-8lmq)2023/02/16(木) 22:23:04.48ID:g7tc6IYz0>>582
>>578
例えば抽出したそれぞれのperson色を使ってhttps://i.imgur.com/fIUQSnI.pngを作る
これをsegに突っ込むとhttps://i.imgur.com/u8A3WmX.png
狙ったのとはちょっと違うんやけどまぁそれぞれの色を別の人間と認識して融けたりしてないというのが大事や
重なり合った部分をちゃんとさせたいならレイヤー分けぐらいはできるペイントツールがあったほうがええやろな
割とダイレクトにシルエットの形状が体型に影響するのであんまり適当やと肩幅や顔のデカさがすごいことになったりする(というかなった)
手描き苦手ニキはデザインドールとかでポーズ取らせた画像に塗るのがええと思うで

591今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ 8655-8lmq)2023/02/16(木) 22:35:11.73ID:g7tc6IYz0
ちゃんと変換に入れる人数が増えるとその分の色が出てくるので3人の画像使えば3色使って
https://i.imgur.com/HE9FtVa.pngこんなんを描いて
https://i.imgur.com/ja6fzus.pngちゃんと左が一番手前で右が一番奥な画像が出せる
こんなマウスで適当に引いた絵じゃなければ変な袖とか出ないでもっと綺麗に出るはずやで
何なら指とかも書き込めばちゃんと認識してくれる
逆に言えばこのレベルの絵でも融け合わないで位置関係把握してくれるんや
ちなみに体が重なってないなら色分ける必要ないで
一色でそれぞれ人型描いてもええしsemantic変換した画像そのまんま突っ込んでもええ
この通り複数人をあまり混ざらず出せるのがメリットやが
オリジナルの構図にしたかったらなんやかんや結構な技量とセンスが問われるのがデメリットやと思う
労力割かずに複雑なポーズ一発出しならそこは断然openposeや
使い分けていこう
||=
[END]
***輪郭線を白で描くことで、腕組の表現の打率を上げる
=|PERL|
701今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった (ﾜｯﾁｮｲ cb28-BTrK)2023/02/20(月) 01:04:22.35ID:WCL0iJ9f0
>>690
なるほど、人物とかくっきり輪郭線付いてるね
試しに腕組seg作ってみたら重なる表現は相変わらず苦手だけどなかなか良さそうだった
色分けと上手く使い分けたいね
https://i.imgur.com/DUfpcSL.png
https://i.imgur.com/N9pdqX1.png
https://i.imgur.com/2H4eWAU.png
https://i.imgur.com/nPccccl.png
||=
***Guess Mode (実験的機能)
WebUIの設定 > Enable CFG-Based guidance をオン
ControlNet使用時に Guess Mode を有効にすると
cannyやdepthを使ってプロンプト無しでも%%いい感じにでっち上げる%%柔軟に推論する機能
サンプリング回数 50, CFGスケール 3 〜 5 を推奨
[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/G2yrqX6Tvk-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/G2yrqX6Tvk.png]][[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/N4L7ONuBoC-s.png)>https://image02.seesaawiki.jp/n/h/nai_ch/N4L7ONuBoC.png]]
例)NSFW, 1girl, NC: (worst quality:1.4), (low quality:1.4) , (monochrome:1.1), のみ 


* ツール
- Openpose rig：Character bones that look like Openpose for blender - https://toyxyz.gumroad.com/l/ciojz?layout=profile
- PoseMaker - a Hugging Face Space by jonigata - https://huggingface.co/spaces/jonigata/PoseMaker2
huggingface上で2Dで某人形を操作
- webuiで棒人間いじれる拡張出とるやで
https://github.com/fkunn1326/openpose-editor
- Posex
https://github.com/hnmr293/posex
1111上で棒人間を動かすやつ。同様の拡張やツールがいくつかある。
- Design Doll
https://terawell.net/ja/
体験版ではポースをセーブできない。使用感は↓
https://twitter.com/Zuntan03/status/1629501963913285633
あとsegに食わせるセグメント画像簡易ペイントツールがとしあきのところのwikiにあるで。
* アセット
** Openpose
Openposeリグの画像をシェアすることで、手軽にポーズアセットとして配布できるようです。
使用時はプリプロセッサ：None、Model：Openposeにしてポーズ画像をImageにドラッグ&ドロップします。

**キャラクター設計図
いわゆるキャラデザ：Turnaround資料を生成できるやつ
とりあえずRTX3060 12GBでHires. fix resize: from 1024x512 to 2201x1100ぐらいまで生成できました。
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/ldUUiyjvpT.TurnDesign_Openpose.png,600)

BlenderのOpenposeRigで作成。
キャラデザ用
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/aGVolQf_rN.jpg,400)　&ref(https://image01.seesaawiki.jp/n/h/nai_ch/4HHXZAsdLF.jpg,400)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/EvnQDcUJyj.jpg,400)　&ref(https://image01.seesaawiki.jp/n/h/nai_ch/anE_vIs97x.jpg,400)
生成例
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/qmOjTfkh0x.png,400)

*サンプル
**活用例
***Scribble と Openpose 同時使用
=|PERL|
Scribble と Openpose を Multi CN で適用すると、ワイのクソヘボ落書きでも姿勢の打率爆上がりするな
Openpose のみやとなんか姿勢固くなっちゃうし、Scribble でサポートするのもいいかもしれん
棒人間はとしあきさんとこのツールを使わせてもろてるで
半透明モードがあるのでキャンバスの上から関節位置を調整してる
CN 設定はこんな感じ
一回 t2i で構図決めてから i2i で Step と CFG を上げて高画質化しとる 
||=
&ref(https://i.imgur.com/AmH8iF5.jpeg,400) &ref(https://i.imgur.com/pSIRWTD.png,400) &ref(https://i.imgur.com/r0RdrQZ.png,400)
***ずらしハメパンツ修正メソッド（t2i簡単超高打率）
=|PERL|
t2iのプロンプトpanties asideで出したずらしハメ…しかし結果はそうじゃない違うんだよ…それが許されるのはブルマとパンツだけなんだ…そんな時に有効な方法
1.画像をControlnetのCannyを通して出力された線画をドラッグ＆ドロップでローカルに保存
2.パンツの不要部分をペイントアプリで除去（白で塗りつぶす）
3.修正した線画をControlnetにドラッグ&ドロップ
4.Preprosessor:Canny、Model:Canny、Weight:0.7±0.2程度
5.プロンプトpanties asideの強度を0にする(panties aside:1.5) → (panties aside:0)
メリット:超高打率でガチャほぼ不要　デメリット:色が少し薄くなる、細部がちょっと変わる
なお、hedでも同様のことが可能。hedの場合は4のPreprosessorをnoneにする。こちらは出力が濃くなる。お好みでどうぞ。
||=
&ref(https://i.imgur.com/eZYJiin.png,400) &ref(https://i.imgur.com/U5YsOp1.png,400) &ref(https://i.imgur.com/Q7ohq3S.png,400)
***depth と hed 併用
https://twitter.com/Zuntan03/status/1629362974547906566
***Scribble + Normal, Depth
=|PERL|
1. Scribble でゴミのような線画をいい感じのリファレンスにしてもらう
2. Daz で用意したなんとなくそれっぽいモデルを Blender に持ち込んで髪の毛だけ別に配置する
3. Normal, Depth を出力してそれぞれ ControlNet で使用してみる。Prompt はある程度具体的に指定しておいた方がよさそう

Normal はちゃんと背景まで作りこんでおかないといけなそう
Depth は背景は勝手にいい感じにしてくれるけど後ろ髪をあんまり髪として認識してくれなかった

ちな Blender から Normal 出力するときは Viewport Shading から Matcap Normal を適用して、Viewport Render Image で書き出す
Outline は無効にしておいた方がエイリアスがなくてよさそう
ControlNet での使用時は RGB to BGR にチェック
Depth は Depth Pass を Normalize + Invert して書き出す
マテリアルに半透明があると Depth にノイズが乗っちゃうので、不透明なマテリアルで Override しておく
||=
&ref(https://i.imgur.com/23smCZI.png,400) &ref(https://i.imgur.com/TeZ0Wf7.png,400) &ref(https://i.imgur.com/KlJmWta.jpeg,400)

***inpaint
=|PERL|
inpaintのcontrolnetでもできるで
一つの方法としては
元画像をt2iのcanyに通して線画を出力する。線画はドラックアンドドロップで適当なフォルダに落とせば保存できる
胸のところの線画を加工して雑でもいいからおっぱいにする
それをcontrolnetに置いて、Preprocessorをnoneにしてmodelをcanyにする
元絵は上半身をインペイントで黒くする
プロンプトはshirt lift, breasts, underwearとか描きたい要素を入れる
加工が面倒ならdepth画像無加工でWeight落とすだけでもいけるかも
||=
&ref(https://i.imgur.com/MgC72PR.png,400)
**動画
外人ニキやけど早送りでもだいたい分かる [[https://www.youtube.com/watch?v=ci7NfTsifd0>>>https://www.youtube.com/watch?v=ci7NfTsifd0]]




