#contents
* 概要
Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning
雑に言うとDreamboothの小さくて速いやつ
''作成方法はいろいろある。''ここでは kohya_ss版とそのフロントエンドについて雑に書いてある
*参考資料
https://rentry.org/lora_train
クラウドGPUを使う場合はリンク先の下の方に Colab Instructions がある
フォルダ命名方法に気をつけて、自前のファイルは半角スペース一切入れないようにすれば無料Colabでも回せる。頑張れ。

*他人の作ったモデルの導入
[[Loraの使用方法>#apply_lora]]だけやればOK

*あかちゃんLoraインストーラー

あかちゃんインストーラーで1111を入れた人向けにPYTHONとGITのPATHをいじってあるやつ
start.batと同じフォルダに入れて実行してください

- コマンドライン用
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts.bat
-- https://github.com/aka7774/elemental_code/blob/main/tools/run_sd_scripts.bat
- ダイアログ用(みかんせい)
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts_easy_training.bat
-- lora_train_popup.pyはワイの知能では使いこなせなかった

*準備
1. エクスプローラーを開き、適当なフォルダ内で右クリック→git bash hereでgitのターミナルを開き、以下のコマンドを実行する
> git clone https://github.com/kohya-ss/sd-scripts.git
2. https://github.com/derrian-distro/LoRA_Easy_Training_Scripts から~~ lora_train_popup.py~~ lora_train_command_line.py~~ run_popup.bat~~ run_command_line.bat~~をダウンロード~~注) 「リンク先をファイルに保存」 ではなく リンク先に飛んでコードの右上の RAW ボタンを押してメモ帳みたいなテキストばっかりの画面を出して 「名前をつけてページを保存」
あるいはCode→Download zipでダウンロードして解凍する(zipに入ってる.gitignore、LICENSE、README.mdは不要)

3. スクリプトを sd-scripts フォルダにコピー

4. README-ja.md に日本語の詳しい説明が書いてあるので読む
=|PERL|
 train_network_README-ja.md: LoRAの学習について
 train_db_README-ja.md:DreamBoothのガイドです。LoRA等の追加ネットワークの学習にも同じ手順を使います。
 fine_tune_README_ja.md:キャプションとか
||=
- 初回セットアップ
--Python初めての場合、PowerShell初めての場合
=|PERL|
PowerShellを管理者として開きます。
「Set-ExecutionPolicy Unrestricted」と入力し、Yと答えます。
管理者のPowerShellを閉じます。
||=
--sd-scripts フォルダで PowerShellかターミナルを開く (Shiftを押しながら右クリック)
--以下のコマンドを順番に入力する(コピペでOK)
=|PERL|
python -m venv venv
.\venv\Scripts\activate
 
pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
pip install --upgrade -r requirements.txt
pip install -U -I --no-deps https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/f/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl

cp .\bitsandbytes_windows\*.dll .\venv\Lib\site-packages\bitsandbytes\
cp .\bitsandbytes_windows\cextension.py .\venv\Lib\site-packages\bitsandbytes\cextension.py
cp .\bitsandbytes_windows\main.py .\venv\Lib\site-packages\bitsandbytes\cuda_setup\main.py
 
accelerate config
||=
なお、python -m venv〜の行で「python」とだけ表示された場合、py -m venv〜のようにpythonをpyに変更してください。

--accelerate config のあとに質問が出るので以下のように答える
=|PERL|
- This machine
- No distributed training
- NO
- NO
- NO
- all
- fp16
||=
--初回セットアップ完了
*学習の手順
**Windowsの場合
***ポップアップ版を使う場合
+ run_popup.batを実行
+ ポップアップにパラメーターを入力する
+ 出来上がりを待つ

***コマンドライン版を使う場合
+ lora_train_command_line.py にパラメーターを書く
+ run_command_line.batを実行
+ 出来上がりを待つ

**Linux(wslやクラウドGPUニキ)の場合

***ポップアップ版を使う場合
+ source venv/bin/activate と入力
+ accelerate launch --num_cpu_threads_per_process 12 lora_train_popup.py と入力
+ ポップアップにパラメーターを入力する
+ 出来上がりを待つ

***コマンドライン版を使う場合
+ lora_train_command_line.py にパラメーターを書く
+ source venv/bin/activate と入力
+ accelerate launch --num_cpu_threads_per_process 12 lora_train_command_line.py　と入力
+ 出来上がりを待つ

**Lora作成手順の画像 (ポップアップ版) 参考程度 (2023-1-16時点)
クリックして展開 アップデートなどで内容は変わる
わからんパラメータが出たらcancelを押しとけばデフォルト値が入る。抜けがあったらスレで質問よろ。
[+]
**ターミナルとかパワーシェルにコマンドを打つか、run_popup.batから実行する
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/yVzXCg9crC.png)
----
**設定ファイルを読み込む?
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ceVWQWf9Uj.png)
----
**学習元のモデルを選ぶ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/jxCNF4YB2L.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/6tebwsnoiv.png)
----
**学習用画像のフォルダを選ぶ
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/QsMm7_wdmg.png)
数字_名前 フォルダが見えるように
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/BC_G2nQ6O4.png)
----
**出力先のフォルダを選ぶ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/d_LHSt9yQv.png)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/Pm3ilZIP1a.png)
----
**設定をjson形式で保存する?
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/1LwRQpGmTB.png)
----
**正則化画像のあるフォルダを選ぶ 使わないときは「いいえ」
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/KXO6r5NWEn.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/e99bPUOmNk.png)
数字_名前 フォルダが見えるように
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/OYsxnrbqxq.png)
----
**学習を再開する?
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/7Vg8xAPetT.png)
----
**バッチサイズ:一度に何枚処理するか つよつよGPU以外なら1で
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xcTV3i_Lbh.png)
**何エポック学習させるか: 1エポックは 繰り返し回数(フォルダの先頭の数字)×学習用画像の枚数 ステップ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DWVNJ2DpMd.png)
**dimサイズ: みんな128をつかっとるらしい
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/I75XLmK3_t.png)
**アルファ: dimサイズと同じがいいらしい
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/vZVRRTw9ES.png)
**学習の解像度: 512で。RTX4090か超強クラウドGPUなら 768 もいける
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ttb3qyi0Ya.png)
----
**学習率(Learning Rate): 1e-4 (= 0.0001) これより上げることはあんまりない。 5e-5 (=0.00005)くらいでもいいかも
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DkV4RQ4ZnK.png)
**スケジューラー: cosine_with_restarts で(よく分からんのでいじらない) 学習率を途中で上げ下げするやり方
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/0jBUxX8eVo.png)
**エポック単位でセーブする?: 2エポック以上学習させるなら
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/M6Jh5Uc7cL.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ZCqzxdoZWw.png)
----
**キャプションをシャッフルする?: する
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/IEAf9r1tqK.png)
**キャプションの最初のトークンを保持する?: 
キャプションを付けた場合フォルダ名のインスタンスプロンプトが無効になる~~のでキャプションファイルの先頭にインスタンスプロンプトを自分で書く必要がある。~~作者のnoteによると「数値を指定するとキャプションの先頭から、指定した数だけのトークン（カンマ区切りの文字列）をシャッフルせず固定します。」~~キャプションの先頭からカンマ区切りで判定されるので 「zkz, 1girl, condom, ass, solo, black panties, one side up,」なら1でおk
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/kMrIwfMsJE.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/SKOgINqr6w.png)
**warmup ratio 使う?: 学習の最初だけ学習率を下げる機能
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/WaHG0v8I5O.png)
----
**学習の様子 縦横の比率は自動で振り分けしてくれる
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/9VizQs0l1c.png)
**出来上がり
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/y3w3sgkzGd.png)
last.safetensors というファイルができる
このスクリプトではログは一切残らないのでわかりやすい名前にリネームしておく
追加学習するときはこのファイルを指定する
[END]
動画(2023-01-24) すぐにアプデで役に立たなくなるが一応
わからんパラメータは キャンセルでデフォルト値が入る
[+]
&video(https://image01.seesaawiki.jp/n/h/nai_ch/ypwjUxQC6D.webm)
[END]
*学習用画像を置くフォルダの配置
-作者の解説が詳しい [[https://note.com/kohya_ss/n/nba4eceaa4594>>https://note.com/kohya_ss/n/nba4eceaa4594]]

-フォルダの配置例:
[-]
※要するに<繰り返し回数>_<インスタンスプロンプト>にリネームした学習画像データのフォルダは直接指定しないでねって話
例えば↓こういうこと
&#10060;E:\kohya_ss\TrainDatas\001\img\40_kdy 1girl
&#128994;E:\kohya_ss\TrainDatas\001\img
間違うと画像が見つかりませんと怒られる

&ref(https://image02.seesaawiki.jp/n/h/nai_ch/8v9xToIuUR.png)
[END]
-同時に10まで概念を学習できるが、少なくとも1つはフォルダが必要。
-フォルダの名前は <繰り返し回数>_<インスタンスプロンプト>
--<繰り返し回数> 繰り返し回数×学習用画像の枚数を1セット(1 epoch)として学習する 
※注 学習用の画像が50枚ある場合、繰り返し回数を20 にすると 20 x 50 = 1000 ステップ学習する
--<インスタンスプロンプト> 呼び出し用のキーワード 英単語にない意味のないワードがよい 
--キャプション ファイルは必須です。そうでない場合、LoRA は概念名をキャプションとして使用してトレーニングを行います。
--キャプションについては以下

*キャプションを付ける
作者の詳しい画像付き説明 [[https://github.com/kohya-ss/sd-scripts/blob/main/fine_tune_README_ja.md>>https://github.com/kohya-ss/sd-scripts/blob/main/fine_tune_README_ja.md]]
自動でつけてくれる・・・のではなく一応コマンド打たないとダメらしい。詳しくは上記
**WD1.4 Taggerで作成
先に学習用画像を連番にリネームしておく (01.png, 02.png, ...など)
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/kbCJ6oEi6E.png)
[END]
Web UI に拡張機能 stable-diffusion-webui-wd14-tagger [[https://github.com/toriato/stable-diffusion-webui-wd14-tagger>>https://github.com/toriato/stable-diffusion-webui-wd14-tagger]]をインストール
「Tagger」タブの「Batch from directly」
-入力ファイル:学習用画像の入っているフォルダ
-Interrogator:wd-14convnext
-アンダースコアの代わりにスペースを使用する:オン
-括弧をエスケープする:オン
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/DGlOwfa65F.png)
[END]
Interrogateを押すと学習用画像のフォルダにタグの付いた .txt ファイルが生成される
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/vrFirugtW9.png)
[END]
**キャプションの細かい加工
https://github.com/starik222/BooruDatasetTagManager や スレ130の>>346>>949のツールで
余計なタグを削ったりこだわりのタグ追加をする
*そもそも学習用画像ってどうやって加工するの
本文で説明している kohya_ss 版のLoRAではトリミングはしなくていい(画像のサイズ別に学習が行われる)
背景の切り抜きは・・・画像の大きさが揃ってないとめんどくさいなどうしよう・・・

キャラの切り出しだけやったら3Dペイント(Win10なら標準、11では標準からリストラされたけどストアにおるで)のマジック選択でええ感じに切り抜きやすいからそこからgimpなりで微調整。
一枚一枚やんのめんどくさい言うんやったらABG_extension言うのが出たんでつこてみたらええんとちゃうかな…？しらんけど
*正則化画像
キャプションつけたらそのプロンプトで学習させるモデルを使って(適当なネガティブプロンプトをつけて)作成すればいい・・・のだが
詳しくはわからないので誰か書いてクレメンス

**透明のpngを正則化画像にする
Web UI に拡張機能をインストールする [[https://github.com/hunyaramoke/Generate-TransparentIMG>>https://github.com/hunyaramoke/Generate-TransparentIMG]]
Generate TransparentIMG タブで
出力フォルダ:正則化画像の保存先
number_of_generation:作成する枚数
を入力して実行
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/l8x2Fjrdz7.png)
[END]

*その他
lora_train_command_line.py のパラメータ記入の補助ツール
GradioベースのGUI [[https://github.com/bmaltais/kohya_ss>>https://github.com/bmaltais/kohya_ss]]
「Tools」タブにフォルダ配置補助機能がある。
スクリプト版のパラメータの一部は設定出来ないかも？

*Loraの使用方法&aname(apply_lora)
**使い方その1 WebUIに拡張機能をインストールして使う
-「拡張機能」タブの「URLからインストール」に https://github.com/kohya-ss/sd-webui-additional-networks を入力してインストール )~~
H:\stablediffusion\stable-diffusion-webui\extensions\sd-webui-additional-networks\models\lora フォルダに 出来上がった .pt や .safetensorsをコピーする
(Web UI の 「設定」> 「Additional Nerwork」タブでフォルダの場所を追加出来る)
「txt2img」や「img2img」の画面の左下の方に「Additional Networks ▼」が追加されているので~~Enable を押してmodelを選びmerge倍率をweightのスライダーで調整する

**使い方その2 WebUIの本体機能のみで使う
-H:\stablediffusion\stable-diffusion-webui\models\lora に拾った .pt や .safetensorsをコピーする
「txt2img」や「img2img」の「生成」ボタンの下の花札みたいなマーク(&#127924;)を押すと
Texutual Inversion, Hypernetworks, Lora の3つのタブが出るので Lora を選択して
一覧から選ぶと <lora:ファイル名:倍率>みたいなタグがプロンプトに追加される
最新のLora(0.4.0)で作ったやつも使える

