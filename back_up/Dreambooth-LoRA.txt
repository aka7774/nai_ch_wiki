* 概要
Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning
雑に言うとDreamboothの小さくて速いやつ
*参考資料
https://rentry.org/lora_train
クラウドGPUを使う場合はリンク先の下の方に Colab Instructions がある
フォルダ命名方法に気をつけて、自前のファイルは半角スペース一切入れないようにすれば無料Colabでも回せる。頑張れ。

*他人の作ったモデルの導入
[[学習の手順の最後>#apply_lora]]だけやればOK

*あかちゃんLoraインストーラー(みかんせい)
- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts.bat
- あかちゃんインストーラーで1111を入れた人向けにPYTHONとGITのPATHをいじってあるやつ
- うちの環境ではMemoryErrorが出て動かなかった
- lora_train_popup.pyもワイの知能では使いこなせなかった
- だれかなおちて

上より引用
*準備
- git で https://github.com/kohya-ss/sd-scripts をコピー
- https://github.com/derrian-distro/LoRA_Easy_Training_Scripts から lora_train_popup.py, lora_train_command_line.py をダウンロード 注) 「リンク先をファイルに保存」 ではなく リンク先に飛んでコードの右上の RAW ボタンを押してメモ帳みたいなテキストばっかりの画面を出して 「名前をつけてページを保存」
- スクリプトを sd-scripts フォルダにコピー
- README-ja.md に日本語の詳しい説明が書いてある
- 初回セットアップ
--sd-scripts フォルダで PowerShellかターミナルを開く (Shiftを押しながら右クリック)
--以下のコマンドを順番に入力する(コピペでOK)
=|PERL|
python -m venv --system-site-packages venv
.\venv\Scripts\activate
 
pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
pip install --upgrade -r requirements.txt
pip install -U -I --no-deps https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/f/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl

cp .\bitsandbytes_windows\*.dll .\venv\Lib\site-packages\bitsandbytes\
cp .\bitsandbytes_windows\cextension.py .\venv\Lib\site-packages\bitsandbytes\cextension.py
cp .\bitsandbytes_windows\main.py .\venv\Lib\site-packages\bitsandbytes\cuda_setup\main.py
 
accelerate config
||=
エラーが出て進めないときは python -m venv venv と書き換えてもう一回やってみる コメント欄参照
--accelerate config のあとに質問が出るので以下のように答える
=|PERL|
- This machine
- No distributed training
- NO
- NO
- NO
- all
- fp16
||=
--初回セットアップ完了
*学習の手順
-1. sd-scripts フォルダのrun_popup.batを実行
-1' linuxやクラウドGPUニキは
sd-scripts ディレクトリ で venv/bin/activate~~次のコマンドどちらかを入力
=|PERL|
accelerate launch --num_cpu_threads_per_process 12 lora_train_popup.py
accelerate launch --num_cpu_threads_per_process 12 lora_train_command_line.py
||=
-2 ポップアップにパラメーターを入力する(lora_train_command_line.py にはあらかじめパラメーターを書いとく)
-3 出来上がりを待つ
-4 WebUIで使うには 「拡張機能」タブの「URLからインストール」に https://github.com/kohya-ss/sd-webui-additional-networks を入力してインストール &aname(apply_lora)~~
stable-diffusion-webui\models\lora フォルダに 出来上がった .pt や .safetensorsをコピー ~~(Web UI の 「設定」> 「Additional Nerwork」タブでフォルダの場所を追加出来る)
-使い方1「txt2img」や「img2img」の画面の左下の方に「Additional Networks ▼」が追加されているので~~Enable を押してmodelを選びmerge倍率をweightのスライダーで調整する
-使い方2「txt2img」や「img2img」の「生成」ボタンの下の花札みたいなマークを押すと
Texutual Inversion, Hypernetworks, Lora の3つのタブが出るので Lora を選択して
一覧から選ぶと <lora:ファイル名:倍率>みたいなタグがプロンプトに追加される 
**手順の画像
クリックして展開 アップデートなどで内容は変わる
わからんパラメータが出たらcancelを押しとけばデフォルト値が入る。抜けがあったらスレで質問よろ。
[+]
**ターミナルとかパワーシェルにコマンドを打つ
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/yVzXCg9crC.png)
----
**学習元のモデルを選ぶ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/jxCNF4YB2L.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/6tebwsnoiv.png)
----
**学習用画像のフォルダを選ぶ
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/QsMm7_wdmg.png)
数字_名前 フォルダが見えるように
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/BC_G2nQ6O4.png)
----
**出力先のフォルダを選ぶ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/d_LHSt9yQv.png)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/Pm3ilZIP1a.png)
----
**正則化画像のあるフォルダを選ぶ 使わないときは「いいえ」
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/KXO6r5NWEn.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/e99bPUOmNk.png)
数字_名前 フォルダが見えるように
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/OYsxnrbqxq.png)
----
**バッチサイズ:一度に何枚処理するか つよつよGPU以外なら1で
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xcTV3i_Lbh.png)
**何エポック学習させるか: 1エポックは 繰り返し回数(フォルダの先頭の数字)×学習用画像の枚数 ステップ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DWVNJ2DpMd.png)
**dimサイズ: みんな128をつかっとるらしい
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/I75XLmK3_t.png)
**学習の解像度: 512で。RTX4090か超強クラウドGPUなら 768 もいける
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ttb3qyi0Ya.png)
----
**学習率(Learning Rate): 1e-4 (= 0.0001) これより上げることはあんまりない。 5e-5 (=0.00005)くらいでもいいかも
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DkV4RQ4ZnK.png)
**スケジューラー: cosine_with_restarts で(よく分からんのでいじらない) 学習率を途中で上げ下げするやり方
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/0jBUxX8eVo.png)
**エポック単位でセーブする?: 2エポック以上学習させるなら
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/M6Jh5Uc7cL.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ZCqzxdoZWw.png)
----
**キャプションをシャッフルする?: する
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/IEAf9r1tqK.png)
**キャプションの最初のトークンを保持する?: 
キャプションを付けた場合フォルダ名のインスタンスプロンプトが無効になる~~のでキャプションファイルの先頭にインスタンスプロンプトを自分で書く必要がある。~~インスタンスプロンプトが消費するトークン数を次のkeep at front? に入力するとシャッフルされないらしい。 コメント欄に詳しく書いてある  
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/kMrIwfMsJE.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/SKOgINqr6w.png)
**warmup ratio 使う?: 学習の最初だけ学習率を下げる機能
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/WaHG0v8I5O.png)
----
**学習の様子 縦横の比率は自動で振り分けしてくれる
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/9VizQs0l1c.png)
**出来上がり
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/y3w3sgkzGd.png)
last.safetensors というファイルができる
このスクリプトではログは一切残らないのでわかりやすい名前にリネームしておく
追加学習するときはこのファイルを指定する
[END]
*学習用画像を置くフォルダの配置
-作者の解説が詳しい [[https://note.com/kohya_ss/n/nba4eceaa4594>>https://note.com/kohya_ss/n/nba4eceaa4594]]
-フォルダの配置例:
[-]
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/8v9xToIuUR.png)
[END]
-同時に10まで概念を学習できるが、少なくとも1つはフォルダが必要。
-フォルダの名前は <繰り返し回数>_<インスタンスプロンプト>
--<繰り返し回数> 繰り返し回数×学習用画像の枚数を1セット(1 epoch)として学習する 
※注 学習用の画像が50枚ある場合、繰り返し回数を20 にすると 20 x 50 = 1000 ステップ学習する
--<インスタンスプロンプト> 呼び出し用のキーワード 英単語にない意味のないワードがよい 
--キャプション ファイルは必須です。そうでない場合、LoRA は概念名をキャプションとして使用してトレーニングを行います。
--キャプションについては以下

*キャプションを付ける
作者の詳しい画像付き説明 [[https://github.com/kohya-ss/sd-scripts/blob/main/fine_tune_README_ja.md>>https://github.com/kohya-ss/sd-scripts/blob/main/fine_tune_README_ja.md]]
自動でつけてくれる・・・のではなく一応コマンド打たないとダメらしい。詳しくは上記
**WD1.4 Taggerで作成
先に学習用画像を連番にリネームしておく (01.png, 02.png, ...など)
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/kbCJ6oEi6E.png)
[END]
Web UI に拡張機能 stable-diffusion-webui-wd14-tagger [[https://github.com/toriato/stable-diffusion-webui-wd14-tagger>>https://github.com/toriato/stable-diffusion-webui-wd14-tagger]]をインストール
「Tagger」タブの「Batch from directly」
-入力ファイル:学習用画像の入っているフォルダ
-Interrogator:wd-14convnext
-アンダースコアの代わりにスペースを使用する:オン
-括弧をエスケープする:オン
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/DGlOwfa65F.png)
[END]
Interrogateを押すと学習用画像のフォルダにタグの付いた .txt ファイルが生成される
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/vrFirugtW9.png)
[END]
**キャプションの細かい加工
https://github.com/starik222/BooruDatasetTagManager や スレ130の>>346>>949のツールで
余計なタグを削ったりこだわりのタグ追加をする
*そもそも学習用画像ってどうやって加工するの
本文で説明している kohya_ss 版のLoRAではトリミングはしなくていい(画像のサイズ別に学習が行われる)
背景の切り抜きは・・・画像の大きさが揃ってないとめんどくさいなどうしよう・・・

キャラの切り出しだけやったら3Dペイント(Win10なら標準、11では標準からリストラされたけどストアにおるで)のマジック選択でええ感じに切り抜きやすいからそこからgimpなりで微調整。
一枚一枚やんのめんどくさい言うんやったらABG_extension言うのが出たんでつこてみたらええんとちゃうかな…？しらんけど
*正則化画像
キャプションつけたらそのプロンプトで学習させるモデルを使って(適当なネガティブプロンプトをつけて)作成すればいい・・・のだが
詳しくはわからないので誰か書いてクレメンス
**透明のpngを正則化画像にする
Web UI に拡張機能をインストールする [[https://github.com/hunyaramoke/Generate-TransparentIMG>>https://github.com/hunyaramoke/Generate-TransparentIMG]]
Generate TransparentIMG タブで
出力フォルダ:正則化画像の保存先
number_of_generation:作成する枚数
を入力して実行
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/l8x2Fjrdz7.png)
[END]
*慣れたら
lora_train_command_line.py をコピーしてパラメーターを直書き
GradioベースのGUI [[https://github.com/bmaltais/kohya_ss>>https://github.com/bmaltais/kohya_ss]]
「Tools」タブにフォルダ配置補助機能がある。スクリプト版のパラメータの一部は設定出来ないかも？



