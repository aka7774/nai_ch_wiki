#contents
* 概要
Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning
簡単に言えば「省メモリで高速に学習できて容量も小さくて済む追加学習法」。''作成方法はいろいろある。''

[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/IgjGWbu1Yn.png,400)>https://image02.seesaawiki.jp/n/h/nai_ch/IgjGWbu1Yn.png]]
他の学習法とどう違うねん？　reddit民によればこんな感じのイメージらしい。
https://www.reddit.com/r/StableDiffusion/comments/10cgxrx/wellresearched_comparison_of_training_techniques/
kohya_ss版sd-scriptsの登場以来、sd-scripts及びそれの派生ツールが人気となっている。
このページではsd-scripts関連の情報について雑に書いてある

*参考資料・スレ住民による学習ガイド
LoRA Training Guide　https://rentry.org/lora_train
- 4chan有志によるLoRAトレーニング法ガイド（英語）
LoRA 学習メモ　https://rentry.co/i5ynb
- スレ住民によるLain・よしなが先生・野原ひろしLoRA作成者によるLoRAガイド（日本語）
ソウリンちゃんLoRAの作成記録 https://rentry.co/sourin_chan
- スレ住民によるマルゼン式(ふたば有志のタグ付け手法の1つ)で作成したLoRA作成記録（日本語）
Genshin Impact LoRA作成メモ https://rentry.org/genshin_lora
- スレ住民によるkohya-ss氏制作のSDスクリプト(https://github.com/kohya-ss/sd-scripts )で次のキャラのLoRAを作成した。ポップアップ版使用。（日本語）
https://rentry.org/lora-tag-faq
- lora training tagging faq（英語）
https://rentry.org/dsvqnd
- スレ住民によるキャラクター学習のタグ付け一例（日本語）
https://rentry.org/lora_namakubi
- スレ住民によるLoRAでのキャラ学習素材の検証
あかちゃんLoRAノートブック [[kohya_train_network_simple]]
- 全然スレに書き込めないけどけなげに頑張っている

- クラウドGPUを使う場合はリンク先の下の方に Colab Instructions がある
- フォルダ命名方法に気をつけて、自前のファイルは半角スペース一切入れないようにすれば無料Colabでも回せる。頑張れ。

*他人の作ったモデルを使いたい。
最新版のWEBUIが既に使用可能な状態ならセットアップ不要→[[LoRAの使用方法>https://seesaawiki.jp/nai_ch/d/Dreambooth-LoRA#content_13]]へ

*インストール、初回セットアップ編
1. エクスプローラーを開き、適当なフォルダ内で右クリック→git bash hereでgitのターミナルを開き、以下のコマンドを実行する
> git clone https://github.com/kohya-ss/sd-scripts.git
2. https://github.com/derrian-distro/LoRA_Easy_Training_Scripts から~~ lora_train_popup.py~~ lora_train_command_line.py~~ run_popup.bat~~ run_command_line.bat~~をダウンロード~~注) 「リンク先をファイルに保存」 ではなく リンク先に飛んでコードの右上の RAW ボタンを押してメモ帳みたいなテキストばっかりの画面を出して 「名前をつけてページを保存」
あるいはCode→Download zipでダウンロードして解凍する(zipに入ってる.gitignore、LICENSE、README.mdは不要)

3. スクリプトを sd-scripts フォルダにコピー

4. README-ja.md に日本語の詳しい説明が書いてあるので読む
=|PERL|
 train_network_README-ja.md: LoRAの学習について
 train_db_README-ja.md:DreamBoothのガイドです。LoRA等の追加ネットワークの学習にも同じ手順を使います。
 fine_tune_README_ja.md:キャプションとか
||=
- 初回セットアップ
--Python初めての場合、PowerShell初めての場合
=|PERL|
PowerShellを管理者として開きます。
「Set-ExecutionPolicy Unrestricted」と入力し、Yと答えます。
管理者のPowerShellを閉じます。
||=
--sd-scripts フォルダで PowerShellかターミナルを開く (Shiftを押しながら右クリック)
--以下のコマンドを順番に入力する(コピペでOK)
=|PERL|
python -m venv venv
.\venv\Scripts\activate
 
pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
pip install --upgrade -r requirements.txt
pip install -U -I --no-deps https://github.com/C43H66N12O12S2/stable-diffusion-webui/releases/download/f/xformers-0.0.14.dev0-cp310-cp310-win_amd64.whl

cp .\bitsandbytes_windows\*.dll .\venv\Lib\site-packages\bitsandbytes\
cp .\bitsandbytes_windows\cextension.py .\venv\Lib\site-packages\bitsandbytes\cextension.py
cp .\bitsandbytes_windows\main.py .\venv\Lib\site-packages\bitsandbytes\cuda_setup\main.py
 
accelerate config
||=
なお、python -m venv〜の行で「python」とだけ表示された場合、py -m venv〜のようにpythonをpyに変更してください。

--accelerate config のあとに質問が出るので以下のように答える
=|PERL|
- This machine
- No distributed training
- NO
- NO
- NO
- all
- fp16
||=
--初回セットアップ完了

*あかちゃんLoraインストーラー

あかちゃんインストーラーで1111を入れた人向けにPYTHONとGITのPATHをいじってあるやつ
start.batと同じフォルダに入れて実行してください

- コマンドライン用
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts.bat
-- https://github.com/aka7774/elemental_code/blob/main/tools/run_sd_scripts.bat
- ダイアログ用(みかんせい)
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts_easy_training.bat

*LoRA_Easy_Training_Scripts Installers v1
-https://github.com/derrian-distro/LoRA_Easy_Training_Scripts/releases/tag/installers-v1
EasyTrainScriptsの人が作った簡易インストールスクリプト
sd-scripts用とkohya-ss用の2種がある。gitがセットアップ済みなら右クリック＞管理者として実行するだけ。
sd-scripts用の場合、sd-scripts本体とEasy_Train_Scriptsの両方をインストールして、インストール後の初期設定までやってくれる。

*学習用画像を置くフォルダの配置
-作者の解説が詳しい [[https://note.com/kohya_ss/n/nba4eceaa4594>>https://note.com/kohya_ss/n/nba4eceaa4594]]

-フォルダの配置例:
[-]
※要するに<繰り返し回数>_<インスタンスプロンプト>にリネームした学習画像データのフォルダは直接指定しないでねって話
例えば↓こういうこと
&#10060;E:\kohya_ss\TrainDatas\001\img\40_kdy 1girl
&#128994;E:\kohya_ss\TrainDatas\001\img
間違うと画像が見つかりませんと怒られる

&ref(https://image02.seesaawiki.jp/n/h/nai_ch/8v9xToIuUR.png)
[END]
-同時に10まで概念を学習できるが、少なくとも1つはフォルダが必要。
-フォルダの名前は <繰り返し回数>_<インスタンスプロンプト>
--<繰り返し回数> 繰り返し回数×学習用画像の枚数を1セット(1 epoch)として学習する 
※注 学習用の画像が50枚ある場合、繰り返し回数を20 にすると 20 x 50 = 1000 ステップ学習する
--<インスタンスプロンプト> クラス 呼び出し用のキーワード クラスは''英単語にない意味のないワード''がよい 
-- 上記kohya氏のサンプルだと「20_sls frog」　脳死で真似するなら 繰り返し回数_意味のないワード WEBUIでプロンプトとして書きたい単語 で設定しておく
--キャプション ファイルは必須です。そうでない場合、LoRA は概念名をキャプションとして使用してトレーニングを行います。
--キャプションについては以下

*キャプション・タグを付ける
- 作者の詳しい画像付き説明 [[https://github.com/kohya-ss/sd-scripts/blob/main/fine_tune_README_ja.md>>https://github.com/kohya-ss/sd-scripts/blob/main/fine_tune_README_ja.md]]
- 学習用の素材画像それぞれに内容を説明するテキストファイルを作る。このテキストファイルには画像生成時のプロンプトと同じようにタグを記載する。
- テキストエディターやメモ帳で１つずつ作っても良いのだが、WD1.4Tagger等のツールを使えば一気に自動生成できて捗る

**WD1.4 Taggerで作成
先に学習用画像を連番にリネームしておく (01.png, 02.png, ...など)
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/kbCJ6oEi6E.png)
[END]
Web UI に拡張機能 stable-diffusion-webui-wd14-tagger [[https://github.com/toriato/stable-diffusion-webui-wd14-tagger>>https://github.com/toriato/stable-diffusion-webui-wd14-tagger]]をインストール
「Tagger」タブの「Batch from directly」
-入力ファイル:学習用画像の入っているフォルダ
-Interrogator:wd-14convnext
-アンダースコアの代わりにスペースを使用する:オン
-括弧をエスケープする:オン
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/DGlOwfa65F.png)
[END]
Interrogateを押すと学習用画像のフォルダにタグの付いた .txt ファイルが生成される
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/vrFirugtW9.png)
[END]

**キャプション・タグの編集
- タグは順序に影響を受けるので、一番最初に有効化したいタグを記述する
- WD1.4Tagger等で自動生成したファイルには不要なタグが含まれたり誤認識されたタグが記載されたりするので編集する。

- BooruDatasetTagManager https://github.com/starik222/BooruDatasetTagManager 
- 学習用タグの入力を速く楽にするやつ →[[ローカルの「ツール」]] https://uploader.cc/s/rdw0k6qd2766czgdwwwjtn2xtmhiay6c1ky0s7dui4o5yaz0pkgfesef18n9nngm.zip
等の便利なツールを使えば捗る。必要なタグを追加、不要なタグの削除、順序の入れ替え等の編集をやる

- taggerで生成したタグの順序のままでも構わないが、重要なタグだけ各ファイルの先頭の方に記載する。例えばコマンドライン版（lora_train_command_line.py ）の場合、
>        self.shuffle_captions: bool = True  # OPTIONAL, False to ignore ~~
>        self.keep_tokens: Union[int, None] = 3  # OPTIONAL, None to ignore ~~
上記のように設定すれば先頭から3つのタグは順序固定として残りはタグの適当にシャッフルして学習できる。

**キャプションの付け方・考え方の参考サイト
-[[lora training tagging faq>https://rentry.org/lora-tag-faq]]
英語サイトだがブラウザの翻訳で読もう
-[[キャラクター学習のタグ付け一例>https://rentry.org/dsvqnd]]
実例を挙げての解説

一言で言えば「呼び出しキーワード」＋「学習から外したいもの」をタグに書く

*そもそも学習用画像ってどうやって加工するの
- 本文で説明している kohya_ss 版のLoRAではトリミングはしなくていい(画像のサイズ別に学習が行われる)
- 背景の切り抜きは・・・画像の大きさが揃ってないとめんどくさいなどうしよう・・・
- キャラの切り出しだけやったら3Dペイント(Win10なら標準、11では標準からリストラされたけどストアにおるで)のマジック選択でええ感じに切り抜きやすいからそこからgimpなりで微調整。
- 一枚一枚やんのめんどくさい言うんやったらABG_extension言うのが出たんでつこてみたらええんとちゃうかな…？しらんけど

ABG_extension
https://github.com/KutsuyaYuki/ABG_extension
WEBUI公式extension 背景を自動で除去します。アニメ画像用に微調整されたonnxモデルを使用。GPUで動作します。

katanuki
https://github.com/aka7774/sd_katanuki
WEBUI用exntension anime-segmentation を 1111 で使えるようにしたやつ。画像の背景を透過したり白背景にしたりマスク画像を出力する


*正則化画像
- キャプションつけたらそのプロンプトで学習させるモデルを使って(適当なネガティブプロンプトをつけて)作成すればいい・・・のだが詳しくはわからないので誰か書いてクレメンス
- 間違っとる可能性大なのやが、例えばAIちゃんが知らない「鳥獣戯画のカエルちゃん」のイメージを教えるとする。学習用画像には「鳥獣戯画のカエルちゃん」画像を用意する。正則化画像にはありふれた「蛙の画像」を用意する。これでAIちゃんには「鳥獣戯画のカエルちゃん覚えようね！でも正則化画像フォルダにある普通の蛙とかは違うやつやから覚えなくていいよ」という感じで伝わる。イメージを覚えてもらうのに言葉では説明しづらいから画像で説明する感じ？多分。知らんけど。
- 正則化画像は必須ではないので用意しなくても学習はできる。とりあえず一度学習動かしてみたいとかなら用意しなくてもいい。

**透明のpngを正則化画像にする
Web UI に拡張機能をインストールする [[https://github.com/hunyaramoke/Generate-TransparentIMG>>https://github.com/hunyaramoke/Generate-TransparentIMG]]
Generate TransparentIMG タブで
出力フォルダ:正則化画像の保存先
number_of_generation:作成する枚数
を入力して実行
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/l8x2Fjrdz7.png)
[END]

*学習の手順
**Windowsの場合
***ポップアップ版を使う場合
+ run_popup.batを実行
+ ポップアップにパラメーターを入力する
+ 出来上がりを待つ

***コマンドライン版を使う場合
+ lora_train_command_line.py にパラメーターを書く
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/C1tEUj_9N8-s.png)>https://image01.seesaawiki.jp/n/h/nai_ch/C1tEUj_9N8.png]]
設定を書き込むのはlora_train_command_line.pyの最初の方あたり。学習ベースになるモデル、学習素材フォルダの場所、出力先は必ず設定する。わからんところはそのままにしとく。
以下lora_train_command_line.py冒頭あたりの設定部分の雑な日本語訳
[+]
=|BOX|
class ArgStore:
    # sd スクリプトのすべての可能な入力全体を表します。 重要度の高いものから順に並べられています
    def __init__(self):
        # 重要 このあたりは変更する可能性が最も高いやつ
        self.base_model: str = r""  # 学習させるベースモデルの場所を右みたいに書いてな r"E:\sd\stable-diffusion-webui\models\Stable-diffusion\nai.ckpt"
        self.img_folder: str = r""  # 学習させる素材画像フォルダの場所書いてな　下記のガイドラインに添って配置してな
                                    # これがフォルダ配置ガイドや: https://rentry.org/2chAI_LoRA_Dreambooth_guide_english#for-kohyas-script
        self.output_folder: str = r""  # 出力先のフォルダをここに設定するやで。学習途中のやつも最終結果もここに出すで
        self.change_output_name: Union[str, None] = None  # 出力ファイル名を変更する？
        self.save_json_folder: Union[str, None] = None  # オプション、設定の json フォルダーをここで設定した場所に保存します。
        self.load_json_path: Union[str, None] = None  # オプション、json ファイルをロードすると、構成が一致するように部分的に変更されます。
        self.json_load_skip_list: Union[list[str], None] = None  # ユーザーがjsonをロードするときにスキップするものを定義できるようにします,
                                                                 # 重要: デフォルトでは、すべてのパスを含むすべてをロードします。
                                                                 # 除外する形式は次のようになります: ["base_model", "img_folder", "output_folder"]
        self.multi_run_folder: Union[str, None] = None  # オプション、スクリプトによって生成された json を含むフォルダーに設定すると、それらのスクリプトを使用してトレーニングが開始されます。
                                                        # すべてが確実にロードされるように、json_load_skip_list を無視することに注意してください。
                                                        # 重要: これにより、ここで設定されたすべてのパラメーターも無視され、代わりに json ファイル内のすべてのパラメーターが使用されます。
        self.save_json_only: bool = False  # トレーニングを行わずに json を生成したい場合は true に設定

        self.net_dim: int = 128  # ネットワーク dim、128 が最も一般的ですが、これよりも少ない値で動作する可能性があります
        self.alpha: float = 64  # 学習用のスカラーを表す。アルファ値が低いほど、1ステップあたりの学習量は少なくなる
                                # 旧来の方法で学習させたい場合は、dimと同じ数値に設定する
        # スケジューラのリスト: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup
        self.scheduler: str = "cosine_with_restarts"  # 学習率に関するスケジューラ。それぞれ特定の処理を行う
        self.cosine_restarts: Union[int, None] = 1  # オプション,  再起動する回数を表す. cosine_with_restartsを使っている場合のみ重要。
        self.scheduler_power: Union[float, None] = 1  # オプション, 多項式のべき乗を表します。多項式を使用している場合のみ重要。
        self.warmup_lr_ratio: Union[float, None] = None  # オプション, 与えられた比率に基づいて，ウォームアップのステップ数を計算する．
                                                         # constant_with_warmupを使用している場合は必ず設定してください。
                                                         # 
        self.learning_rate: Union[float, None] = 1e-4  # オプション,  設定しない場合、adamWのようにlrは1e-3に設定される。個人的には、lrが低い方が少し良さそうなので、実際に設定することをお勧めします。
        self.text_encoder_lr: Union[float, None] = None  # オプション, テキストエンコーダの特定のlrを設定する、これはベースlrを上書きすると思う。
        self.unet_lr: Union[float, None] = None  # オプション, unetに特定のlrを設定、これはベースlrを上書きすると思います。 無視する場合はNone
        self.num_workers: int = 1  # 画像の読み込みに使用されるスレッドの数、低いと高速化される。
                                   # エポックの開始は速くなるが、データのロードは遅くなる。ここでの仮定は
                                   # この値を小さくすると学習時間が長くなる # と想定している。
        self.persistent_workers: bool = True  # ワーカーを永続化させ、エポック間の遅延をさらに減らす/なくす。

        self.batch_size: int = 1  # 一度に処理される画像の枚数。
                                  # 12gbのVRAMで512レゾの場合、最大6バッチサイズになります。
        self.num_epochs: int = 1  # エポック数、もし最大ステップ数を設定した場合、この値はステップ数を計算しないので無視される。
        self.save_at_n_epochs: Union[int, None] = 1  # オプション, エポックごとに保存する頻度を設定、Noneと書くと保存しない。
        self.keep_tokens: Union[int, None] = None  # オプション, 先頭に書いたトークンをキープするかどうか。Noneと書くと何もしない
        self.max_steps: Union[int, None] = None  # オプション, ステップ数を決めている場合、直接設定することができる。設定しない場合はNoneと書く

        # このあたりからは猛者は変えるかもしれない設定
        self.train_resolution: int = 512
        self.min_bucket_resolution: int = 320
        self.max_bucket_resolution: int = 960
        self.lora_model_for_resume: Union[str, None] = None  # オプション, 学習を継続するための入力ローラを受け取る。
                                                             # 正確にはそうあるべきでないが、動作する。
        self.save_state: bool = False  # OPTIONAL, 学習状態を保存して学習を継続するためのもの, Falseは無視する。
        self.load_previous_save_state: Union[str, None] = None  # オプション, トレーニングの状態をロードして継続的なトレーニングに利用する、設定しないならNone
        self.training_comment: Union[str, None] = None  # オプション, アクティベーショントークンのようなものを
                                                        # メタデータに入れるには最適な方法。
        self.unet_only: bool = False  # OPTIONAL, unetだけを学習させるように設定する。
        self.text_only: bool = False  # OPTIONAL, テキストエンコーダの学習のみを行うように設定する。

        # ここからはマジでガチ勢向けの設定
        self.reg_img_folder: Union[str, None] = None  # オプション, 設定しない場合はNoneと書く
        self.clip_skip: int = 2  # アニメ系のモデルで学習する場合は、ほとんどのモデルがそのように設計されているので、この値を2にしておく。
        self.test_seed: int = 23  # これは「再現可能なシード」であり、基本的にこのシードに設定すれば、
                                  # 学習用画像からプロンプトを入力し、それに近い表現を得ることができるはずである。
        self.prior_loss_weight: float = 1  # これはDreamboothと同じように、LoRAの学習に必要な損失重み付けである。
        self.gradient_checkpointing: bool = False  # オプション, グラデーションのチェックポイントを有効にする．
        self.gradient_acc_steps: Union[int, None] = None  # オプション, ワイも実際何かわからんけど設定できるようにしといた
        self.mixed_precision: str = "fp16"  # もしbf16を使えるなら使ったほうがいい。
        self.save_precision: str = "fp16"  # bf16でも保存できるが、汎用的ではないので、fp16で保存しておくことをお勧めします。
        self.save_as: str = "safetensors"  # pt, ckpt, safetensorsのどれかで保存できるよ
        self.caption_extension: str = ".txt"  # .captions,形式も使えるけどwd1.4taggerはtxtで出力するから、txtをデフォルトとする。
        self.max_clip_token_length = 150  #  75, 150, または225にすることができると思う。
        self.buckets: bool = True
        self.xformers: bool = True
        self.use_8bit_adam: bool = True
        self.cache_latents: bool = True
        self.color_aug: bool = False  #  重要: ccache_latents と衝突するので、どちらか一方だけをオンにすること!
        self.flip_aug: bool = False
        self.vae: Union[str, None] = None  #  特定のVAEを使わない場合、結果を悪化させるだけのようなので、おそらく使用しない方がよいでしょう。
        self.no_meta: bool = False  # safetensorsに保存されるメタデータが削除されます(これは残しておく必要があります)。
        self.log_dir: Union[str, None] = None  # ログ出力する？ほとんどの人にとって有益なものではありません。.

    # 残りのコードで使用される dict を作成し、json の保存と読み込みを容易にします。
    @staticmethod
    def convert_args_to_dict():
        return ArgStore().__dict__
||=
[END]
+ run_command_line.batを実行
+ 出来上がりを待つ

**Linux(wslやクラウドGPUニキ)の場合

***ポップアップ版を使う場合
+ source venv/bin/activate と入力
+ accelerate launch --num_cpu_threads_per_process 12 lora_train_popup.py と入力
+ ポップアップにパラメーターを入力する
+ 出来上がりを待つ

***コマンドライン版を使う場合
+ lora_train_command_line.py にパラメーターを書く
+ source venv/bin/activate と入力
+ accelerate launch --num_cpu_threads_per_process 12 lora_train_command_line.py　と入力
+ 出来上がりを待つ

**Lora作成手順の画像 (ポップアップ版) 参考程度 (2023-1-16時点)
クリックして展開 アップデートなどで内容は変わる
わからんパラメータが出たらcancelを押しとけばデフォルト値が入る。抜けがあったらスレで質問よろ。
[+]
**ターミナルとかパワーシェルにコマンドを打つか、run_popup.batから実行する
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/yVzXCg9crC.png)
----
**設定ファイルを読み込む? (前と同じ設定を使いたければ次でjsonファイルを読み込む)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ceVWQWf9Uj.png)
----
**学習元のモデルを選ぶ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/jxCNF4YB2L.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/6tebwsnoiv.png)
----
**学習用画像のフォルダを選ぶ
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/QsMm7_wdmg.png)
数字_名前 フォルダが見えるように
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/BC_G2nQ6O4.png)
----
**出力先のフォルダを選ぶ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/d_LHSt9yQv.png)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/Pm3ilZIP1a.png)
----
**設定をjson形式で保存する?
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/1LwRQpGmTB.png)
----
**正則化画像のあるフォルダを選ぶ 使わないときは「いいえ」
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/KXO6r5NWEn.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/e99bPUOmNk.png)
数字_名前 フォルダが見えるように
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/OYsxnrbqxq.png)
----
**学習を再開する? (以前の続きをやるときはsafetensorファイルなどを次で読み込む)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/7Vg8xAPetT.png)
----
**バッチサイズ:一度に何枚処理するか VRAM12Gなら8くらいいける(解像度512に限る)で
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xcTV3i_Lbh.png)
**何エポック学習させるか: 1エポックは 繰り返し回数(フォルダの先頭の数字)×学習用画像の枚数 ステップ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DWVNJ2DpMd.png)
**dimサイズ: ケモナーは128推奨
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/I75XLmK3_t.png)
**アルファ: dimサイズと同じがいいらしい。下げたら学習率に注意
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/vZVRRTw9ES.png)
**学習の解像度: 512が速い, 768ならRTX3060やColabで10000ステップ4~5時間コース
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ttb3qyi0Ya.png)
----
**学習率(Learning Rate): 1e-4 (= 0.0001)くらいで。alphaを1にした場合dimの分学習率が割られるらしいので上げ目にする 1e-3程度
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DkV4RQ4ZnK.png)
**スケジューラー: cosine_with_restarts で(よく分からんのでいじらない) 学習率を途中で上げ下げするやり方
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/0jBUxX8eVo.png)
**エポック単位でセーブする?: 2エポック以上学習させるなら
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/M6Jh5Uc7cL.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ZCqzxdoZWw.png)
----
**キャプションをシャッフルする?: する
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/IEAf9r1tqK.png)
**キャプションの最初のトークンを保持する?: 
キャプションを付けた場合フォルダ名のインスタンスプロンプトが無効になる~~のでキャプションファイルの先頭にインスタンスプロンプトを自分で書く必要がある。~~作者のnoteによると「数値を指定するとキャプションの先頭から、指定した数だけのトークン（カンマ区切りの文字列）をシャッフルせず固定します。」~~キャプションの先頭からカンマ区切りで判定されるので 「zkz, 1girl, condom, ass, solo, black panties, one side up,」なら1でおk
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/kMrIwfMsJE.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/SKOgINqr6w.png)
**warmup ratio 使う?: 学習の最初だけ学習率をあげる機能
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/WaHG0v8I5O.png)
----
**学習の様子 縦横の比率は自動で振り分けしてくれる
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/9VizQs0l1c.png)
**出来上がり
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/y3w3sgkzGd.png)
last.safetensors というファイルができる
追加学習するときはこのファイルを指定する
[END]
動画(2023-01-30) すぐにアプデで役に立たなくなるが一応
字幕がめんどいのでそのうちテキストで書く・・・とおもう
わからんパラメータは キャンセルでデフォルト値が入る
[+]
&video(https://image02.seesaawiki.jp/n/h/nai_ch/Kr6H7y5ZXq.webm)&video(https://image01.seesaawiki.jp/n/h/nai_ch/ypwjUxQC6D.webm)
[END]
ポップアップの質問と答えの例
[+]
|英文|訳|バッチファイルのデフォルト値|コメント|
|Do you want to load a json config file?|jsonファイルから前の設定読み込む?||1回設定セーブしとかないとダメ|
|Select your base model|学習元のモデルを選ぶ|||
|Select your image folder|学習用画像のフォルダを選ぶ||数字が先頭についているフォルダの上|
|Select your output folder|(loraの)出力先のフォルダを選ぶ|||
|Do you want to save a json of your configuration?|jsonファイルに設定を保存する?|||
|How many workers do you want? 〜||8|よくわからん|
|Do you want to use regularisation images?|正則化画像を使う?|||
|Select your regularisation folder|正則化画像のフォルダを選ぶ||数字が先頭についているフォルダの上|
|Do you want to continue from an earlier version?|前回のつづきから学習する?||学習を中断した場合続きから再開できる|
|How large is your batch size going to be|バッチサイズをいくつにする?|1|VRAMに余裕があれば2〜8|
|How many epochs do you want?|何エポック学習する?|1|多くすると学習回数が増える|
|What is the dim size you want to use?|loraのランク(network dim)をいくつにする?|128|数が多いほど表現力は増すが時間、メモリ、ファイルサイズも増える 4〜128|
|What Alpha do you want?|alpha値をいくつにする?|network dimと同じ|alpha値を1にした場合学習率をあげたほうがいいらしい|
|How large of a resolution do you want to train at?|学習の解像度をいくつにする?|512|768や1024も指定できるがmax_bucket_resolutionも書き換え必要|
|What learning rate do you want to use?|学習率をいくつにする?|1e-4|alpha値を1にした場合1e-3くらいにあげたほうがいい|
|Do you want to set the text_encoder_lr?|テキストエンコーダの学習率を設定する?||参考値 5e-5|
|Do you want to set the unet_lr?|U-Netの学習率を設定する?||参考値 1e-3|
|Which scheduler do you want?|学習率のスケジューラーをどうする?|cosine_with_restarts|よくわからん"linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"|
|How many times do you want cosine to restart?||1|よくわからん|
|What power do you want to set your polynomial to?||1|よくわからん|
|do you want to save intermediate epochs?|途中のエポックでセーブする?|||
|How often do you want to save epochs?|何エポックごとにセーブする?|1||
|Do you want to shuffle captions?|キャプションをシャッフルする?|||
|Do you want to keep some tokens at the front of your captions?|キャプションの先頭からいくつをシャッフルしないで残す|1|手動でキャプションの先頭に単語を追加した場合その分を残す|
|Do you want to have a warmup ratio?|ウォームアップレシオを使う?||学習率を最初小さくする機能|
|What is the ratio of steps to use as warmup||0.05|学習率を最初からどれくらいのあいだ小さくしておくか 10%とか5%とか|
||エポック毎に出力ファイルの名前を変える?|||
|What do you want your output name to be?|出力ファイルの名前|省略時はオリジナルと一緒||
[END]

*GUI版
lora_train_command_line.py のパラメータ記入の補助ツール
GradioベースのGUI [[https://github.com/bmaltais/kohya_ss>>https://github.com/bmaltais/kohya_ss]]
「Tools」タブにフォルダ配置補助機能がある。
スクリプト版のパラメータの一部は設定出来ないかも？

*WebUIエクステンション版
ddPn08ニキが作ってくれた中身はkohya版 sd-scripts WebUIがでたで。
- エクステンションとしても使えるけどモデル分のVRAM余分に食うからエクステンションで使う場合[[空モデル>https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/908#issuecomment-1256198421]]読み込ましやとのことや
- 使い方は本人のnoteでもぐぐって調べるんやで。検索もでけへんやつはこのWikiにはおらんやろ。
- WebUI割としょっちゅう壊れることやしこいつは単独起動もできるから単独で入れたほうがええんとちゃうかな…？
-- https://github.com/ddPn08/kohya-sd-scripts-webui

*Loraの使用方法&aname(apply_lora)
**使い方その1 WebUIに拡張機能をインストールして使う
-「拡張機能」タブの「URLからインストール」に https://github.com/kohya-ss/sd-webui-additional-networks を入力してインストール )~~
H:\stablediffusion\stable-diffusion-webui\extensions\sd-webui-additional-networks\models\lora フォルダに 出来上がった .pt や .safetensorsをコピーする
(Web UI の 「設定」> 「Additional Nerwork」タブでフォルダの場所を追加出来る)
「txt2img」や「img2img」の画面の左下の方に「Additional Networks ▼」が追加されているので~~Enable を押してmodelを選びmerge倍率をweightのスライダーで調整する

**使い方その2 WebUIの本体機能のみで使う
-H:\stablediffusion\stable-diffusion-webui\models\lora に拾った .pt や .safetensorsをコピーする
「txt2img」や「img2img」の「生成」ボタンの下の花札みたいなマーク(&#127924;)を押すと
Texutual Inversion, Hypernetworks, Lora の3つのタブが出るので Lora を選択して
一覧から選ぶと <lora:ファイル名:倍率>みたいなタグがプロンプトに追加される
最新のLora(0.4.0)で作ったやつも使える







