#contents
* 概要
Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning
ウェイトをn×Rankの小さい行列A,Bの二つに分解してその行列をFinetuningする。そして差分情報を出力する。
簡単に言えば「省メモリで高速に学習できて容量も小さくて済む追加学習法」。''作成方法はいろいろある。''

[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/IgjGWbu1Yn.png,400)>https://image02.seesaawiki.jp/n/h/nai_ch/IgjGWbu1Yn.png]]
他の学習法とどう違うねん？　reddit民によればこんな感じのイメージらしい。
https://www.reddit.com/r/StableDiffusion/comments/10cgxrx/wellresearched_comparison_of_training_techniques/
kohya_ss版sd-scriptsの登場以来、sd-scripts及びそれの派生ツールが人気となっている。
このページではsd-scripts関連の情報について雑に書いてある。
''ページの更新がほとんどされておらず、一部の情報が古くなっていることに留意。''
&size(10){このページもといローカル部のページのほとんどが更新停止で悲しい&#129402;}
SDXL関連はスレか他所のページを見ましょう。

* 公式情報

** sd-scripts (kohya-ss)
一番はじめは作者が詳しく書いてくれている公式READMEを見よう！話はそれからだ！

- &color(#ff0000){★公式導入ガイド}：https://github.com/kohya-ss/sd-scripts/blob/main/README-ja.md
- &color(#ff0000){★公式LoRAガイド}：https://github.com/kohya-ss/sd-scripts/blob/main/docs/train_network_README-ja.md
- 公式学習データガイド：https://github.com/kohya-ss/sd-scripts/blob/main/docs/train_README-ja.md
- 公式コマンドライン引数表：https://github.com/kohya-ss/sd-scripts/blob/main/docs/config_README-ja.md
- 公式DreamBoothガイド：https://github.com/kohya-ss/sd-scripts/blob/main/docs/train_db_README-ja.md
- 公式Finetuneガイド：https://github.com/kohya-ss/sd-scripts/blob/main/docs/fine_tune_README_ja.md

*&aname(loraguide){参考資料・スレ住民による学習ガイド}

** 記事
これらの記事は基本的にSD1向けです。
: LoRA学習用サンプルデータ | https://note.com/kohya_ss/n/nb20c5187e15a
- 作者本人によるサンプル。

: LoRA Training Guide | https://rentry.org/lora_train
- 4chan有志によるLoRAトレーニング法ガイド（英語）

: いろいろLoRA作成記録 | https://rentry.co/irir_lora
- 512,768,1024の違い、小物、シチュエーション、背景、画風とかいろいろ

: LoRA 学習メモ | https://rentry.org/i5ynb
- スレ住民によるLain・よしなが先生・野原ひろしLoRA作成者によるLoRAガイド（日本語）~~更新：2023-03-15｜"--caption_extension=.txt"の引数を追加して明示的に指定しないとタグファイル(.txt)を読みにいかない仕様の注意喚起を追加しました。~~更新：2023-02-09｜低リソース学習(NIKKE)、低dim学習(ゆるキャン 犬山あおい)などを追加しました。

: ソウリンちゃんLoRAの作成記録 | https://rentry.org/sourin_chan
- スレ住民によるマルゼン式(ふたば有志のタグ付け手法の1つ)で作成したLoRA作成記録（日本語）

: 原神LoRA作成メモ・検証 | https://rentry.org/genshin_lora
- スレ住民によるkohya-ss氏制作のSDスクリプト(https://github.com/kohya-ss/sd-scripts )で次のキャラのLoRAを作成した。ポップアップ版使用。（日本語）
-- 筆者による追記:SD1.5時代の試行錯誤していた頃の情報で古くなっています。参考にしないでください。

: スレ住民によるキャラクター学習のタグ付け一例（日本語） | https://rentry.org/dsvqnd

: LAZY TRAINING GUIDE | https://rentry.org/LazyTrainingGuide
- loraをいっぱい作ってる海外ニキの打率9割学習ガイド

: あかちゃんLoRAノートブック | [[kohya_train_network_simple]]
- 全然スレに書き込めないけどけなげに頑張っている
- クラウドGPUを使う場合はリンク先の下の方に Colab Instructions がある
- フォルダ命名方法に気をつけて、自前のファイルは半角スペース一切入れないようにすれば無料Colabでも回せる。頑張れ。

* インストール、初回セットアップ編

** sd-scripts (作: kohya-ss)

純正のsd-scripts。コマンドプロンプトとか黒い画面にコマンドを打って使うやつや
+ 最新機能を使いたい
+ 安定した動作をのぞむ
こういう場合はこれを最初に試してな
インストールは下記を参考に
- &color(#ff0000){★公式導入ガイド}：https://github.com/kohya-ss/sd-scripts/blob/main/README-ja.md

* GUI・補助ツール
純正のsd-scriptsをかんたんに使えるようにしたものやで
bmaltais版GUIかKohya_lora_param_guiがおすすめ。Kohya_lora_param_guiは日本語で親切設計で使いやすい。

** bmaltais版 GUI
https://github.com/bmaltais/kohya_ss
GUI といっても作者の sd-scripts にパラメータを渡すだけや。性能は変わらん。セットアップがちょっとだけ楽。
GUIのほうが更新が数日遅れるのが常やから我慢してや。
-GradioベースのGUI。
-「Tools」タブにフォルダ配置補助機能がある。
-スクリプト版のパラメータの一部は設定出来ないかも？
-起動は、powershellならactivate.ps1、gui-user.ps1を順番に呼ぶのが早い（自動でブラウザも立ち上がる）
-アップデートは upgrade.ps1 をpowershellで実行

** Kohya_lora_param_gui (スレ住民作)
https://github.com/RedRayz/Kohya_lora_param_gui
https://github.com/kohya-ss/sd-scripts の学習用のパラメータ設定してコマンドラインに渡すGUIです。
- accelerate実行時のオプション（引数）「パラメーター設定をGUIで行えるようにした」もの。
- 単体でsd-scriptsのインストール・更新ができるようになった。
- としあきbatやlora_train_command_lineを毎回編集してた人、accelerateの引数を色々メモってた人にオススメかも。GUI上で数値とか編集してそのままaccelerateに渡す感。捗る。

** あかちゃんLoraインストーラー
あかちゃんインストーラーで1111を入れた人向けにPYTHONとGITのPATHをいじってあるやつ
start.batと同じフォルダに入れて実行してください

- コマンドライン用
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts.bat
-- https://github.com/aka7774/elemental_code/blob/main/tools/run_sd_scripts.bat
- ダイアログ用(みかんせい)
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts_easy_training.bat

** sd-webui-train-tools
https://github.com/liasece/sd-webui-train-tools
AUTOMATIC1111 SD WebUI 用の 拡張機能
画像をドロップしてパラメータを程よく埋めると出来る。簡単
動かんときは別にWebUIをセットアップして学習専用にしたほうがいい。
ReadMEが中国語なのでgoogle翻訳頑張れるなら 
更新頻度が低く最新の機能は使えない

** その他補助スクリプト
としあきbatや4chan製のスクリプトがある

アップデートは
=|(box=textarea)|
cd sd-scripts
git pull
.\venv\Scripts\activate
pip install --use-pep517 --upgrade -r requirements.txt
||=

** クラウド
[[あかちゃんLoraノートブック>https://seesaawiki.jp/nai_ch/d/kohya_train_network_simple]]
海外ニキのjupyter notebook
リンクのみ紹介。すでに更新停止(Archived)しており、動くかどうかは不明。
https://github.com/Linaqruf/kohya-trainer

* SDXLについて
基礎解像度が1024になった。SD1より安定性が良く、低いRank(Dim)でも良好な結果を得られる。
VRAM使用量が大幅に増加しており、8GBでTE込みの学習をするには--fp8_baseが必要。

* 学習用画像を置くフォルダの配置
%%%sd-scripts で --dataset_config で設定ファイルを渡す場合は異なる%%%
%%%https://github.com/kohya-ss/sd-scripts/blob/main/config_README-ja.md を読んで%%%
-作者の解説が詳しい
-- [[https://note.com/kohya_ss/n/nba4eceaa4594>>https://note.com/kohya_ss/n/nba4eceaa4594]]

-フォルダの配置例:
[-]
※要するに<繰り返し回数>_<インスタンスプロンプト>にリネームした学習画像データのフォルダは直接指定しないでねって話
例えば↓こういうこと
&#10060;E:\kohya_ss\TrainDatas\001\img\40_kdy 1girl
&#128994;E:\kohya_ss\TrainDatas\001\img
間違うと画像が見つかりませんと怒られる

&ref(https://image02.seesaawiki.jp/n/h/nai_ch/8v9xToIuUR.png)
[END]
-同時に10まで概念を学習できるが、少なくとも1つはフォルダが必要。
-フォルダの名前は <繰り返し回数>_<インスタンスプロンプト>
--<繰り返し回数> 繰り返し回数×学習用画像の枚数を1セット(1 epoch)として学習する 
※注 学習用の画像が50枚ある場合、繰り返し回数を20 にすると 20 x 50 = 1000 ステップ学習する
--<インスタンスプロンプト> クラス 呼び出し用のキーワード クラスは''英単語にない意味のないワード''がよい 
-- 上記kohya氏のサンプルだと「20_sls frog」　脳死で真似するなら 繰り返し回数_意味のないワード WEBUIでプロンプトとして書きたい単語 で設定しておく
--キャプション ファイルは必須です。そうでない場合、LoRA は概念名をキャプションとして使用してトレーニングを行います。
--キャプションについては以下

* Aspect Ratio Bucketing
データセットの画像を元の縦横比をできるだけ維持しつつ、min/max_bukect_resoの範囲内で画素数が学習解像度の二乗に近似する解像度へリサイズするもの
min/maxは学習解像度の半分、二倍にするといい
注意として、max bucket resoに学習解像度と同じ値を指定すると以下の画像のように正方形以外の画像が小さくなる
以下の画像ではSDXL学習なので学習解像度1024pxを指定した。縦横比2:3の画像は画素数が1024^2=1048576に近似する832x1216が望ましいが、704x1024(=720896画素)になってしまった
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/S8JsST7i_q.png)

* キャプション・タグを付ける
: 作者の詳しい画像付き説明 | [[https://github.com/kohya-ss/sd-scripts/blob/main/docs/train_README-ja.md>>https://github.com/kohya-ss/sd-scripts/blob/main/docs/train_README-ja.md]]
- 学習用の素材画像それぞれに内容を説明するテキストファイルを作る。このテキストファイルには画像生成時のプロンプトと同じようにタグを記載する。
- テキストエディターやメモ帳で1つずつ作っても良いのだが、WD1.4 Tagger 等のツールを使えば一気に自動生成できて捗る

** WD1.4 Taggerで作成
先に学習用画像を連番にリネームしておく (01.png, 02.png, ...など)
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/kbCJ6oEi6E.png)
[END]
Web UI に拡張機能 stable-diffusion-webui-wd14-tagger [[https://github.com/toriato/stable-diffusion-webui-wd14-tagger>>https://github.com/toriato/stable-diffusion-webui-wd14-tagger]]をインストール
「Tagger」タブの「Batch from directory」
-入力ファイル:学習用画像の入っているフォルダ
-Interrogator:wd-14convnext
-アンダースコアの代わりにスペースを使用する:オン
-括弧をエスケープする:オン
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/DGlOwfa65F.png)
[END]
Interrogateを押すと学習用画像のフォルダにタグの付いた .txt ファイルが生成される
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/vrFirugtW9.png)
[END]

** Dataset Tag Editorで作成/編集
たぶんとしあきがつくったやつ。サブディレクトリまで読んで一括でタグ付けしたり、慣れればかなり快適にタグ付けできる。

*** タグ付け
+ まずフォルダ指定してLoad なんちゃら2つチェックでOverwrite選択してInterrogatorsを好みのやつ選択
+ Interrogator Settingsの下の閾値チェックしたら上のLoadで画像読み込みや
+ ほんで画像読み込みと同時にタグも内部で生成しとるから終わったら次は右上のタブFile by Selectionでどの画像に適用するか選択や
+ とりあえず右のAdd ALL Displayedですべての画像選択して下のApply selection filterで適用して最後に左上のSave all changeですべて適用や
これでタグテキストが生成される
空の000ファイルが出てくるのは編集前のテキストファイルのバックアップや


** キャプション・タグの編集
- タグは順序に影響を受けるので、一番最初に有効化したいタグを記述する
- WD1.4Tagger等で自動生成したファイルには不要なタグが含まれたり誤認識されたタグが記載されたりするので編集する。
: BooruDatasetTagManager | https://github.com/starik222/BooruDatasetTagManager
: 学習用タグの入力を速く楽にするやつ | →[[ローカルの「ツール」]] https://uploader.cc/s/rdw0k6qd2766czgdwwwjtn2xtmhiay6c1ky0s7dui4o5yaz0pkgfesef18n9nngm.zip
: WebUI用拡張機能 Dataset Tag Editor | https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor 日本語READMEに使い方が書いてある
等の便利なツールを使えば捗る。必要なタグを追加、不要なタグの削除、順序の入れ替え等の編集をやる
- taggerで生成したタグの順序のままでも構わないが、重要なタグだけ各ファイルの先頭の方に記載する。例えばコマンドライン版（lora_train_command_line.py）の場合、
=|PY|
        self.shuffle_captions: bool = True  # OPTIONAL, False to ignore ~~
        self.keep_tokens: Union[int, None] = 3  # OPTIONAL, None to ignore ~~
||=
上記のように設定すれば先頭から3つのタグは順序固定として残りはタグの適当にシャッフルして学習できる。
- Dataset Tag Editorを使った簡単タグ整理法
　1.WD1.4Taggerなどを使用してタグ付けを実施。
　2.Dataset Tag EditorでBatch Edit Captionsを選択し、さらにRemoveを選択。
　3.誤検出で生じる完全に不要なタグを選択し削除する。
　4.学習タグおよびそれに類するタグを一度削除する。
　5.RemoveからEdit tagsへ項目を移動、Common Tagsを空欄にしEdit Tagsに学習タグを記載。
　6.Prepend additional tagsにチェックを入れてApply Changes to filtered imagesを選択。
　このように作業すればどのファイルも学習タグを先頭に持ってこられている状態であるため、タグに学習内容を関連付けしやすくなる。

** キャプションの付け方・考え方の参考サイト
: [[lora training tagging faq>https://rentry.org/lora-tag-faq]] | 英語サイトだがブラウザの翻訳で読もう
: [[キャラクター学習のタグ付け一例>https://rentry.org/dsvqnd]] | 実例を挙げての解説
一言で言えば「呼び出しキーワード」＋「学習から外したいもの」をタグに書く
: [[LoRAキャプション編集の話>https://rentry.co/burumalorav2]] | 住民作ブルマ体操服LoRAのタグ付けを解説
: [[Danbooruタグにない1トークンのワード一覧>https://rentry.co/no_danbooru_tokens]] | 公式ドキュメントで言及されている「tokinizerで1トークンになる3文字以下でレアな単語」を使いたい場合に有用
* 学習用画像の加工
- 本文で説明している kohya 版のLoRAではトリミングはしなくていい(画像のサイズ別に学習が行われる)
- あまりにも小さい画像(200pxとか)はUpscaylなどで拡大しておくか bucket_no_upscale をオンにする
- 背景の切り抜きは画像の大きさを揃えて下記のツールでやると楽。
- 切り抜く場合の背景色について (折りたたみ)
[+]
165スレ>>602
生首学習の背景の比較で今度はで白背景、黒背景、白背景透明色化、黒背景透明色化、白背景と黒背景両方使用、白背景透明色と黒背景透明色化の6パターンで比較してみた
白背景と黒背景の両方を使うと画像数が2倍になるので繰り返し数を半分にして全ての学習で総ステップ数が同じになるよう調整
・プロンプトに色タグ付けずに画像生成 (クリックで拡大)
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/P9FYT8JvUy-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/P9FYT8JvUy.jpg]]
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/Tn0Rmnea4m-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/Tn0Rmnea4m.jpg]]
・blue jacket, yellow shirt, red skirtと服装に色タグを付けて画像生成 (クリックで拡大)
[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/jhjYeHjEuK-s.jpg)>https://image02.seesaawiki.jp/n/h/nai_ch/jhjYeHjEuK.jpg]]
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/2VbIExKAVQ-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/2VbIExKAVQ.jpg]]
背景を透明化しても元の色は学習されるし、結果にあまり差はないから透明化する意味は無さそうに見える
服、背景ともに学習素材の背景色の影響を受け、淡いとか濃いといった差はあるけど絵の品質自体の差は無さそう
[END]
- キャラの切り出しだけやったら3Dペイント(Win10なら標準、11では標準からリストラされたけどストアにおるで)のマジック選択でええ感じに切り抜きやすいからそこからgimpなりで微調整。
- 一枚一枚やんのめんどくさい言うんやったらABG_extension言うのが出たんでつこてみたらええんとちゃうかな…？しらんけど

: ABG_extension | https://github.com/KutsuyaYuki/ABG_extension
WebUI公式extension。背景を自動で除去します。アニメ画像用に微調整されたonnxモデルを使用。GPUで動作します。

: katanuki | https://github.com/aka7774/sd_katanuki
WebUI用extension。anime-segmentation を 1111 で使えるようにしたやつ。画像の背景を透過したり白背景にしたりマスク画像を出力する
: stable-diffusion-webui-rembg | https://github.com/AUTOMATIC1111/stable-diffusion-webui-rembg
WebUI用extension。[[rembg>>https://github.com/danielgatis/rembg]] を 1111 で使えるようにしたやつ。動かんときはvenv消してみる。
u2net_human_seg あたりがちょうど良さげ
: 切り抜きノイズ除去ツール | https://12.gigafile.nu/0629-bc2ae51e82ab361567d60e0af3df9ef49
174スレ>>294 切り抜きで残った半透明のノイズを削除したり白背景にしたりするツールを作ったで
[+]
機能は羅列するとこんな感じや
(1)rembgやkatanukiで出力したマスク画像で画像を切り抜く
(2)切り抜き済み画像の背景を白・黒・指定色に変更して出力
ここからはノイズ消し機能
(3)半透明を不透明にすることで切り抜き残しやノイズを可視化
(4)(3)の半透明を薄い色から順番に削除した画像を出力することでノイズが消える設定を見つける
(5)(4)の画像の設定を使うことでノイズを削除したうえで背景を白などに変更して出力
(6)(5)のままだと半透明を削除したことで色の境目がはっきりしすぎてしまうので色の濃さに補正をかける
例 アルファ値(透明度)が10以下を削除すると0の隣に11が来てしまうため、11を6くらいの色の濃さとする補正をかける、これが50%での補正で0&#65374;100％に変更可能
[END]
: Lama-Cleaner | https://github.com/Sanster/lama-cleaner
AIで不純物とかを消してでっち上げるやつ。割と重い。windows用インストーラーあり。
WebUI用extensionもある [[Extensions_URL>https://seesaawiki.jp/nai_ch/d/Extensions%5fURL#content_3_15]]
* 正則化画像
LoRAの用途を考えると基本的に不要。なお、透明正則化の効果は無いと思われる。
- ChatGPTたん曰く「過学習を抑えるためのもの」
- キャプションつけたらそのプロンプトで学習させるモデルを使って(適当なネガティブプロンプトをつけて)作成すればいい・・・のだが詳しくはわからないので誰か書いてクレメンス
- 「鳥獣戯画のカエルちゃん」を覚える代わりに普通のカエルを忘れて書けなくなるのを防ぐために学習時に普通のカエルの画像を渡しておく感じ？
- 他所のノートブックを利用しているので確かな事は言えないが、正則化画像を同じような画像で学習させすぎると正則化画像につけたクラストークンで正則化画像の内容を生成するようになる。単に「学習画像と正則画像を二つとも学習する」という挙動のように思われる。
** 正則化画像検証 (クリックして展開)
[+]

*** キャプションつけて正則化画像指定以外完全同条件のLoCon使った比較
165スレ>>529〜より要約
-536
Dim/NW=128/64(LoCon:32/16) 元画像38枚*20回 10epoch Adafactor
列の左から5枚は正則化画像として「girl」100枚、それより右は正則化画像なしで回しとる。
で、エポックごとにWeight3段階に変えて「girl , solo, 1girl, sky, cloud, beach, upper body」、NPに「monochrome」だけ指定した結果がこうや
[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/WhSkmVVrZZ-s.jpg)>https://image02.seesaawiki.jp/n/h/nai_ch/WhSkmVVrZZ.jpg]]
正則化画像なしのほうは元キャラの「オレンジの帽子」とかの影響を初手から受け取るのが見える。
一方で当然やが正則化画像使っとるとかなり後半まで影響は低減されとる
-537
同じようにgirlをトリガーワードに変えるとこうなる。
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/om0uYc10Y6-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/om0uYc10Y6.jpg]]
正則化画像ないほうは形になるまでは早いんやが、キャラの上鎧部分、宝玉の表現とかが案外甘くてワイの場合ガビとの戦いになるんや
多少妥協するなら000006あたりで形になっとるから正則化画像なしは早く結果に繋がるのが利点なんやないか？
つうわけで正則化画像が問答無用でいらんっちゅうわけでもないんやないかなーと個人的には思うで
(編注:正則化画像なしだと学習に必要なステップ数は約半分になります。>>620 正確に言うと「正則化を入れてると、エポック毎のステップ数が２倍になる」んや)
-545
ゼノギアスのマルーを教師画像として使う時の話
>>536は「marouer」をトリガーワードとして設定して学習
>>537は「girl」をトリガーワードに(つまり既存のgirl概念に関与する形で)学習
またタグ付け学習で教師画像のキャプションに服装などを残す(つまり学習から除外する)条件
正則化(クラス付与)ありの場合となしの場合を比較すると
教師画像の情報を「marouer」に学習させるステップにおいて、
前者は人物の特徴(顔つき、画風、髪型などgirlが内包する概念)を優先して学習し、付帯的な要素(服装、帽子など)の影響を受けにくいのに対し、
後者は早期から「除外したいはずの」付帯的な要素の影響を受けてしまっている
-558
537は「girl」をトリガーワードに変えているので、教師画像の概念(髪型、顔つき、服装などすべて)が「girl」に干渉する場合を考える
正則化画像ありとなしの場合を比較すると、後者の方が「教師画像らしさ」を学習する速度が速い(これは正則化画像の「girl」概念の影響を受けないから？)
しかし、完成速度は比較的遅いが正則化画像ありの方が鎧などのディティールは正確に反映される傾向にある
正則化画像なしの学習でもある程度の妥協点(たとえば>>537でいう000006列)は見つかるので、速度を重視するなら正則化画像を使わないという選択肢もある

[END]
** 透明のPNGを正則化画像にする
透明正則化は効果を確認できないため、やる必要性はない。ただの都市伝説。
[+] 古い情報
Web UI に拡張機能 Generate-TransparentIMG をインストールする
[[https://github.com/hunyaramoke/Generate-TransparentIMG>>https://github.com/hunyaramoke/Generate-TransparentIMG]]
Generate TransparentIMG タブで、「出力フォルダ」には正則化画像の保存先を、「number_of_generation」には作成する枚数を入力して実行
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/l8x2Fjrdz7.png)
[END]

* 学習の手順 sd-scripts (直接指定)の場合
以下は一例(OptimizerにAdamW8bitを使った場合の例)
venvをactivate して
=|BOX|
.\venv\Scripts\activate
||=
accelerate launch の行を入力して実行
※長いので改行しているが、''&color(#ff0000){改行を取って全てを1行で書く}''事！
もしくは行末に ^ 記号を付けると ^直後の改行が無視され、複数行に分けたまま入力できる
=|BOX|
accelerate launch --num_cpu_threads_per_process 1 train_network.py 
 --pretrained_model_name_or_path=D:\stable-diffusion-webui\models\Stable-diffusion\model.safetensors 
 --train_data_dir=D:\sd-scripts\training 
 --output_dir=D:\sd-scripts\outputs 
 --reg_data_dir=D:\sd-scripts\seisoku 
 --resolution=512,512 
 --save_model_as=safetensors 
 --clip_skip=2 --seed=42 
 --color_aug 
 --min_bucket_reso=320 
 --max_bucket_reso=1024 
 --lr_scheduler=cosine_with_restarts 
 --lr_warmup_steps=500 
 --keep_tokens=1 
 --shuffle_caption 
 --enable_bucket 
 --mixed_precision=fp16 
 --xformers 
 --lr_scheduler_num_cycles=4 
 --caption_extension=.txt 
 --persistent_data_loader_workers 
 --bucket_no_upscale 
 --caption_dropout_rate=0.05 
 --optimizer_type=AdamW8bit 
 --learning_rate=1e-4 
 --network_module=networks.lora 
 --network_dim=128 
 --network_alpha=64 
 --max_train_epochs=10 
 --save_every_n_epochs=1 
 --train_batch_size=2 
||=
-メモ帳などにコピペして
-必要部分を書き換えて
-最後に改行を取って1行にして
-ターミナル(Powershell等)に貼り付けて
-実行
-メモ帳で保存しておけば次回からコピペで使い回せる


** 備考

- .txt拡張子のタグファイルを読ませる
"--caption_extension=.txt"の引数を追加して明示的に指定しないとタグファイル(.txt)を読みにいかない仕様があるっぽい。wd tagger等でタグ付けした.txt拡張子のタグファイルがを読ませたい場合、必ずaccelerate launch〜のコマンドに次の引数を追加しておこう
=|BOX|
--caption_extension=.txt
||=
なお学習したLoRAがタグファイルを読んでいるかは、Aditional network拡張機能をWEBUIにインストールして、Trainning infoのトグルを開いてタグリストの有無で判別できる。
詳しくは[[LoRAのメタデータの閲覧/編集の欄を参照>#howto_check_metadata]]

- 'Triton'エラー
学習開始時に次のメッセージが出ても気にしなくていい。なくても問題なし。そもそもTritonはWindowsに対応してない。
> A matching Triton is not available, some optimizations will not be enabled. ~~ Error caught was: No module named 'Triton'

* オプティマイザー

指定方法が空欄のものは名前と同じことを指します。※精度と安定性は主観的
|~名前|指定方法|精度|安定性|計算速度|収束速度|メモリ使用量|使いやすさ|備考|
|AdamW||良|中|高速|低速|普通|良||
|Adam8bit||良|やや低|高速|低速|少|良||
|Lion||中|中|高速|普通|普通|良||
|Lion8bit||中|やや低|高速|普通|少|良||
|AdaFactor||中|低|低速|低速|少|低|完全自動でこの表の中で最も遅い。多分大規模学習向け|
|DAdaptLion||中|やや低|低速|高速|普通|やや低||
|Prodigy||優良|中|低速|かなり高速|普通|やや低||
|AdamWScheduleFree||良|良|高速|高速(高LR)|普通|優良|sd3ブランチ限定、LR Schedulerはconstant_with_warmupでOK|
|RAdamScheduleFree||良|優良|高速|高速(高LR)|普通|優良|sd3ブランチ限定、LR schedulerはconstantでOK(wamupも不要)|
|CAME|came_pytorch.CAME|優良|良|高速|超高速|少|良|pytorch_cameのインストール必要|


*LoRAの種類とLyCORISについて
LyCORISについて
https://github.com/KohakuBlueleaf/LyCORIS
LoRAの新しい学習手法をまとめたリポジトリ。通常のLoRAより性能がいいかもしれない。一部のアルゴリズムの計算速度(it/s)は遅い。
もともとLoConのみだったが、のちに新しいアルゴリズムも追加されLyCORISに統合された。

**LyCORIS のインストールと使用方法
sd-scriptsのvenvで
=|BOX|
pip install lycoris_lora
||=
でインストールすることで使用可能。
学習時に使用するにはnetwork_moduleに lycoris.kohya を指定する。
=|BOX|
python3 sd-scripts/train_network.py 
  --network_module lycoris.kohya 
  --network_dim ○○ --network_alpha ○○
  --network_args "conv_dim=○○" "conv_alpha=○○" "dropout=○○" "algo=lora"
||=
注：1行で入力する事
デフォルトで畳み込み層(Conv)を学習するが、それが不要ならconv_dimに0を指定する。

○○は数値を入力
指定の例
=|BOX|
--network_module lycoris.kohya 
--network_dim 64 --network_alpha 32
--network_args "conv_dim=64" "conv_alpha=32" "dropout=0.05" "algo=lora"
||=
生成で使用するには1111のExtensionに[[a1111-sd-webui-locon>https://github.com/KohakuBlueleaf/a1111-sd-webui-locon]]、又は[[a1111-sd-webui-lycoris>https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris]]のインストールが必要。
標準のExtra Networksおよびkohya-ss氏作成の[[sd-webui-addtional-networks>>https://github.com/kohya-ss/sd-webui-additional-networks]]で使用できる。

**LoRAの種類
LoRAはその種類によって、次のように分けられる

|NO|LoRAの種類|LoRAの名称|使用ネットワークモジュール~~(network_module)|使用パラメータ~~(network_args)|モデルマージ|備考|
|��|LoRA|LierLa/リエラ~~(kohya版LoRA)|networks.lora|(なし)|可~~kohya版マージスクリプト使用|狭義の意味でのLoRAでこれが大元|
|��|LoCon|旧版LoCon|locon.locon_kohya|network_args 'conv_dim=XXX' 'conv_alpha=XXX'|可~~LyCORIS版マージスクリプト使用|LierLaを拡張した元祖LoCon~~LyCORISになる前にリリースされた物|
|��|^|C3Lier/セリア~~(kohya版LoCon)|networks.lora|network_args 'conv_dim=XXX' 'conv_alpha=XXX'|可~~kohya版マージスクリプト使用|kohya版LoCon|
|��|^|LyCORIS版LoCon|lycoris.kohya|network_args 'conv_dim=XXX' 'conv_alpha=XXX' 'algo=lora' 又は~~network_args 'conv_dim=XXX' 'conv_alpha=XXX' 'algo=locon'|可~~LyCORIS版マージスクリプト使用~~'disable_conv_cp=True' で作成したものはマージ不可|LyCORIS版LoCon|
|��|その他LyCORIS|LoHa|lycoris.kohya|network_args 'conv_dim=XXX' 'conv_alpha=XXX' 'algo=loha'|可~~LyCORIS版マージスクリプト使用|容量を小さく出来る|
|��|^|(IA)^3|lycoris.kohya|network_args 'conv_dim=XXX' 'conv_alpha=XXX' 'algo=ia3'|可~~LyCORIS版マージスクリプト使用||
|��|^|LoKR|lycoris.kohya|network_args 'conv_dim=XXX' 'conv_alpha=XXX' 'algo=lokr'|可~~LyCORIS版マージスクリプト使用||
|��|^|DyLoRA|lycoris.kohya|network_args 'conv_dim=XXX' 'conv_alpha=XXX' 'algo=dylora'|||

**画像生成する時の指定方法や使用extensions
|&#127924;指定(プロンプト指定)|必要extensions|対応LoRA|TEnc/UNet等の個別パラメータ指定|階層LoRA|標準のファイル設定フォルダ~~(変更は可能)|備考|
|<lora:>|(無し)|LoRA(��)~~LoCon(��,��,��)~~LyCORIS(��,��,��,��)|不可(一括の強度指定のみ)|可|models\LoRA|1111標準機能を使用~~v1.5.0以降でLyCORISにも対応|
|プロンプト指定なし|[[Additional Networks>https://github.com/kohya-ss/sd-webui-additional-networks]]|LoRA(��)~~LoCon(��,��,��)|可|不可|extensions\sd-webui-additional-networks\models\lora|プロンプト入力ではなく拡張タブで指定|
**階層LoRAの指定の方法
v1.5.0以降では指定の仕方が変わっているので注意
LoRAの場合でも ''lbw='' を指定するようになった
|種類|指定例(IN02を指定する場合)|解説|
|<lora:>|<lora:"lora name":1:1:lbw=IN02>|"lora name"の後に　TEnc:UNet の強度を指定してから　lbw=　で階層指定|

**<lora:>と<lyco:>の使い分けについて
v1.5.0以降では、lycorisプラグインが不要になり、1111の内蔵のLoRAで生成が出来る用になった。
その為 a1111-sd-webui-lycoris を導入する必要はなく、<lyco:>を使う必要も無くなった
* LoRA層別学習
U-Netの階層別に学習率やDim/Alphaを指定できるようになった。
特定の層のLRを下げたり削除することで画風への影響を減らしたりできる。

* コピー機LoRA学習法
過学習させたLoRAをあれこれすることで、目を大きくしたり、アウトラインを太くしたりと非常に細かい部分のみを変更できるLoRAが作成できる手法。
正確な名称はない。複雑な工程があるため長い間謎に包まれていたが読解してくれたニキがいたので掲載↓

649: 今、天王星のwiki見てきたら軌道傾斜角(i) が0.774°だった2023/05/04(木) https://fate.5ch.net/test/read.cgi/liveuranus/1683047281/649
>>318
flat式のLora作成工程ってこんな感じであってる？
1：元モデルAで出力画像Bを出す
2：元モデルAに出力画像Bを1枚だけ過学習させて過学習LoraCを作る
3：元モデルAに過学習LoraCをマージしてコピー機モデルDを作る(プロンプト未入力だと出力画像Bしか生成されないモデル)
4：出力画像Bを加工して加工画像Eを作る(この加工部分が完成品LoraHに反映される)
5：コピー機モデルDに加工画像Eを学習させて加工LoraFを作る
6：2から5の工程を別の出力画像Bで行い加工LoraFを複数作る
7：元モデルAに複数の加工LoraFをマージさせて加工モデルGを作る(flatでは彩度明度上げる用のLoraも配合を調整してマージ)
8：元モデルAと加工モデルGでモデル差分からLora作成して完成品LoraHができる(dimの調整はここで行う)

▼Tips
1：元モデルAで出力画像Bを出す(加工画像Eとの差分がはっきりするように加工するとよい、レイヤー分けすれば加工画像Eも作りやすくなる)

▼加工画像サンプル
フラット化LoRAの場合。
←加工前｜加工後→
出典：https://fate.5ch.net/test/read.cgi/liveuranus/1683047281/318
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/ZhpBGOiu9Y.png,700)

その他にどのようなLoRAが作れるかは https://huggingface.co/2vXpSwA7/iroiro-lora/tree/main をみると良い


コピー機学習法はこちらも参考になる。
https://rentry.co/kopiki_lora

* Loraの使用方法&aname(apply_lora)

''使い方その1とその2で、配置するフォルダが違うので注意！''

** 使い方その1 WebUIに拡張機能をインストールして使う
-「拡張機能」タブの「URLからインストール」に https://github.com/kohya-ss/sd-webui-additional-networks を入力してインストール )~~
stable-diffusion-webui\extensions\sd-webui-additional-networks\models\lora フォルダに 出来上がった .pt や .safetensorsをコピーする
(Web UI の 「設定」> 「Additional Nerwork」タブでフォルダの場所を追加出来る)
「txt2img」や「img2img」の画面の左下の方に「Additional Networks ▼」が追加されているので~~Enable を押してmodelを選びmerge倍率をweightのスライダーで調整する
[+]わからんとき用画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xl2vcs_kBK.png)
[END]
** 使い方その2 WebUIの本体機能のみで使う
-stable-diffusion-webui\models\lora に拾った .pt や .safetensorsをコピーする
「txt2img」や「img2img」の「生成」ボタンの下の花札みたいなマーク(&#127924;)を押すと
Texutual Inversion, Hypernetworks, Lora の3つのタブが出るので Lora を選択して
一覧から選ぶと <lora:ファイル名:倍率>みたいなタグがプロンプトに追加される
むかーしに作られたloraは動かんことがある
[+]わからんとき用画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/Gje_zP3vzL.png)
[END]
画像にマウスオーバーして出る(i)をクリックするとloraの作成パラメータや使われたタグが表示される(埋め込んであれば)
WebUIや拡張機能の更新で調子悪くてもどっちかでは動くはず

* LoRAのメタデータの閲覧/編集
&aname(howto_check_metadata){}
-WebUIのExtra Networks > Lora タブで 画像にマウスオーバーして出る(i)をクリックするとloraの作成パラメータや使われたタグが表示される(埋め込んであれば)
-[[Additional-networks拡張機能>>https://github.com/kohya-ss/sd-webui-additional-networks]]をインストールすると増えるタブ(Additional-networks)からメタデータ編集とかトレーニングデータとか見れる

** メタデータの閲覧(Additionak Networks Extension)
+ タブ(Additional-networks)に移動する
+ Model path filterに探したいLoRAの名前を入れてフィルタリングしておく
+ Model から該当するLoRAを選ぶと情報が読み込まれます

図1：UIのサンプル
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/9qfbpGdMR5.png,650)

図2：トレーニングデータのサンプル。最新のsd-scriptsで学習されたLoRAは、Taggerで付けられたタグとそのタグがどれぐらい含まれているかがグラフで閲覧できるようです。
インターネッツの奥底で入手したり いつダウンロードしたか忘れた謎のLoRAもこれでタグを確認してプロンプトに入れれば使えるかもしれない。便利。
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/sLdm0Bovhq.png,,650)

** メタデータの編集
+ 編集したいLoRAを読み込んだうえで、まんなかの下のほうにある「Editing Enabled」にチェックを入れる
+ メタデータを編集する。~~ とりあえずKeyword欄にトリガープロンプト、Descriptionに雑多な説明文、CoverImageにサムネ用画像を設定しておくと良いかと思う
+ 「Save Metadata」でLoRAが保存されます。ちなみに編集前のものは「○○.safetensors.backup」で保存しておいてくれるようです。

なおLoRAはテキストエディタでふつうに開くことができ、トレーニングの設定等(network_dim":"16"とか) も一応確認することができます。
ただ仕様見る限りDescription等を直で編集するのはむずかしそうなので、GUIから設定するのが無難っぽいです。

* LECO(Low-rank adaptation for Erasing COncepts)
https://github.com/p1atdev/LECO

プロンプトから特定の概念を消したり強くしたりできる。
例えば、mikoが原神のyae mikoに汚染されているのを取り除くといったことができる。

** 注意
学習時の精度のbfloat16はNVIDIAのAmpere世代(RTX30)以降のGPUのみ使用可能。それ以外のGPUではfloat32推奨。

* メモ / Tips

** 途中から学習を再開したい

コマンドに以下の引数とパスを指定すれば学習前に学習済みのLoRAの重みを読み込み、そこから追加で学習できます。
=|BOX|
--network_weights=
||=


** メモ

=|BOX|
初めてのLoRA学習素材準備
枚数10〜20　成功するまでそれでいい
エポック　もう5でいいや
ステップ　500以上にするな　成功するまで回転は速く
サイズ　2048x2048以上とかでなければそのまま放り込め
キャプション/タグ　編集無しでも何かしら覚えるまず回せ
正則化→なしでやれ　失敗の原因は多分そこじゃない
学習元と出力モデルは揃えろ
||=

** スペックに関するTips(SD1,Windows)
VRAM8GBでは512x512、Batch size2は余裕で動く。
gradient_checkpointingを有効にすればVRAM8GBで1024x1024もできる。
瞬間的に物理メモリを20GB以上確保するので16GB以上あったほうが安心。
仮想メモリはtorchバージョンやmax_data_loader_n_workersの値によるが、20-60GB消費する。

*注意点やで
- 基本的にLoraは元々「DreamBoothみたいに学習した差分ファイルをモデルにマージするための差分パッチみたいなもんとして使う」事が前提で作られとるから、今の個別適用は元々の設計と違う使い方なんや、なんで色々制限事項がある。
-- Loraは''原則「作ったモデルと同じ系統(SD-v1.x系 or SD-v2.x)」でしか適用できへん''で。要するにAnyとかで作ったLoraはWD1.4以降とかには使われへんし、その逆もしかりや。
--- よく似た使い方するHyperNetworkは系統またいでも一応反映はされとるみたいやで？しらんけど。
--- Extra Netrowksで間違えてSD1.x用のLoRAをSD2.xで使うとWebUIを再起動するまでエラーで二度と使えなくなるから注意やで
-- また、Loraを複数1倍で重ねて使うと絵が崩壊しやすくなる。適用したい階層が違う場合、階層適用出来るエクステンションとかでずらしたらええんとちゃうかな？しらんけど。
-- 先にも書いた通り基本的に差分パッチみたいなもんやからモデルごとに最適な倍率はちゃうかったりするで。あっちのモデルでは1倍でちょうどよかったんがこっちのモデルでは絵が崩壊するとかも普通にあるで。倍率は適度に変えや。
-- 基本的に配布されとるんはkohya氏による拡張版Loraやけど元々の実装版のLoraもDreamBoothエクステンションとかで作れたりするから作った際にはごっちゃにせんようにな？
---拡張機能の方はkohya氏による拡張版Loraのみが対応や。本体機能の方は元々の実装版でも行けるんとちゃうかな？しらんけど。



