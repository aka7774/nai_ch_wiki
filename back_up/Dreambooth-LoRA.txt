#contents
* 概要
Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning
簡単に言えば「省メモリで高速に学習できて容量も小さくて済む追加学習法」。''作成方法はいろいろある。''

[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/IgjGWbu1Yn.png,400)>https://image02.seesaawiki.jp/n/h/nai_ch/IgjGWbu1Yn.png]]
他の学習法とどう違うねん？　reddit民によればこんな感じのイメージらしい。
https://www.reddit.com/r/StableDiffusion/comments/10cgxrx/wellresearched_comparison_of_training_techniques/
kohya_ss版sd-scriptsの登場以来、sd-scripts及びそれの派生ツールが人気となっている。
このページではsd-scripts関連の情報について雑に書いてある

* 公式情報

** sd-scripts (kohya)
一番はじめは作者が詳しく書いてくれている公式READMEを見よう！話はそれからだ！

- &color(#ff0000){★公式導入ガイド}：https://github.com/kohya-ss/sd-scripts/blob/main/README-ja.md
- &color(#ff0000){★公式LoRAガイド}：https://github.com/kohya-ss/sd-scripts/blob/main/train_network_README-ja.md
- 公式学習データガイド：https://github.com/kohya-ss/sd-scripts/blob/main/train_README-ja.md
- 公式コマンドライン引数表：https://github.com/kohya-ss/sd-scripts/blob/main/config_README-ja.md
- 公式DreamBoothガイド：https://github.com/kohya-ss/sd-scripts/blob/main/train_db_README-ja.m
- 公式Finetuneガイド：https://github.com/kohya-ss/sd-scripts/blob/main/fine_tune_README_ja.md

*&aname(loraguide){参考資料・スレ住民による学習ガイド}

** 記事
: LoRA学習用サンプルデータ | https://note.com/kohya_ss/n/nb20c5187e15a
- 作者本人によるサンプル。

: LoRA Training Guide | https://rentry.org/lora_train
- 4chan有志によるLoRAトレーニング法ガイド（英語）

: LoRA 学習メモ | https://rentry.org/i5ynb
- スレ住民によるLain・よしなが先生・野原ひろしLoRA作成者によるLoRAガイド（日本語）~~更新：2023-03-15｜"--caption_extension=.txt"の引数を追加して明示的に指定しないとタグファイル(.txt)を読みにいかない仕様の注意喚起を追加しました。~~更新：2023-02-09｜低リソース学習(NIKKE)、低dim学習(ゆるキャン 犬山あおい)などを追加しました。

: ソウリンちゃんLoRAの作成記録 | https://rentry.org/sourin_chan
- スレ住民によるマルゼン式(ふたば有志のタグ付け手法の1つ)で作成したLoRA作成記録（日本語）

: 原神LoRA作成メモ・検証 | https://rentry.org/genshin_lora
- スレ住民によるkohya-ss氏制作のSDスクリプト(https://github.com/kohya-ss/sd-scripts )で次のキャラのLoRAを作成した。ポップアップ版使用。（日本語）
-- 02/10:繰り返し数の検証結果を追加。 02/23:簡易的な検証結果を追加。 03/04:LoConの検証結果を追加。

: lora training tagging faq（英語） | https://rentry.org/lora-tag-faq
- 現在はリンク切れ

: スレ住民によるキャラクター学習のタグ付け一例（日本語） | https://rentry.org/dsvqnd

: スレ住民によるLoRAでのキャラ学習素材の検証 | https://rentry.org/lora_namakubi

: LAZY TRAINING GUIDE | https://rentry.org/LazyTrainingGuide
- loraをいっぱい作ってる海外ニキの打率9割学習ガイド

: あかちゃんLoRAノートブック | [[kohya_train_network_simple]]
- 全然スレに書き込めないけどけなげに頑張っている
- クラウドGPUを使う場合はリンク先の下の方に Colab Instructions がある
- フォルダ命名方法に気をつけて、自前のファイルは半角スペース一切入れないようにすれば無料Colabでも回せる。頑張れ。



* インストール、初回セットアップ編

** sd-scripts (作: kohya)

純正のsd-scripts。コマンドプロンプトとか黒い画面にコマンドを打って使うやつや
+ 最新機能を使いたい
+ 安定した動作をのぞむ
こういう場合はこれを最初に試してな
インストールは下記を参考に
- &color(#ff0000){★公式導入ガイド}：https://github.com/kohya-ss/sd-scripts/blob/main/README-ja.md
ちなみにこれ以降のものは純正のsd-scriptsをかんたんに使えるようにしたものやで


** LoRA_Easy_Training_Scripts Installers
最初はまっさらなフォルダにインストールするんやで
: https://github.com/derrian-distro/LoRA_Easy_Training_Scripts/ | 下記の学習の手順で使うEasyTrainScriptsの人が作った簡易インストールスクリプト
画面右の Releases の下の Installers をクリック > 下にスクロールして install_sd_scripts_v5.bat をダウンロードする
右クリックして管理者として実行すると sd-scripts 本体と Easy_Train_Scripts の両方をインストールして、インストール後の初期設定までやってくれる。
うまくいかなかったら [[Gitをインストール>>https://seesaawiki.jp/nai_ch/d/%a5%ed%a1%bc%a5%ab%a5%eb%a4%ce%c6%b3%c6%fe%ca%fd%cb%a1#content_2_2]] してリトライ
わからないとき用画像↓
[+]
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/D_lYBs46Kx.png)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/lKDWlTHI_G.png)
&video(https://image01.seesaawiki.jp/n/h/nai_ch/9crBKdm8fu.webm)
[END]
アップデートはupdate.bat からの upgrade.bat (大きなバージョンアップのときは入れ直し)

** あかちゃんLoraインストーラー
あかちゃんインストーラーで1111を入れた人向けにPYTHONとGITのPATHをいじってあるやつ
start.batと同じフォルダに入れて実行してください

- コマンドライン用
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts.bat
-- https://github.com/aka7774/elemental_code/blob/main/tools/run_sd_scripts.bat
- ダイアログ用(みかんせい)
-- https://github.com/aka7774/elemental_code/blob/main/tools/install_sd_scripts_easy_training.bat

** bmaltais GUI
GUI といっても作者の sd-scripts にパラメータを渡すだけや。性能は変わらん。セットアップがちょっとだけ楽。
GUIのほうが更新が数日遅れるのが常やから我慢してや。
: bmaltais版 GUI | [[https://github.com/bmaltais/kohya_ss>>https://github.com/bmaltais/kohya_ss]]
-GradioベースのGUI。
-「Tools」タブにフォルダ配置補助機能がある。
-スクリプト版のパラメータの一部は設定出来ないかも？
-起動は、powershellならactivate.ps1、gui-user.ps1を順番に呼ぶのが早い（自動でブラウザも立ち上がる）
-アップデートは upgrade.ps1 をpowershellで実行

** AUTOMATIC1111 SD WebUI 拡張機能版 (作: ddPn08 GUI)
https://github.com/ddPn08/kohya-sd-scripts-webui
WebUIのエクステンションに対応。スタンドアロンで動かすと上記のbmaltais版と似たような感じ。
- エクステンションとしても使えるけどモデル分のVRAM余分に食うからエクステンションで使う場合[[空モデル>https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/908#issuecomment-1256198421]]読み込ましやとのことや
- 使い方は本人のnoteでもぐぐって調べるんやで。検索もでけへんやつはこのWikiにはおらんやろ。
- WebUI 割としょっちゅう壊れることやしこいつは単独起動もできるから単独で入れたほうがええんとちゃうかな…？
- アップデートは update.bat

** Kohya_lora_param_gui | https://github.com/RedRayz/Kohya_lora_param_gui
https://github.com/kohya-ss/sd-scripts の学習用のパラメータ設定してコマンドラインに渡すGUIです。
- accelerate実行時のオプション（引数）「パラメーター設定をGUIで行えるようにした」もの。
- kohya-ssやEasyTrainScriptsとは違い、sd-scriptsを標準の方法でインストールしておくことが前提。
- としあきbatやlora_train_command_lineを毎回編集してた人、accelerateの引数を色々メモってた人にオススメかも。GUI上で数値とか編集してそのままaccelerateに渡す感。捗る。
** sd-webui-train-tools | https://github.com/liasece/sd-webui-train-tools
AUTOMATIC1111 SD WebUI 用の 拡張機能
画像をドロップしてパラメータを程よく埋めると出来る。簡単
動かんときは別にWebUIをセットアップして学習専用にしたほうがいい。
ReadMEが中国語なのでgoogle翻訳頑張れるなら 
** その他補助スクリプト
としあきbatや4chan製のスクリプトがある

アップデートは
=|(box=textarea)|
cd sd-scripts
git pull
.\venv\Scripts\activate
pip install --use-pep517 --upgrade -r requirements.txt
||=

** クラウド
[[あかちゃんLoraノートブック>https://seesaawiki.jp/nai_ch/d/kohya_train_network_simple]]
海外ニキのjupyter notebook
リンクのみ紹介。その都度最新版を取ってくるので動かなくても泣かない。
https://github.com/Linaqruf/kohya-trainer

* 学習用画像を置くフォルダの配置
%%%sd-scripts で --dataset_config で設定ファイルを渡す場合は異なる%%%
%%%https://github.com/kohya-ss/sd-scripts/blob/main/config_README-ja.md を読んで%%%
-作者の解説が詳しい
-- [[https://note.com/kohya_ss/n/nba4eceaa4594>>https://note.com/kohya_ss/n/nba4eceaa4594]]

-フォルダの配置例:
[-]
※要するに<繰り返し回数>_<インスタンスプロンプト>にリネームした学習画像データのフォルダは直接指定しないでねって話
例えば↓こういうこと
&#10060;E:\kohya_ss\TrainDatas\001\img\40_kdy 1girl
&#128994;E:\kohya_ss\TrainDatas\001\img
間違うと画像が見つかりませんと怒られる

&ref(https://image02.seesaawiki.jp/n/h/nai_ch/8v9xToIuUR.png)
[END]
-同時に10まで概念を学習できるが、少なくとも1つはフォルダが必要。
-フォルダの名前は <繰り返し回数>_<インスタンスプロンプト>
--<繰り返し回数> 繰り返し回数×学習用画像の枚数を1セット(1 epoch)として学習する 
※注 学習用の画像が50枚ある場合、繰り返し回数を20 にすると 20 x 50 = 1000 ステップ学習する
--<インスタンスプロンプト> クラス 呼び出し用のキーワード クラスは''英単語にない意味のないワード''がよい 
-- 上記kohya氏のサンプルだと「20_sls frog」　脳死で真似するなら 繰り返し回数_意味のないワード WEBUIでプロンプトとして書きたい単語 で設定しておく
--キャプション ファイルは必須です。そうでない場合、LoRA は概念名をキャプションとして使用してトレーニングを行います。
--キャプションについては以下

* キャプション・タグを付ける
: 作者の詳しい画像付き説明 | [[https://github.com/kohya-ss/sd-scripts/blob/main/train_README-ja.md>>https://github.com/kohya-ss/sd-scripts/blob/main/train_README-ja.md]]
- 学習用の素材画像それぞれに内容を説明するテキストファイルを作る。このテキストファイルには画像生成時のプロンプトと同じようにタグを記載する。
- テキストエディターやメモ帳で1つずつ作っても良いのだが、WD1.4 Tagger 等のツールを使えば一気に自動生成できて捗る

** WD1.4 Taggerで作成
先に学習用画像を連番にリネームしておく (01.png, 02.png, ...など)
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/kbCJ6oEi6E.png)
[END]
Web UI に拡張機能 stable-diffusion-webui-wd14-tagger [[https://github.com/toriato/stable-diffusion-webui-wd14-tagger>>https://github.com/toriato/stable-diffusion-webui-wd14-tagger]]をインストール
「Tagger」タブの「Batch from directory」
-入力ファイル:学習用画像の入っているフォルダ
-Interrogator:wd-14convnext
-アンダースコアの代わりにスペースを使用する:オン
-括弧をエスケープする:オン
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/DGlOwfa65F.png)
[END]
Interrogateを押すと学習用画像のフォルダにタグの付いた .txt ファイルが生成される
[+]画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/vrFirugtW9.png)
[END]

** Dataset Tag Editorで作成/編集
たぶんとしあきがつくったやつ。サブディレクトリまで読んで一括でタグ付けしたり、慣れればかなり快適にタグ付けできる。

*** タグ付け
+ まずフォルダ指定してLoad なんちゃら2つチェックでOverwrite選択してInterrogatorsを好みのやつ選択
+ Interrogator Settingsの下の閾値チェックしたら上のLoadで画像読み込みや
+ ほんで画像読み込みと同時にタグも内部で生成しとるから終わったら次は右上のタブFile by Selectionでどの画像に適用するか選択や
+ とりあえず右のAdd ALL Displayedですべての画像選択して下のApply selection filterで適用して最後に左上のSave all changeですべて適用や
これでタグテキストが生成される
空の000ファイルが出てくるのは編集前のテキストファイルのバックアップや


** キャプション・タグの編集
- タグは順序に影響を受けるので、一番最初に有効化したいタグを記述する
- WD1.4Tagger等で自動生成したファイルには不要なタグが含まれたり誤認識されたタグが記載されたりするので編集する。
: BooruDatasetTagManager | https://github.com/starik222/BooruDatasetTagManager
: 学習用タグの入力を速く楽にするやつ | →[[ローカルの「ツール」]] https://uploader.cc/s/rdw0k6qd2766czgdwwwjtn2xtmhiay6c1ky0s7dui4o5yaz0pkgfesef18n9nngm.zip
: WebUI用拡張機能 Dataset Tag Editor | https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor 日本語READMEに使い方が書いてある
等の便利なツールを使えば捗る。必要なタグを追加、不要なタグの削除、順序の入れ替え等の編集をやる
- taggerで生成したタグの順序のままでも構わないが、重要なタグだけ各ファイルの先頭の方に記載する。例えばコマンドライン版（lora_train_command_line.py）の場合、
=|PY|
        self.shuffle_captions: bool = True  # OPTIONAL, False to ignore ~~
        self.keep_tokens: Union[int, None] = 3  # OPTIONAL, None to ignore ~~
||=
上記のように設定すれば先頭から3つのタグは順序固定として残りはタグの適当にシャッフルして学習できる。

** キャプションの付け方・考え方の参考サイト
: [[lora training tagging faq>https://rentry.org/lora-tag-faq]] | 英語サイトだがブラウザの翻訳で読もう
: [[キャラクター学習のタグ付け一例>https://rentry.org/dsvqnd]] | 実例を挙げての解説
一言で言えば「呼び出しキーワード」＋「学習から外したいもの」をタグに書く

* 学習用画像の加工
- 本文で説明している kohya 版のLoRAではトリミングはしなくていい(画像のサイズ別に学習が行われる)
- あまりにも小さい画像(200pxとか)はUpscaylなどで拡大しておくか bucket_no_upscale をオンにする
- 背景の切り抜きは画像の大きさを揃えて下記のツールでやると楽。
- 切り抜く場合の背景色について (折りたたみ)
[+]
165スレ>>602
生首学習の背景の比較で今度はで白背景、黒背景、白背景透明色化、黒背景透明色化、白背景と黒背景両方使用、白背景透明色と黒背景透明色化の6パターンで比較してみた
白背景と黒背景の両方を使うと画像数が2倍になるので繰り返し数を半分にして全ての学習で総ステップ数が同じになるよう調整
・プロンプトに色タグ付けずに画像生成 (クリックで拡大)
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/P9FYT8JvUy-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/P9FYT8JvUy.jpg]]
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/Tn0Rmnea4m-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/Tn0Rmnea4m.jpg]]
・blue jacket, yellow shirt, red skirtと服装に色タグを付けて画像生成 (クリックで拡大)
[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/jhjYeHjEuK-s.jpg)>https://image02.seesaawiki.jp/n/h/nai_ch/jhjYeHjEuK.jpg]]
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/2VbIExKAVQ-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/2VbIExKAVQ.jpg]]
背景を透明化しても元の色は学習されるし、結果にあまり差はないから透明化する意味は無さそうに見える
服、背景ともに学習素材の背景色の影響を受け、淡いとか濃いといった差はあるけど絵の品質自体の差は無さそう
[END]
- キャラの切り出しだけやったら3Dペイント(Win10なら標準、11では標準からリストラされたけどストアにおるで)のマジック選択でええ感じに切り抜きやすいからそこからgimpなりで微調整。
- 一枚一枚やんのめんどくさい言うんやったらABG_extension言うのが出たんでつこてみたらええんとちゃうかな…？しらんけど

: ABG_extension | https://github.com/KutsuyaYuki/ABG_extension
WebUI公式extension。背景を自動で除去します。アニメ画像用に微調整されたonnxモデルを使用。GPUで動作します。

: katanuki | https://github.com/aka7774/sd_katanuki
WebUI用extension。anime-segmentation を 1111 で使えるようにしたやつ。画像の背景を透過したり白背景にしたりマスク画像を出力する
: stable-diffusion-webui-rembg | https://github.com/AUTOMATIC1111/stable-diffusion-webui-rembg
WebUI用extension。[[rembg>>https://github.com/danielgatis/rembg]] を 1111 で使えるようにしたやつ。動かんときはvenv消してみる。
u2net_human_seg あたりがちょうど良さげ
: 切り抜きノイズ除去ツール | https://12.gigafile.nu/0629-bc2ae51e82ab361567d60e0af3df9ef49
174スレ>>294 切り抜きで残った半透明のノイズを削除したり白背景にしたりするツールを作ったで
[+]
機能は羅列するとこんな感じや
(1)rembgやkatanukiで出力したマスク画像で画像を切り抜く
(2)切り抜き済み画像の背景を白・黒・指定色に変更して出力
ここからはノイズ消し機能
(3)半透明を不透明にすることで切り抜き残しやノイズを可視化
(4)(3)の半透明を薄い色から順番に削除した画像を出力することでノイズが消える設定を見つける
(5)(4)の画像の設定を使うことでノイズを削除したうえで背景を白などに変更して出力
(6)(5)のままだと半透明を削除したことで色の境目がはっきりしすぎてしまうので色の濃さに補正をかける
例 アルファ値(透明度)が10以下を削除すると0の隣に11が来てしまうため、11を6くらいの色の濃さとする補正をかける、これが50%での補正で0&#65374;100％に変更可能
[END]
: Lama-Cleaner | https://github.com/Sanster/lama-cleaner
AIで不純物とかを消してでっち上げるやつ。割と重い。windows用インストーラーあり。
WebUI用extensionもある [[Extensions_URL>https://seesaawiki.jp/nai_ch/d/Extensions%5fURL#content_3_15]]
* 正則化画像
- ChatGPTたん曰く「過学習を抑えるためのもの」
- キャプションつけたらそのプロンプトで学習させるモデルを使って(適当なネガティブプロンプトをつけて)作成すればいい・・・のだが詳しくはわからないので誰か書いてクレメンス
- 間違っとる可能性大なのやが、例えばAIちゃんが知らない「鳥獣戯画のカエルちゃん」のイメージを教えるとする。学習用画像には「鳥獣戯画のカエルちゃん」画像を用意する。正則化画像にはありふれた「蛙の画像」を用意する。これでAIちゃんには「鳥獣戯画のカエルちゃん覚えようね！でも正則化画像フォルダにある普通の蛙とかは違うやつやから覚えなくていいよ」という感じで伝わる。イメージを覚えてもらうのに言葉では説明しづらいから画像で説明する感じ？多分。知らんけど。
- 他所のノートブックを利用しているので確かな事は言えないが、正則化画像を同じような画像で学習させすぎると正則化画像につけたクラストークンで正則化画像の内容を生成するようになる。これは上の「普通の蛙は覚えなくていいよ」というよりも、単に「学習画像と正則画像を二つとも学習する」という挙動のように思われる。
- 正則化画像は必須ではないので用意しなくても学習はできる。とりあえず一度学習動かしてみたいとかなら用意しなくてもいい。透明正則化も効果は不明瞭（良い影響があるとしても悪影響がないとも言えない）なので面倒ならやらなくてもいい。
** 正則化画像検証 (クリックして展開)
[+]

*** キャプションつけて正則化画像指定以外完全同条件のLoCon使った比較
165スレ>>529〜より要約
-536
Dim/NW=128/64(LoCon:32/16) 元画像38枚*20回 10epoch Adafactor
列の左から5枚は正則化画像として「girl」100枚、それより右は正則化画像なしで回しとる。
で、エポックごとにWeight3段階に変えて「girl , solo, 1girl, sky, cloud, beach, upper body」、NPに「monochrome」だけ指定した結果がこうや
[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/WhSkmVVrZZ-s.jpg)>https://image02.seesaawiki.jp/n/h/nai_ch/WhSkmVVrZZ.jpg]]
正則化画像なしのほうは元キャラの「オレンジの帽子」とかの影響を初手から受け取るのが見える。
一方で当然やが正則化画像使っとるとかなり後半まで影響は低減されとる
-537
同じようにgirlをトリガーワードに変えるとこうなる。
[[&ref(https://image01.seesaawiki.jp/n/h/nai_ch/om0uYc10Y6-s.jpg)>https://image01.seesaawiki.jp/n/h/nai_ch/om0uYc10Y6.jpg]]
正則化画像ないほうは形になるまでは早いんやが、キャラの上鎧部分、宝玉の表現とかが案外甘くてワイの場合ガビとの戦いになるんや
多少妥協するなら000006あたりで形になっとるから正則化画像なしは早く結果に繋がるのが利点なんやないか？
つうわけで正則化画像が問答無用でいらんっちゅうわけでもないんやないかなーと個人的には思うで
(編注:正則化画像なしだと学習に必要なステップ数は約半分になります。>>620 正確に言うと「正則化を入れてると、エポック毎のステップ数が２倍になる」んや)
-545
ゼノギアスのマルーを教師画像として使う時の話
>>536は「marouer」をトリガーワードとして設定して学習
>>537は「girl」をトリガーワードに(つまり既存のgirl概念に関与する形で)学習
またタグ付け学習で教師画像のキャプションに服装などを残す(つまり学習から除外する)条件
正則化(クラス付与)ありの場合となしの場合を比較すると
教師画像の情報を「marouer」に学習させるステップにおいて、
前者は人物の特徴(顔つき、画風、髪型などgirlが内包する概念)を優先して学習し、付帯的な要素(服装、帽子など)の影響を受けにくいのに対し、
後者は早期から「除外したいはずの」付帯的な要素の影響を受けてしまっている
-558
537は「girl」をトリガーワードに変えているので、教師画像の概念(髪型、顔つき、服装などすべて)が「girl」に干渉する場合を考える
正則化画像ありとなしの場合を比較すると、後者の方が「教師画像らしさ」を学習する速度が速い(これは正則化画像の「girl」概念の影響を受けないから？)
しかし、完成速度は比較的遅いが正則化画像ありの方が鎧などのディティールは正確に反映される傾向にある
正則化画像なしの学習でもある程度の妥協点(たとえば>>537でいう000006列)は見つかるので、速度を重視するなら正則化画像を使わないという選択肢もある

[END]
** 透明のPNGを正則化画像にする
Web UI に拡張機能 Generate-TransparentIMG をインストールする
[[https://github.com/hunyaramoke/Generate-TransparentIMG>>https://github.com/hunyaramoke/Generate-TransparentIMG]]
Generate TransparentIMG タブで、「出力フォルダ」には正則化画像の保存先を、「number_of_generation」には作成する枚数を入力して実行
[+]画像
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/l8x2Fjrdz7.png)
[END]

* 学習の手順 sd-scripts (直接指定)の場合
以下は一例(OptimizerにAdamW8bitを使った場合の例)
venvをactivate して
=|BOX|
.\venv\Scripts\activate
||=
accelerate launch の行を入力して実行
※長いので改行しているが、''&color(#ff0000){改行を取って全てを1行で書く}''事！
もしくは行末に ^ 記号を付けると ^直後の改行が無視され、複数行に分けたまま入力できる
=|BOX|
accelerate launch --num_cpu_threads_per_process 1 train_network.py 
 --pretrained_model_name_or_path=D:\stable-diffusion-webui\models\Stable-diffusion\model.safetensors 
 --train_data_dir=D:\sd-scripts\training 
 --output_dir=D:\sd-scripts\outputs 
 --reg_data_dir=D:\sd-scripts\seisoku 
 --resolution=512,512 
 --save_model_as=safetensors 
 --clip_skip=2 --seed=42 
 --color_aug 
 --min_bucket_reso=320 
 --max_bucket_reso=1024 
 --lr_scheduler=cosine_with_restarts 
 --lr_warmup_steps=500 
 --keep_tokens=1 
 --shuffle_caption 
 --enable_bucket 
 --mixed_precision=fp16 
 --xformers 
 --lr_scheduler_num_cycles=4 
 --caption_extension=.txt 
 --persistent_data_loader_workers 
 --bucket_no_upscale 
 --caption_dropout_rate=0.05 
 --optimizer_type=AdamW8bit 
 --learning_rate=1e-4 
 --network_module=networks.lora 
 --network_dim=128 
 --network_alpha=64 
 --max_train_epochs=10 
 --save_every_n_epochs=1 
 --train_batch_size=2 
||=
-メモ帳などにコピペして
-必要部分を書き換えて
-最後に改行を取って1行にして
-ターミナル(Powershell等)に貼り付けて
-実行
-メモ帳で保存しておけば次回からコピペで使い回せる


** 備考

- .txt拡張子のタグファイルを読ませる
"--caption_extension=.txt"の引数を追加して明示的に指定しないとタグファイル(.txt)を読みにいかない仕様があるっぽい。wd tagger等でタグ付けした.txt拡張子のタグファイルがを読ませたい場合、必ずaccelerate launch〜のコマンドに次の引数を追加しておこう
=|BOX|
--caption_extension=.txt
||=
なお学習したLoRAがタグファイルを読んでいるかは、Aditional network拡張機能をWEBUIにインストールして、Trainning infoのトグルを開いてタグリストの有無で判別できる。
詳しくは[[LoRAのメタデータの閲覧/編集の欄を参照>#howto_check_metadata]]

- 'Triton'エラー
学習開始時に次のメッセージが出ても気にしなくていい。そもそもTritonはWindowsに対応してない。
> A matching Triton is not available, some optimizations will not be enabled. ~~ Error caught was: No module named 'Triton'



* 学習の手順 LoRA_Easy_Training_Scriptsの場合
** ポップアップ版を使う場合
+ run_popup.batを実行
+ ポップアップにパラメーターを順次入力する
+ 出来上がりを待つ
*** LoRA_Easy_Training_Scripts ポップアップ版の手順画像 参考程度 (2023-04-02時点)
クリックして展開 アップデートなどで内容は変わる
わからんパラメータが出たらcancelを押しとけばデフォルト値が入る。
英語がわからんときはスクリーンショット撮ってスレで聞いてクレメンス。動かしとるやつなら誰かわかるやろ。
[+]
** ターミナルとかパワーシェルにコマンドを打つか、run_popup.batから実行する
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/yVzXCg9crC.png)
----
** 設定ファイル関係
設定ファイルをまとめて読み込んでバッチ処理しますか？「いいえ」
(あらかじめ作っておいた)設定ファイルを読み込みますか？「いいえ」
設定ファイルを保存しますか？「はい」
設定ファイルの保存場所を選んでください
設定ファイルの名前をつけてください
設定ファイルの保存だけして学習はしないですか「いいえ」
[[&ref(https://image02.seesaawiki.jp/n/h/nai_ch/EnnOPYPWpz-s.jpg)>https://image02.seesaawiki.jp/n/h/nai_ch/EnnOPYPWpz.jpg]]
----
** 入力
学習元のモデルを選んでください
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/jxCNF4YB2L.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/6tebwsnoiv.png)
----
学習用画像のフォルダを選んでください
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/QsMm7_wdmg.png)
数字_名前 フォルダが見えるように
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/BC_G2nQ6O4.png)
----
** 出力
出力先のフォルダを選んでください
出力モデルに名前をつけますか？「はい」
出力モデルに名前をつけてください
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/d_LHSt9yQv.png)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/Pm3ilZIP1a.png)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/TgS0LdaNq_.png)
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/DUzNjAiBHz.png)
----
** タグ、キャプション
学習用画像のタグを全部まとめたファイルを出力しますか？「はい」
タグを並べ替えをどうしますか？そのまま「occurrence-ly」
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/yMW3voD3gc.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/OkrJFdEvhE.png)
** 学習モデル別微調整
Stable Diffusion V2のモデルで学習する? 「いいえ」
実写(っぽい)モデルで学習する? 実写風モデルなら「はい」アニメ風モデルなら「いいえ」
----
** 正則化画像
正則化画像を使いますか？使わないときは「いいえ」
正則化画像のあるフォルダを選んでください
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/KXO6r5NWEn.png)
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/e99bPUOmNk.png)
数字_名前 フォルダが見えるように
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/OYsxnrbqxq.png)
----
** optimizer
どのオプティマイザーを使いますか？ デフォルトはAdamW8bit 詳しくは作者README
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/Mwxd8fadST.png)
** LoRA設定 各自いじって
network dim 次元数: デフォルト値は32
network alpha アルファ: dimの半分がデフォルト値
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/9BxUs0JJhS.png)&ref(https://image01.seesaawiki.jp/n/h/nai_ch/VlKOhNLhCY.png)
LoRAタイプ LoRA,LoCon,LoHa から選ぶ
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/zCQ3fKlsIm.png)
** LoCon設定 各自いじって (編注:なんかおかしい気がするがよくわからん)
LoConの次元数: デフォルト値はnetwork dimと同じ
LoConのアルファ: デフォルト値はLoCon dim と同じ
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/T_2kLlHDS7.png)&ref(https://image01.seesaawiki.jp/n/h/nai_ch/oe0Fpwo_9L.png)
----
** 学習率の設定
学習率(Learning Rate): 1e-4 (= 0.0001)くらいで。alphaを1にした場合dimの分学習率が割られるらしいので上げ目にする
U-Netの学習率:よくわからんときはキャンセルで。
テキストエンコーダの学習率:よくわからんときはキャンセルで。学習率の50分の1くらいがいいって言うとる海外ニキもおる
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DkV4RQ4ZnK.png)&ref(https://image01.seesaawiki.jp/n/h/nai_ch/I3USsDCepb.png)&ref(https://image01.seesaawiki.jp/n/h/nai_ch/I__fvgugVG.png)
学習率について具体的な例は [[参考資料・スレ住民による学習ガイド>#loraguide]]
----
** スケジューラ
どのスケジューラを使いますか？ 学習率を途中で上げ下げするやり方。詳しくはぐぐって。
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/cDPsMBCQT7.png)
----
** cosine with restartの回数を決めてください
----
** 学習の解像度
解像度をいくつにしますか？デフォルトは512
512が速い, 768ならRTX3060やColabで10000ステップ4~5時間コース
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/0I6Xi5Edzq.png)
この後横の解像度も聞かれる。キャンセルを押せば正方形になる
----
** バッチサイズ:一度に何枚処理するか VRAM12Gなら4〜6くらいいける(解像度512の場合)で、動かんかったら1で
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/O0bJN0I8s4.png)
----
** ステップとエポック
ステップ数の計算をエポック単位にしますか？
何エポック学習させますか？: 1エポックは 繰り返し回数(フォルダの先頭の数字)×学習用画像の枚数 ステップ
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/HYqExu2pMY.png)&ref(https://image01.seesaawiki.jp/n/h/nai_ch/DWVNJ2DpMd.png)
学習中エポックごとにloraを保存しますか？
何エポックごとに保存しますか？
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/M6Jh5Uc7cL.png)&ref(https://image01.seesaawiki.jp/n/h/nai_ch/ZCqzxdoZWw.png)
----
** warmup ratio を使いますか？使う場合の比率を決めてください: 学習の最初だけ学習率をあげる機能
----
** シャッフルキャプションの設定
キャプションをシャッフルしますか？:「はい」
キャプションの最初のトークンを保持する？「はい」
キャプションを付けた場合フォルダ名のインスタンスプロンプトが無効になる~~のでキャプションファイルの先頭にインスタンスプロンプトを自分で書く必要がある。~~作者のnoteによると「数値を指定するとキャプションの先頭から、指定した数だけのトークン（カンマ区切りの文字列）をシャッフルせず固定します。」~~キャプションの先頭からカンマ区切りで判定されるので 「zkz, 1girl, condom, ass, solo, black panties, one side up,」なら1でおk
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/QKUhVMReO3.png)&ref(https://image01.seesaawiki.jp/n/h/nai_ch/Jrj2Ubei84.png)&ref(https://image02.seesaawiki.jp/n/h/nai_ch/H7aQQo0LQ1.png)
----
** U-netとテキストエンコーダのどれを学習しますか？ 普通は「both」
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/Aq2qiEHm3a.png)
----
** 学習画像を左右反転して2倍に水増ししますか？ 髪の毛の分け目とかオッドアイが逆になってもいいなら「はい」
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/S9IKM82sNS.png)
** メタデータを埋め込みますか？ Addtional Networksで読めるメモを書いてもいい
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/eLB_Or0EYB.png)

----
** 画像の拡大をやめますか？ 小さな画像を無理やり拡大してガビガビにならんようにする
----
** mixed precision
fp16で。 RTX3000以降ならbf16を使うとNaNsエラーが出にくくなるらしい。
** cacha latent と random crop
cacha latentは学習前に画像をキャッシュして速くする方法
random cropは自動的にランダムに学習用画像を切り抜きすることで少ない画像でも効果が上がるかもしれない方法
どちらか片方しか使えないのでここで選ぶ
----
min SNR gamma training を使いますか？ 新機能 よくわからん
----
** テストイメージ
dreamboothとかでできてた学習の様子を画像で出すやつ
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/VX_bsaoUGp.png)
txtファイルにプロンプトを1行で書いて用意しておく必要がある
プロンプト --n ネガティブプロンプト --w 768 --h 768 --d 1 --l 7.5 --s 28
など
デフォルトでは200stepごとに1枚生成
//** キャプションをわざと抜く? 詳しくは作者README
//** Noise offset入れる? よくわかりません デフォルト値は0.1
//----
//** 次の学習も仕込む? このあと一連の手順を追記して別の学習を予約できる
//
----
** 学習の様子 縦横の比率は自動で振り分けしてくれる
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/9VizQs0l1c.png)
** 出来上がり
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/y3w3sgkzGd.png)
last.safetensors というファイルができる
追加学習するときはこのファイルを指定する
[END]
動画(2023-03-04) すぐにアプデで役に立たなくなるが一応 2時間でゴミになりました。
わからんパラメータは キャンセルでデフォルト値が入る
[+]
&video(https://image02.seesaawiki.jp/n/h/nai_ch/OXhMCWcV4H.webm)
[END]
研究心旺盛ニキはコマンドラインの訳を読むとええで [[lora_train_command_line.py>lora_train_command_line.py]]
*** コマンドライン版を使う場合
+ ArgsList.pyにパラメーターを書く
※2023/02/24版から設定ファイルがlora_train_command_line.pyからArgsList.pyに変更されているので注意
設定を書き込むのはArgsList.pyの最初の方あたり。学習ベースになるモデル、学習素材フォルダの場所、出力先は必ず設定する。わからんところはそのままにしとく。
記法は以下を参考に、文字列(str)は r"c:\hogehoge"　のように入力、数値(float,int)はそのまま数値を入力、Falseとなっている部分はTrueで有効になる。
以下ArgsList.pyの日本語訳
[+]
#include(lora_train_command_line.py)
[END]
+ run_command_line.batを実行
+ 出来上がりを待つ
//コマンドライン版の説明が詳しいのでコメントアウトしました
//ポップアップの質問と答えの例
//[+]
//|英文|訳|バッチファイルのデフォルト値|コメント|
//|Do you want to load a json config file?|jsonファイルから前の設定読み込む?||1回設定セーブしとかないとダメ|
//|Select your base model|学習元のモデルを選ぶ|||
//|Select your image folder|学習用画像のフォルダを選ぶ||数字が先頭についているフォルダの上|
//|Select your output folder|(loraの)出力先のフォルダを選ぶ|||
//|Do you want to save a json of your configuration?|jsonファイルに設定を保存する?|||
//|How many workers do you want? 〜||8|よくわからん|
//|Do you want to use regularisation images?|正則化画像を使う?|||
//|Select your regularisation folder|正則化画像のフォルダを選ぶ||数字が先頭についているフォルダの上|
//|Do you want to continue from an earlier version?|前回のつづきから学習する?||学習を中断した場合続きから再開できる|
//|How large is your batch size going to be|バッチサイズをいくつにする?|1|VRAMに余裕があれば2〜8|
//|How many epochs do you want?|何エポック学習する?|1|多くすると学習回数が増える|
//|What is the dim size you want to use?|loraのランク(network dim)をいくつにする?|128|数が多いほど表現力は増すが時間、メモリ、ファイルサイズも増える 4〜128|
//|What Alpha do you want?|alpha値をいくつにする?|network dimと同じ|alpha値を1にした場合学習率をあげたほうがいいらしい|
//|How large of a resolution do you want to train at?|学習の解像度をいくつにする?|512|768や1024も指定できるがmax_bucket_resolutionも書き換え必要|
//|What learning rate do you want to use?|学習率をいくつにする?|1e-4|alpha値を1にした場合1e-3くらいにあげたほうがいい|
//|Do you want to set the text_encoder_lr?|テキストエンコーダの学習率を設定する?||参考値 5e-5|
//|Do you want to set the unet_lr?|U-Netの学習率を設定する?||参考値 1e-3|
//|Which scheduler do you want?|学習率のスケジューラーをどうする?|cosine_with_restarts|よくわからん"linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"|
//|How many times do you want cosine to restart?||1|よくわからん|
//|What power do you want to set your polynomial to?||1|よくわからん|
//|do you want to save intermediate epochs?|途中のエポックでセーブする?|||
//|How often do you want to save epochs?|何エポックごとにセーブする?|1||
//|Do you want to shuffle captions?|キャプションをシャッフルする?|||
//|Do you want to keep some tokens at the front of your captions?|キャプションの先頭からいくつをシャッフルしないで残す|1|手動でキャプションの先頭に単語を追加した場合その分を残す|
//|Do you want to have a warmup ratio?|ウォームアップレシオを使う?||学習率を最初小さくする機能|
//|What is the ratio of steps to use as warmup||0.05|学習率を最初からどれくらいのあいだ小さくしておくか 10%とか5%とか|
//||エポック毎に出力ファイルの名前を変える?|||
//|What do you want your output name to be?|出力ファイルの名前|省略時はオリジナルと一緒||
//[END]
** オプティマイザー
*** Adafactor
再現度はAdamW以下なのでおすすめしない。
Adなんとかを使って学習してみたらなんも学習してないのができて悲しかったので、スレでマークしておいたものを記録しておきます。クリックして展開
[+]2023/02/22あたりのAdafactorの話題メモ
----
-396: としあき  2023/02/22(水) 00:56:16.00
>>Adafactorって簡単に言うとどういうものなの？
Optimizerと呼ばれるものの一種（LoRAデフォルトはAdamW）
Optimizerはネットワークの最適なパラメータを探す手法だけど
Optimizerがやってる内容的には例えば足元が1寸先しか見えない状況で最も高い場所探すにはどうすればいいかみたいな感じの計算してる
Adafactorは学習率とかを自動調整する機能が優秀だから調整に頭悩ませなくて済むメリットがある
分かり易い解説をありがとう
----
NVA部157あたり：https://fate.5ch.net/test/read.cgi/liveuranus/1677145939/661
-264: 
AdaFactorを使った学習で1repeats50epochと10repeats5epochでどちらが良いのか調べてみました！！
&ref(https://i.imgur.com/wJfpJPj.jpg,400)
結果、よく分かりませんでした・・・
いかがでしたでしょうか？
1rp50epの方が再現度は高い気がする
が、epochの切り替わりに時間がかかるので、10rp5pに比べて3倍ぐらい時間がかかる

-269: 
AdaFactorとAdamW8bitの違いを凝光で検証したのでLoRA作成メモ更新したで
AdaFactorは少し遅い代わりにいい感じに過学習を抑えながら再現できるっぽい
https://rentry.org/genshin_lora
&ref(https://i.imgur.com/0KUdTD1.jpg,600)
ついでに凝光とかのlora上げとくわ
https://huggingface.co/RedRayz/genshin_lora_set

-488: 
1e-4で2500回回すのと
5e-5で5000回回すの
なんとなく後者でやった方が過学習なさそうで後者でやってるけど実際1e-4で少ないステップでもええの？

-507: 
Adafactor君、再現度確保するためにはかなりステップ回さないとダメっぽいな
それだけ過学習がしづらいということなんだろうけどこれだったら別にAdamWのままでも良さそう・・・

-509: 
AdaFactor学習進まんのはalphaが何か悪さしとる気がしないでもない
あれを1にするとLR以前の問題で学習のフィードバックががた落ちするからな
イージースクリプトやとdimの半分が推奨値やし
今試しとるところや

-575: 
AdaFactorのオプション --optimizer_args "relative_step=True" "scale_parameter=True" "warmup_init=True"
でラーニングレート1e-3から1.0に変えてalpha1から128に変えたらいい感じになったで
今から壊れるまでSTEPぶん回してくる

-646:
AdaFactorで学習進まんのはやっぱりalphaが低すぎるのが主要因やと思うわ
alphaをdimの半分の64でやったら数千ステップで普通に学習されたから
alpha1でうまくいかん奴がおったらワイ個人としてはAdaFactor使用時はalphaをdimの半分で運用するのを勧めるで

-658:
AdaFactor、前と同じステップにするとなんも学習してないようなのができるよな
時間かかっても精度上げるためのオプティマイザらしいが、そんなに精度に差があるかはまだようわからんな…

-659: 
https://paperswithcode.com/method/adafactor
これがAdafactorや
本格的に深層学習やるなら魚本買って学習した方が手っ取り早いかもしれんな
いよいよ数学の話になってもうた

-785:
>>774
これはsdじゃなくてcraiyonで使われてるDall-e-miniモデルの比較みたいやが
adamの最適値が1e-4に対してadafactorの最適値は1e-3なんでもう少しlr上げてもええかもしれんね
&ref(https://i.imgur.com/gwaFVBw.jpg,800)

-770: 
バッチ4で回したけどバッチ1に換算して表示しとるで
これ見ると2000ステップは門前払い、4000でようやく要素が似てきて、6000でやっとという感じやな
&ref(https://i.imgur.com/yQqusU7.jpg,600)

res 512
dim 128
alpha 64
batch 4
LR none
relative True
scale True
warmup True

-869: 
adafactorのオプションのwarmup_init=Trueすると>>770のいう通り相当回さんと学習せんで
結果はいいのかもしれんけど時間かかりすぎるからワイは無効にしとる
adafactor使用時はlr=1e-3でaddmWの1e-4と同じような学習になる気がする

-876:
Adafactorスクリプト版で>>774を見よう見まねで試してみたけど何一つ学習してなくて草枯れる…
と思って前スレ（>>981）見たら
＞AdaFactorは"warmup_init=True"すると学習しなくなる。ワイの設定の仕方のせいかもしれんけど
＞"relative_step=True" "scale_parameter=True"はデフォで有効だから--optimizer_type=AdaFactorでええ
こんなんあってoptimizer_args消したらとりあえず学習してくれたわ

-897:
AdaFactorとLion、収束遅い割にそこまで画質向上した感無くて辛くない？結局使い慣れたAdamW+Cosineに戻ってきちゃった
昔スレニキが言ってた、色んなOptimizerが出てはAdamに勝てず消えていくって言ってた意味が何カ月ぶりかにようやく理解できたわ...

-905: 
>>770の続きで今度はAdaFactorのwarmup有無やで
&ref(https://i.imgur.com/ly0AEs5.jpg,600)
これを見るにwarmupが無ければ2000でもほぼ写し取りが完了していて
warmupはどっかをいじって立ち上げ角度変えんと邪魔にしかならんことが分かる

ただ↑のはベースモデルの出力でwarmup無しでいかにも過学習が起きにくいように見えとるけど
これを7th3.1Cに持って行くとこうや
&ref(https://i.imgur.com/JfdD4eX.jpg,600)
学習が進むにつれベースモデルへの依存が増すのかモデル間の持ち運びが破綻しとる
8bitAdamが無ければ省メモリダントツの革命的なオプティマイザなんやけど使い方の研究が進まんと今のところは好みの問題やな

-907: 
おもかげ宿し始めてんのワロス
&ref(https://i.imgur.com/QVOGR2F.jpg,200) &ref(https://i.imgur.com/cERqQbw.jpg,200)
>>876
--optimizer_args "relative_step=True" "scale_parameter=True" "warmup_init=True" をまるごと削除して通ったわ
シャイニングショルダー飽きたからポプ子とピピ美でやってみる

-973: 
オプティマイザーをAdfactorで学習したやつ：ポプテピ
--optimizer_type=AdaFactor --save_every_n_epochs=1 --learning_rate=1e-3 --network_dim=64 --network_alpha=32
2080ステップ/1epoch目のやつだけど学習は十分な感じはある
なんか混ざっちゃうのはワイがタグ付け&#128118;だからだと思う
ちなモデルはリアル系は合わなくてHXDCにしたらかわいくなってしまった、クソアニメなのに
&ref(https://i.imgur.com/9VaR1jC.jpg,300) &ref(https://i.imgur.com/sii4OxP.jpg,300)
[END]
** LoCon(LyCORIS)について
https://github.com/KohakuBlueleaf/LyCORIS
LoRAの新しい学習手法。通常のLoRAより性能がいいかもしれない。学習速度(it/s)は遅い。
名前が変わって LyCORIS に統合された
sd-scriptsのvenvで
=|BOX|
pip install lycoris_lora
||=
でインストールすることで使用可能。
学習時に使用するにはnetwork_moduleに lycoris.kohya を指定する。
=|BOX|
python3 sd-scripts/train_network.py 
  --network_module lycoris.kohya 
  --network_dim ○○ --network_alpha ○○
  --network_args "conv_dim=○○" "conv_alpha=○○" "dropout=○○" "algo=lora"
||=
注：1行で入力する事

○○は数値を入力
指定の例
=|BOX|
--network_module lycoris.kohya 
--network_dim 64 --network_alpha 32
--network_args "conv_dim=64" "conv_alpha=32" "dropout=0.05" "algo=lora"
||=
生成で使用するには1111のExtensionに[[a1111-sd-webui-locon>https://github.com/KohakuBlueleaf/a1111-sd-webui-locon]]のインストールが必要。
標準のExtra Networksおよびkohya-ss氏作成の[[sd-webui-addtional-networks>>https://github.com/kohya-ss/sd-webui-additional-networks]]で使用できる。

* Loraの使用方法&aname(apply_lora)

''使い方その1とその2で、配置するフォルダが違うので注意！''

** 使い方その1 WebUIに拡張機能をインストールして使う
-「拡張機能」タブの「URLからインストール」に https://github.com/kohya-ss/sd-webui-additional-networks を入力してインストール )~~
stable-diffusion-webui\extensions\sd-webui-additional-networks\models\lora フォルダに 出来上がった .pt や .safetensorsをコピーする
(Web UI の 「設定」> 「Additional Nerwork」タブでフォルダの場所を追加出来る)
「txt2img」や「img2img」の画面の左下の方に「Additional Networks ▼」が追加されているので~~Enable を押してmodelを選びmerge倍率をweightのスライダーで調整する
[+]わからんとき用画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/xl2vcs_kBK.png)
[END]
** 使い方その2 WebUIの本体機能のみで使う
-stable-diffusion-webui\models\lora に拾った .pt や .safetensorsをコピーする
「txt2img」や「img2img」の「生成」ボタンの下の花札みたいなマーク(&#127924;)を押すと
Texutual Inversion, Hypernetworks, Lora の3つのタブが出るので Lora を選択して
一覧から選ぶと <lora:ファイル名:倍率>みたいなタグがプロンプトに追加される
むかーしに作られたloraは動かんことがある
[+]わからんとき用画像
&ref(https://image01.seesaawiki.jp/n/h/nai_ch/Gje_zP3vzL.png)
[END]
画像にマウスオーバーして出る(i)をクリックするとloraの作成パラメータや使われたタグが表示される(埋め込んであれば)
WebUIや拡張機能の更新で調子悪くてもどっちかでは動くはず

* LoRAのメタデータの閲覧/編集
&aname(howto_check_metadata){}
-WebUIのExtra Networks > Lora タブで 画像にマウスオーバーして出る(i)をクリックするとloraの作成パラメータや使われたタグが表示される(埋め込んであれば)
-[[Additional-networks拡張機能>>https://github.com/kohya-ss/sd-webui-additional-networks]]をインストールすると増えるタブ(Additional-networks)からメタデータ編集とかトレーニングデータとか見れる

** メタデータの閲覧
+ タブ(Additional-networks)に移動する
+ Model path filterに探したいLoRAの名前を入れてフィルタリングしておく
+ Model から該当するLoRAを選ぶと情報が読み込まれます

図1：UIのサンプル
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/9qfbpGdMR5.png,650)

図2：トレーニングデータのサンプル。最新のsd-scriptsで学習されたLoRAは、Taggerで付けられたタグとそのタグがどれぐらい含まれているかがグラフで閲覧できるようです。
インターネッツの奥底で入手したり いつダウンロードしたか忘れた謎のLoRAもこれでタグを確認してプロンプトに入れれば使えるかもしれない。便利。
&ref(https://image02.seesaawiki.jp/n/h/nai_ch/sLdm0Bovhq.png,,650)

** メタデータの編集
+ 編集したいLoRAを読み込んだうえで、まんなかの下のほうにある「Editing Enabled」にチェックを入れる
+ メタデータを編集する。~~ とりあえずKeyword欄にトリガープロンプト、Descriptionに雑多な説明文、CoverImageにサムネ用画像を設定しておくと良いかと思う
+ 「Save Metadata」でLoRAが保存されます。ちなみに編集前のものは「○○.safetensors.backup」で保存しておいてくれるようです。

なおLoRAはテキストエディタでふつうに開くことができ、トレーニングの設定等(network_dim":"16"とか) も一応確認することができます。
ただ仕様見る限りDescription等を直で編集するのはむずかしそうなので、GUIから設定するのが無難っぽいです。

* メモ / Tips

** 途中から学習を再開したい

コマンドに以下の引数とパスを指定すれば学習前に学習済みのLoRAの重みを読み込み、そこから追加で学習できます。
=|BOX|
--network_weights=
||=


** メモ

=|BOX|
初めてのLoRA学習素材準備
枚数10〜20　成功するまでそれでいい
エポック　もう5でいいや
ステップ　500以上にするな　成功するまで回転は速く
サイズ　2048x2048以上とかでなければそのまま放り込め
キャプション/タグ　編集無しでも何かしら覚えるまず回せ
正則化→なしでやれ　失敗の原因は多分そこじゃない
学習元と出力モデルは揃えろ
||=

** スペックに関するTips
VRAM8GBでは512x512、Batch size2は余裕で動く。
mem_eff_attenとgradient_checkpointingを有効にすれば1024x1024もギリギリできる。5-6倍遅くなるが。
ただ、解像度を上げることで品質向上する可能性は低いので現時点ではお勧めしない。要検証。
瞬間的に物理メモリを20GB消費するので16GB以上あったほうが安心。
仮想メモリはtorchバージョンによるが、40-60GB消費する。

*注意点やで
- 基本的にLoraは元々「DreamBoothみたいに学習した差分ファイルをモデルにマージするための差分パッチみたいなもんとして使う」事が前提で作られとるから、今の個別適用は元々の設計と違う使い方なんや、なんで色々制限事項がある。
-- Loraは''原則「作ったモデルと同じ系統(SD-v1.x系 or SD-v2.x)」でしか適用できへん''で。要するにAnyとかで作ったLoraはWD1.4以降とかには使われへんし、その逆もしかりや。
--- よく似た使い方するHyperNetworkは系統またいでも一応反映はされとるみたいやで？しらんけど。
--- Extra Netrowksで間違えてSD1.x用のLoRAをSD2.xで使うとWebUIを再起動するまでエラーで二度と使えなくなるから注意やで
-- また、Loraを複数1倍で重ねて使うと絵が崩壊しやすくなる。適用したい階層が違う場合、階層適用出来るエクステンションとかでずらしたらええんとちゃうかな？しらんけど。
-- 先にも書いた通り基本的に差分パッチみたいなもんやからモデルごとに最適な倍率はちゃうかったりするで。あっちのモデルでは1倍でちょうどよかったんがこっちのモデルでは絵が崩壊するとかも普通にあるで。倍率は適度に変えや。
-- 基本的に配布されとるんはkohya氏による拡張版Loraやけど元々の実装版のLoraもDreamBoothエクステンションとかで作れたりするから作った際にはごっちゃにせんようにな？
---拡張機能の方はkohya氏による拡張版Loraのみが対応や。本体機能の方は元々の実装版でも行けるんとちゃうかな？しらんけど。



