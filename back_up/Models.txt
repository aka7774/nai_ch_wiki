#contents

* モデル

広義では、HypernetworkやLoraで作成したファイルなども含まれるが、
ここでは1111の「Stable Diffusion checkpoint」で切り替えて使うやつの話をする。

** まとめ

- 生成に使うのは fp16, pruned, safetensors を満たすファイルが良い。

** Stable Diffusionのバージョン

*** v1系

- そのまま使える。

*** v2系

- 使うためには同名のyamlファイルを設置する必要がある。
- 大抵はモデルの配布元が配っている。
-- 無い場合は SD2.xのyamlを使ってみるとか。

** ハッシュ

[[Model hashとsha256の対応表はここ>sha256]]

- Model hashはファイルの一部のみを対象とするため衝突が頻発している
- プルリクは送られているが1111氏は対応しない意思がある
- 完全なファイルのsha256を取ることで衝突回避は可能
-- https://github.com/aka7774/sd_filer
-- https://github.com/aka7774/sd_infotext_ex
-- https://github.com/aka7774/sd_infotexts

** ファイル形式

拡張子で判別する。

*** .ckpt

- checkpointの略。
- チクポチではない。が、チクポチが出るかどうかはこいつ次第。
- ロード時にPythonのコードを実行できるため安全とは言い切れない
-- まれにウィルスソフトが誤検知を起こして騒ぎになっていたりする
- 狭義のモデルではないファイル(VAEとか)にもこの拡張子が使われることがあるので注意

*** .safetensors

- &#129303;提唱の形式
- ロード時にPythonのコードを実行できないので安全
- 読み込みも速くなる(はず)
- NMKDでは対応していない

** 精度

あくまで数値の精度であって、見た目の綺麗さや絵の細かさに直結するわけではない。

*** fp32(単精度浮動小数点数)

- floating pointの略。A1111系でfullとかいう言葉が出てきたら大抵これのこと。
- 1ビットの符号と8ビットの指数部と23ビットの仮数部
- おおよそのCPUやGPUで対応している

*** fp16(半精度浮動小数点数)

- floating pointの略。halfとかいう言葉が出てきたら大抵これのこと。
- 1ビットの符号と5ビットの指数部と10ビットの仮数部
- 処理が速い
- 容量が半分で済む
- IntelとNVIDIAのGPUで対応している

*** bf16(bfloat16)

- 1ビットの符号と8ビットの指数部と7ビットの仮数部
- fp32と指数部が一緒になる（ダイナミックレンジが広い代わりに仮数部が減ってるので小数点以下の表現力はfp16より少し落ちる）
- IntelとNVIDIAのGPUで対応している

** ema

- Exponential Moving Average(指数移動平均)の略
- モデルのトレーニング中に、トレーニングしたパラメータの移動平均を維持すると有益らしい([[TensorFlow API>https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage]])

*** emaあり

- モデルにトレーニング中のemaが付いている
- 学習用らしい。

*** emaなし(no-ema, pruned)

- 学習用データを取り除いて生成に特化させたものらしい。
- 容量が小さい。

* VAE

- 数が少ないので横着せずにドロップダウンから選択したほうがいい
-- Quicksetting listにsd_vaeを追加する
- モデルファイルに内蔵することが出来る
-- というか元々内蔵されているものを上書きするらしい

* モデル配布

** 配布方法

- [[HuggingFace]] Modelsがおすすめ

** 配布ファイル

- 生成用は、 fp16, pruned, safetensors, VAE内蔵 が理想的。
- 学習用は、 fp32, ema が理想的。
- ファイル名には日本語やスペースが無いほうがトラブルが減るかも。

** マージ方法

- Add Differenceを使う場合は注意が必要・・・
