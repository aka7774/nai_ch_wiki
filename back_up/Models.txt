#contents

* モデル

広義では、HypernetworkやLoraで作成したファイルなども含まれるが、
ここでは1111の「Stable Diffusion checkpoint」で切り替えて使うやつの話をする。

** まとめ

- 生成に使うのは fp16, pruned, safetensors を満たすファイルが良い。

** Stable Diffusionのバージョン

*** v1系

- そのまま使える。

*** v2系

- 使うためには同名のyamlファイルを設置する必要がある。
- 大抵はモデルの配布元が配っている。
-- 無い場合は SD2.xのyamlを使ってみるとか。

** インペイント専用モデル

inpaint.ckptは普通のckptとは違うらしい。

** ハッシュ

[[Model hashとsha256の対応表はここ>sha256]]

- Model hash(shorthash)はファイルの一部のみを対象とするため衝突が頻発している
- 2023-01-15の更新でとうとうModel hashが10桁の完全なsha256に更新された
-- 新旧の見分け方はたぶん桁数だけ
-- VAEとHypernetworkに対応していないのは実用上(まだ)問題が出ていないからだと思われる
- 完全なファイルのsha256を取ることで衝突回避は可能
-- https://github.com/aka7774/sd_filer
-- https://github.com/aka7774/sd_infotext_ex
-- https://github.com/aka7774/sd_infotexts
** ファイル形式

拡張子で判別する。

*** .ckpt

- checkpointの略。
- チクポチではない。が、チクポチが出るかどうかはこいつ次第。
- ロード時にPythonのコードを実行できるため安全とは言い切れない
-- まれにウィルスソフトが誤検知を起こして騒ぎになっていたりする
- 狭義のモデルではないファイル(VAEとか)にもこの拡張子が使われることがあるので注意

*** .safetensors

- &#129303;提唱の形式
- ロード時にPythonのコードを実行できないので安全
- 読み込みも速くなる(はず)
- NMKDでは対応していない

** 精度

あくまで数値の精度であって、見た目の綺麗さや絵の細かさに直結するわけではない。

*** fp32(単精度浮動小数点数)

- floating pointの略。A1111系でfullとかいう言葉が出てきたら大抵これのこと。
- 1ビットの符号と8ビットの指数部と23ビットの仮数部
- おおよそのCPUやGPUで対応している

*** fp16(半精度浮動小数点数)

- floating pointの略。halfとかいう言葉が出てきたら大抵これのこと。
- 1ビットの符号と5ビットの指数部と10ビットの仮数部
- 処理が速い
- 容量が半分で済む
- IntelとNVIDIAのGPUで対応している

*** bf16(bfloat16)

- 1ビットの符号と8ビットの指数部と7ビットの仮数部
- fp32と指数部が一緒になる（ダイナミックレンジが広い代わりに仮数部が減ってるので小数点以下の表現力はfp16より少し落ちる）
- IntelとNVIDIAのGPUで対応している

** ema

- Exponential Moving Average(指数移動平均)の略
- モデルのトレーニング中に、トレーニングしたパラメータの移動平均を維持すると有益らしい([[TensorFlow API>https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage]])

- どっちが学習向きなの？
- 容量は変わらんやつがあるけどなんなん？

*** emaあり

- モデルにトレーニング中のemaが付いている

*** emaなし(no-ema)

- 容量が小さくなるの？

** prune

- 生成に不要なデータを削除することで容量を削減すること
- ファイル名に pruned をつけて一緒に配布されていることがある
- 学習時には prune してないファイルがいいの？ 大差ないの？

* VAE

- 数が少ないので横着せずにドロップダウンから選択したほうがいい
-- Quicksetting listにsd_vaeを追加する
- モデルファイルに内蔵することが出来る
-- というか元々内蔵されているものを上書きするらしい

* モデル作成

モデルファイルを得られる方法

- Dreambooth
- マージ

** Dreambooth

- 狭義では、finetuneと追加学習とDreamboothは別の意味らしいけどようわからん。
- Extension版が正月のアップデート不具合と時代遅れによりツールは戦国時代に戻った。
- Loraもまたツールが分かれてバラバラになった。
- [[StableTuner]]

** マージ

標準Checkpoint Mergerにバグがあり、Add Differenceを使うとモデルが壊れていた。
n番目のトークンが無視されたり効果が弱まったりする。nは不定(1や76以外もありうる)
これは2023-01-15に修正された。(層別マージニキがプルリク送ってくれた)
今までに作られたマージモデルはほぼバグの影響を受けていると思われるので要修正。

詳しい説明と修正方法はこちら。
https://note.com/bbcmc/n/n12c05bf109cc

- [[Checkpoint Merger>ローカルのマージ]]
- [[層別マージ>階層マージ]]

* モデル配布

** 配布方法

- [[HuggingFace]] Modelsがおすすめ

** 配布ファイル

- 生成用は、 fp16, pruned, safetensors, VAE内蔵 が理想的。
- 学習用は、 fp32, ema が理想的。
- ファイル名には日本語やスペースが無いほうがトラブルが減るかも。

*** 作成方法

- StableTunerが最も理想に近そうだけど学習しないと出力できない？
- https://github.com/arenatemp/stable-diffusion-webui-model-toolkit
-- これが決定版かも(未確認)
- https://github.com/Akegarasu/sd-webui-model-converter
-- EMAが壊れてるらしい
- https://note.com/kohya_ss/n/nf5893a2e719c
-- 古くてsafetensors未対応だしうまく動かない

