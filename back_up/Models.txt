#contents

* モデル

広義では、HypernetworkやLoraで作成したファイルなども含まれるが、
ここではいわゆるCheckpointの話をする。

** まとめ

- 生成に使うのは fp16, pruned, safetensors を満たすファイルが良い。

** Stable Diffusionのバージョン

*** SD1/SDXL
- U-Netアーキテクチャ
- そのまま使える。

*** SD2
- U-Netアーキテクチャ
- 使うためには同名のyamlファイルを設置する必要がある。
- 大抵はモデルの配布元が配っている。
-- 無い場合は SD2.xのyamlを使ってみるとか。

*** SD3.5
- DiTアーキテクチャ
- Large(8.1B)とMedium(2.6B)の二種類がある
- 使うには、モデル本体に加えてCLIP L/G、T5XXL、SD3 VAEが必要

** ほかのText-to-Imageモデル
*** FLUX.1
- DiTアーキテクチャ
- DevとSchnellの二種類がある
- 蒸留モデル
- オープンなモデルの中では最大級のパラメータ数(12B)。
- それゆえに非常に重い

*** AuraFlow
- DiTアーキテクチャ
- 現在開発中
- 開発中のものが公開されている

*** Sana
- DiTアーキテクチャ
- 軽量なことが売り
- まだリリースされていない

** インペイント専用モデル

inpaint.ckptは普通のcheckpointとは違うらしい。

** ハッシュ

[[Model hashとsha256の対応表はここ>sha256]]

- Model hash(shorthash)はファイルの一部のみを対象とするため衝突が頻発している
- 2023-01-15の更新でとうとうModel hashが10桁の完全なsha256に更新された
-- 新旧の見分け方はたぶん桁数だけ
-- VAEとHypernetworkに対応していないのは実用上(まだ)問題が出ていないからだと思われる
- 完全なファイルのsha256を取ることで衝突回避は可能
-- https://github.com/aka7774/sd_filer
-- https://github.com/aka7774/sd_infotext_ex
-- https://github.com/aka7774/sd_infotexts
** ファイル形式

拡張子で判別する。

*** .ckpt

- checkpointの略。
- チクポチではない。が、チクポチが出るかどうかはこいつ次第。
- ロード時にPythonのコードを実行できるため安全とは言い切れない
-- まれにウィルスソフトが誤検知を起こして騒ぎになっていたりする
- 狭義のモデルではないファイル(VAEとか)にもこの拡張子が使われることがあるので注意

*** safetensors
- Hugging Face提唱の形式
- ロード時にPythonのコードを実行できないので安全
- 読み込みも速くなる(はず)
- NMKDでは対応していない

** 精度

あくまで数値の精度であって、見た目の綺麗さや絵の細かさに直結するわけではない。

*** fp32(単精度浮動小数点数)
- floating pointの略。A1111系でfullとかいう言葉が出てきたら大抵これのこと。コンピュータの世界ではfloatと呼ぶ場合はこれ。
- 1ビットの符号と8ビットの指数部と23ビットの仮数部
- ほぼすべてのCPUやGPUで対応している
- 高精度だが計算量が多い

*** fp16/float16(半精度浮動小数点数)
- floating pointの略。halfとかいう言葉が出てきたら大抵これのこと。
- 1ビットの符号と5ビットの指数部と10ビットの仮数部
- 処理が速い
- 容量が半分で済む
- IntelとNVIDIAのGPUで対応している(AMDはRDNA3以降で対応？)
- ダイナミックレンジが狭いため比較的NaN演算を起こしやすい

*** bf16/bfloat16
- 1ビットの符号と8ビットの指数部と7ビットの仮数部
- fp32と指数部が一緒になる（ダイナミックレンジが広い代わりに仮数部が減ってるので小数点以下の表現力はfp16より少し落ちる）
- IntelとNVIDIAのGPUで対応している(AMDはRDNA3以降で対応？)
- fp16よりNaN演算を起こしづらい

*** fp8(float8_e4m3fn)
- 1ビットの符号と4ビットの指数部と3ビットの仮数部(ほかにもある)
- fp16の半分のメモリ使用量
- だが精度も多少落ちる
- Torchが自動でキャストするので非対応のGPUでも動作する。
- NVIDIAのRTX4000以降であればそのまま動いて多少速くなる・・・はずがほとんどのソフトがfp8演算に非対応でfp16にアップキャストするのでむしろ遅くなる。

*** NormalFloat4(LinearNF4)
- 量子化をすることで精度低下を抑える？

[+] 古い情報
SDXLはpruned,fp16がデフォルトです。左の状態でSD1なら2.13GB(1.98GiB)、SDXLなら6.94GB(6.46GiB)前後になる。
** ema

- Exponential Moving Average(指数移動平均)の略
- モデルのトレーニング中に、トレーニングしたパラメータの移動平均を維持すると有益らしい([[TensorFlow API>https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage]])

- どっちが学習向きなの？
- 容量は変わらんやつがあるけどなんなん？
- LoRA学習ではなくてもいい

*** emaあり
- モデルにトレーニング中のemaが付いている

*** emaなし(no-ema)

- 容量が小さくなるの？

** prune

- 生成に不要なデータを削除することで容量を削減すること
- ファイル名に pruned をつけて一緒に配布されていることがある
- 学習時には prune してないファイルがいいの？ 大差ないの？
- LoRA学習ではprunedでもいい
[END]

* VAE

- 数が少ないので横着せずにドロップダウンから選択したほうがいい
-- Quicksetting listにsd_vaeを追加する
- モデルファイルに内蔵することが出来る
-- というか元々内蔵されているものを上書きするらしい

* モデル作成

モデルファイルを得られる方法

- Dreambooth
- マージ

** Dreambooth

- 狭義では、finetuneと追加学習とDreamboothは別の意味らしいけどようわからん。
- Extension版が正月のアップデート不具合と時代遅れによりツールは戦国時代に戻った。
- Loraもまたツールが分かれてバラバラになった。
- [[StableTuner]]

** マージ
- [[Checkpoint Merger>ローカルのマージ]]
- [[層別マージ>階層マージ]]

[+] 古い情報
標準Checkpoint Mergerにバグがあり、Add Differenceを使うとモデルが壊れていた。
n番目のトークンが無視されたり効果が弱まったりする。nは不定(1や76以外もありうる)
これは2023-01-15に修正された。(層別マージニキがプルリク送ってくれた)
今までに作られたマージモデルはほぼバグの影響を受けていると思われるので要修正。

詳しい説明と修正方法はこちら。
https://note.com/bbcmc/n/n12c05bf109cc
[END]

* モデル配布

** 配布方法

- [[HuggingFace]] Modelsがおすすめ

** 配布ファイル

- 生成用は、 fp16, pruned, safetensors, VAE内蔵 が理想的。
- 学習用は、 fp32, ema が理想的。
- ファイル名には日本語やスペースが無いほうがトラブルが減るかも。

*** 作成方法

- StableTunerが最も理想に近そうだけど学習しないと出力できない？
- https://github.com/arenatemp/stable-diffusion-webui-model-toolkit
-- これが決定版かも(未確認)
-- 更新停止。一部のVAE焼き込み済みのcheckpointを読み込めない。
- https://github.com/Akegarasu/sd-webui-model-converter
-- EMAが壊れてるらしい
- https://note.com/kohya_ss/n/nf5893a2e719c
-- 古くてsafetensors未対応だしうまく動かない

