#contents

* 画像生成モデル

広義では、HypernetworkやLoraで作成したファイルなども含まれるが、ここではいわゆるCheckpointの話をする。
2025年8月時点で依然としてSDXLが主流。次世代DiTモデルはFine-tuningが難しいとされる上、パラメータ数が増加して重くなるばかり。そのため、この状況が変わらない限り変化はないだろう。

** Stable Diffusionのバージョン
*** SD1
SD1.5: https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5
- U-Netアーキテクチャ
- 860M(8.6億)パラメータ
- 基礎解像度は512x512
- 軽量かつ古いため低性能
- 欠陥のあるVAE(アーティファクトが発生したり特殊なノイズに弱かったりする)
- 文章は全く生成できない

*** SD2
SD2.1: https://huggingface.co/stabilityai/stable-diffusion-2-1
- U-Netアーキテクチャ
- 865M(8.65億)パラメータ
- 基礎解像度は768x768
- SD1と大差無し
- 使うためにはconfigファイル(yaml)を設置する必要がある。
- 大抵はモデルの配布元が配っている。
-- 無い場合は SD2.xのyamlを使ってみるとか。

*** SDXL
SDXL Base 1.0: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
- U-Netアーキテクチャ
- 2.6B(26億)パラメータ
- BaseとRefinerの二種類が存在する(通常はBaseのみでok)
- 基礎解像度の1024x1024への変更とVAEの改良によりディティールが改善
-- 特殊なノイズの影響を受けにくい
- 文章は生成できない

*** SD3.0
https://huggingface.co/stabilityai/stable-diffusion-3-medium
- MMDiTアーキテクチャ
- Medium(2.5B)のみ
- 基礎解像度は1024x1024
- VAEの改良によりディティールが大幅改善(だが重い)
- 表現規制がきつく解剖がおかしくなる欠陥あり
- 文章を正しく生成できる
- NSFW禁止

*** SD3.5
Large: https://huggingface.co/stabilityai/stable-diffusion-3.5-large
Medium: https://huggingface.co/stabilityai/stable-diffusion-3.5-medium
- MMDiTアーキテクチャ
- Large(8.1B)とMedium(2.6B)の二種類がある
- 基礎解像度は1024x1024
- MediumはT5を除けばSDXLと同程度のメモリ使用量だが、計算はSDXLの二倍遅い。Largeはメモリ使用量、計算量ともに非常に多い。
- 使うには、モデル本体に加えてCLIP L/Gが必要。T5XXLは任意
- 規制の緩和とSD3.0の問題を解消
- NSFW禁止
- パラメータ効率が低い

** ほかのText-to-Imageモデル

*** FLUX.1
dev: https://huggingface.co/black-forest-labs/FLUX.1-dev
schnell: https://huggingface.co/black-forest-labs/FLUX.1-schnell
- DiTアーキテクチャ
- DevとSchnellの二種類がある
- 使うには、モデル本体に加えてCLIP LとT5XXLが必要
- 文章を正しく生成できる
- 蒸留モデル(Fine-tuningが困難らしい)
-- 故に品質はそこまで高くない？
- SD3と同様の16チャネルVAE

*** NovelAI Diffusion V4
- U-netアーキテクチャ
- Text EncoderはT5
- クローズソース
- アニメイラスト(Danbooru)生成に特化

*** HiDreami-I1
https://huggingface.co/HiDream-ai/HiDream-I1-Full
- Sparse DiTアーキテクチャ
- パラメータ数17Bの超巨大モデルでFLUX.1超えの品質と計算量
-- Block Swapを使わないときつい
- TEはCLIP-L/GとT5XXLに加えてLlama 3.1を使用する

*** Qwen-Image
https://huggingface.co/Qwen/Qwen-Image
- MMDiTアーキテクチャ
- パラメータ数はHiDreamをも上回る20Bですさまじい計算量
-- Block Swapを使わないときつい
- VAEのデコーダーはデュアル
- TEはQwen-VL
- 文字の描写が得意
-- 漢字もOK
- Apache 2.0 ライセンス

* predictionの種類
※間違っている可能性あり。間違っていたら修正してください。

ノイズ予測アルゴリズムの種類。
v-predictionだからといって劇的に性能が上がるわけではない。わかりやすい変化はZero Terminal SNRを併用することで色の精度が上がる程度。一方で構図の不安定化の原因となる。
なお、計算式の違いで推論時に本来の予測方法とは別のものに変えても正しく動作しない(学習時のpredictionタイプに合わせる)。
SD3以降のほとんどのモデルはv-predictionより高性能なFlow Matchingを使用する。

** ε-prediction / epsilon-prediction
SD1とSDXLのデフォルト
画像のノイズ部分を予測する。
SNR=0(純粋なノイズ)では機能しない。そのためSNR=0になるよう修正するZero Terminal SNRは使用できない。
*** epsilonの欠陥
学習時、画像にノイズを付与するが、不具合により完全なノイズになるべき状況でわずかに元画像が残るため、間違った学習をしてしまう。
結果、プロンプトを無視して中間的な明るさにしたり関係のないものを生成してしまうことがある。いわゆるハルシネーション(幻覚)？
これはNoise offsetやMultires noiseで緩和できる。

** v-prediction(v-pred)
SD2と一部のSDXLモデル(NovelAI Diffusion V3、NoobAI-XL)が使用
ノイズ除去前と除去後の差分を予測する。
SNR=0(純粋なノイズ)でも機能する。
''Zero Terminal SNRを併用することで''全体が明るい、暗い、または高コントラストの状況でグレー寄りになる問題を解消できる。単色背景やシルエットなどの表現が改善する。
これは実質Zero Terminal SNRのための技術であり、それがなければepsilonと変わらずv-predの利点もなくなる。

*** Zero Terminal SNR(ZTNSR/ZSNR)
学習時、SNR=0(完全なノイズ)であるべき状況で0にならない(ノイズの中にわずかに元画像が残る)欠陥を修正するもの。
全体が明るい、暗い、または高コントラストの状況でグレー寄りになる問題を解消できる。単色背景やシルエットなどの表現が改善する。
低めのGuidance Scaleで運用できる(5以下)。言い換えれば低めでないと彩度が過度に高くなる。
欠点として、彩度やコントラストが極端に高くなったり構図の破綻が増加したりする。
また、LoRA学習においては、不安定性が増大する(特に画風LoRA学習で相性が悪いと再現できずに品質が著しく低下する)。

ZTSNR有効のv-predictionモデルとepsilonモデルをマージするとZTSNRの効果が減少または消失し、「なんちゃってv-pred」になりやすいため注意。

*** よくある勘違い
v-predictionは''高速化する技術ではない''。低ステップ動作を謳うcheckpointは蒸留LoRAをマージしている。
v-predictionそのものは''明暗に強くする効果はない''。それはZero Terminal  SNRの効果。
VはVelocityの頭文字でありν(ニュー)ではない。


** v-predictionモデルの使い方
2024年11月27日時点の情報です。
*** AUTOMATIC1111
webuiのディレクトリ直下で次のコマンドを実行する。
 git checkout -b dev origin/dev 
あるいは、Github DesktopでFile->Add local repositoryで1111のフォルダを選択してリポジトリを追加した後、Current branchでorigin/devを選択する。
You have changed...と聞かれたらBring my changesを選択してswitch branchを押す。
あとはいつも通りwebuiを使用する。

*** Forge/ComfyUI
最新版にアプデする。

*** 1111/Forgeでノイズ製造機/真っ黒になる、またはmainブランチの1111で使う
次のリンクからsd_xl_v.yamlをwebui/models/Stable-diffusionにDLする。
https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/dev/configs/sd_xl_v.yaml
DLしたyamlファイルを使用するcheckpointと同じ名称に変更する。

必要に応じて、Settings>Sampler parametersにあるNoise schedule for sampling(sd_noise_schedule)をZero Terminal SNRにする。

* インペイント専用モデル

inpaint.ckptは普通のcheckpointとは違うらしい。

* ファイル形式

拡張子で判別する。

** .ckpt

- checkpointの略。
- チクポチではない。が、チクポチが出るかどうかはこいつ次第。
- ロード時にPythonのコードを実行できるため安全とは言い切れない
-- まれにウィルスソフトが誤検知を起こして騒ぎになっていたりする
- 狭義のモデルではないファイル(VAEとか)にもこの拡張子が使われることがあるので注意

** safetensors
- Hugging Face提唱の形式
- ロード時にPythonのコードを実行できないので安全
- 読み込みも速くなる(はず)

* 精度

あくまで数値の精度であって、見た目の綺麗さや絵の細かさに直結するわけではない。

** fp32(単精度浮動小数点数)
- floating pointの略。A1111系でfullとかいう言葉が出てきたら大抵これのこと。コンピュータの世界ではfloatと呼ぶ場合はこれ。
- 1ビットの符号と8ビットの指数部と23ビットの仮数部
- 現行のハードウェアはすべて対応
- 高精度だが計算量が多い
- 生成AIでは重い割にfp16とあまりかわらない

** fp16/float16(半精度浮動小数点数)
- 生成AIで主流の精度
- floating pointの略。halfとかいう言葉が出てきたら大抵これのこと。
- 1ビットの符号と5ビットの指数部と10ビットの仮数部
- 処理が速い
- 容量が半分で済む
- IntelとNVIDIAのGPUで対応している(AMDはRDNA3以降で対応？)
- ダイナミックレンジが狭いためオーバーフローによるNaN演算を起こしやすい
- Torch2.6以降であればCPUでも動作する

** bf16/bfloat16
- 1ビットの符号と8ビットの指数部と7ビットの仮数部
- fp32と指数部が一緒になる（ダイナミックレンジが広い代わりに仮数部が減ってるので小数点以下の表現力はfp16より少し落ちる）
- IntelとNVIDIAのGPUで対応している(AMDはRDNA3以降で対応？)
- fp16よりNaN演算を起こしづらい

** fp8(float8_e4m3fn)
- 1ビットの符号と4ビットの指数部と3ビットの仮数部(ほかにもある)
- fp16の半分のメモリ使用量
- だが精度も落ちる
- Torchが自動でキャストするので非対応のGPUでも動作する。
- NVIDIAのRTX40以降であればそのまま動いて速くなる(TensorRTが必要?)
-- しかしほとんどのソフトがfp8演算に非対応でfp16にアップキャストするので若干遅くなる。

** NormalFloat4(LinearNF4)
- 量子化をすることで精度低下を抑える？

** fp4
- NVIDIAのBlackwellアーキテクチャで利用可能
-- TensorRTが必要？
- fp8の半分、fp16の1/4の計算量
- 精度が低い
-- 4ビット量子化のほうがマシだとか

[+] 古い情報
SDXLはpruned,fp16がデフォルトです。左の状態でSD1なら2.13GB(1.98GiB)、SDXLなら6.94GB(6.46GiB)前後になる。
** ema(SD1)

- Exponential Moving Average(指数移動平均)の略
- モデルのトレーニング中に、トレーニングしたパラメータの移動平均を維持すると有益らしい([[TensorFlow API>https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage]])

- どっちが学習向きなの？
- 容量は変わらんやつがあるけどなんなん？
- LoRA学習ではなくてもいい

*** emaあり
- モデルにトレーニング中のemaが付いている

*** emaなし(no-ema)

- 容量が小さくなるの？

** prune(SD1)

- 生成に不要なデータを削除することで容量を削減すること
- ファイル名に pruned をつけて一緒に配布されていることがある
- 学習時には prune してないファイルがいいの？ 大差ないの？
- LoRA学習ではprunedでもいい
- SDXL以降は元から不要なデータが含まれていないので関係なし。
[END]

* VAE

- 数が少ないので横着せずにドロップダウンから選択したほうがいい
-- Quicksetting listにsd_vaeを追加する
- Checkpointに内蔵されているものを上書きする
- SDXL BaseのVAEはfp16ではオーバーフローによるNaN演算で動作しないので、sdxl-vae-fp16-fixを使う

* モデル作成

モデルファイルを得られる方法

- Full Fine-tuning
- DreamboothやLoRAのマージ
- checkpoint同士のマージ

** マージ
- [[Checkpoint Merger>ローカルのマージ]]
- [[層別マージ>階層マージ]]

[+] 古い情報(2022)
AUTOMATIC1111 WebUIの標準Checkpoint Mergerにバグがあり、Add Differenceを使うとモデルが壊れていた。
n番目のトークンが無視されたり効果が弱まったりする。nは不定(1や76以外もありうる)
これは2023-01-15に修正された。(層別マージニキがプルリク送ってくれた)
それ以前に作られたマージモデルはほぼバグの影響を受けていると思われるので要修正。

詳しい説明と修正方法はこちら。
https://note.com/bbcmc/n/n12c05bf109cc
[END]

* モデル配布

** 配布方法

- [[HuggingFace]] Modelsがおすすめ

** 配布ファイル
- 学習・推論共にfp16, pruned, safetensorsでよい。
- ファイル名には日本語やスペースが無いほうが文字化けやパスの区切りの誤判定によるトラブルが減る。



