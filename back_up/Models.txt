#contents

* 画像生成モデル

広義では、HypernetworkやLoraで作成したファイルなども含まれるが、ここではいわゆるCheckpointの話をする。
2025年8月時点で依然としてSDXLが主流。次世代DiTモデルはFine-tuningが難しいとされる上、パラメータ数が増加して重くなるばかり。そのため、この状況が変わらない限り変化はないだろう。

* Stable Diffusion
** SD1
SD1.5: https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5
- U-Netアーキテクチャ
- 860M(8.6億)パラメータ
- 解像度は512x512のみ
- 超軽量かつとても古いため低性能
- 欠陥のあるVAE(アーティファクトが発生したり特殊なノイズに弱かったりする)
- 文章は全く生成できない
- Text EncoderはCLIP-L
-- 最低限の性能でプロンプトの応答性が低い

** SD2
SD2.1: https://huggingface.co/stabilityai/stable-diffusion-2-1
- U-Netアーキテクチャ
- 865M(8.65億)パラメータ
- 基礎解像度は768x768
- ''SD1と大差無し''
- 使うためにはconfigファイル(yaml)を設置する必要がある。
- 大抵はモデルの配布元が配っている。
-- 無い場合は SD2.xのyamlを使ってみるとか。

** SDXL
SDXL Base 1.0: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
- U-Netアーキテクチャ
- 2.6B(26億)パラメータ(あるいは3.5B?)
- BaseとRefinerの二種類が存在する(通常はBaseのみでok)
- 基礎解像度の1024x1024への変更とVAEの改良によりディティールが改善
-- 特殊なノイズの影響を受けにくい
- Text EncoderはCLIP-LとCLIP-G
-- 平凡な性能で具体的な命令を無視する

** SD3.0
https://huggingface.co/stabilityai/stable-diffusion-3-medium
- MMDiT
- Medium(2.5B)のみ
- 基礎解像度は1024x1024
- 16チャネルVAE
-- ディティールが大きく改善
- Text EncoderはCLIP L/Gに加えてT5XXL(任意)
-- 位置や文字の描写などの具体的な命令もOK
- 表現規制がきつく解剖がおかしくなる欠陥あり
- 実質NSFW禁止

** SD3.5
Large: https://huggingface.co/stabilityai/stable-diffusion-3.5-large
Medium: https://huggingface.co/stabilityai/stable-diffusion-3.5-medium
- MMDiT
- Large(8.1B)とMedium(2.6B)の二種類がある
- 基礎解像度は1024x1024
- MediumはT5を除けばSDXLと同程度のメモリ使用量だが、計算はSDXLの二倍遅い。Largeはメモリ使用量、計算量ともに非常に多い。
- Text EncoderはCLIP L/Gに加えてT5XXL(任意)
-- 位置や文字の描写などの具体的な命令もOK
- 規制の緩和とSD3.0の問題を解消
-- でもライセンスにより実質NSFW禁止
- パラメータ効率が低い

* ほかのText-to-Imageモデル

** FLUX.1
dev: https://huggingface.co/black-forest-labs/FLUX.1-dev
schnell: https://huggingface.co/black-forest-labs/FLUX.1-schnell
- MMDiT
- パラメータ数は12Bで重い
- DevとSchnellの二種類がある
- Text EncoderはCLIP LとT5XXL
-- 位置や文字の描写などの具体的な命令もOK
- 蒸留モデル(Fine-tuningが困難らしい)
-- 故に品質はそこまで高くない？
- 16チャネルVAE

** FLUX.2
dev: https://huggingface.co/black-forest-labs/FLUX.2-dev
- APIのみのpro,flexとオープンなdev,kleinの四種類ある(kleinは近日公開)
- MMDiT
- パラメータ数は32Bで素の状態ではほとんどの一般消費者向けハードウェアで動作困難
-- 量子化やBlock Swapを使ってもなおメモリ消費が非常に多い
- ベンチマークスコアは高い(なおメモリ消費量)

** NovelAI Diffusion V4
- U-net
- Text EncoderはT5
- クローズソース
- アニメイラスト(Danbooru)生成に特化

** Qwen-Image
https://huggingface.co/Qwen/Qwen-Image
- MMDiT
- パラメータ数は20Bですさまじいメモリ使用量と計算量
-- ハイエンドGPUでもBlock Swapを使わないときつい
- VAEのデコーダーはデュアル
- TEはQwen-VL
-- 位置や文字の描写などの具体的な命令もOK
-- 漢字を含む文字の描写が得意
- Apache 2.0 ライセンス

** Z-Image
https://huggingface.co/Tongyi-MAI/Z-Image-Turbo
- MMDiT
- パラメータ数6Bでやや重い
- パラメータ数のわりに高品質
- TEはパラメータ効率のいいQwen3-4B
-- 位置や文字の描写などの具体的な命令もOKで応答性が高い
- TurboはDMDRで蒸留した収束が速いモデル

** Lumina-Image
https://huggingface.co/Alpha-VLLM/Lumina-Image-2.0
- MMDiT
- パラメータ数はSDXLより小さい2BでSDXL程度のメモリ使用量
-- でもSDXLより4倍遅い
- Text EncoderにはGemma 2 2Bを採用
-- 位置や文字の描写などの具体的な命令もOK(大型モデルほど得意ではない)
- SDXLよりは良いがパラメータ数が圧倒的に大きいSD3.5 Large(8.1B)やQwen-Image(20B)には及ばない
- Apache 2.0ライセンス
- 16チャネルVAE

*** NetaYume Lumina
https://huggingface.co/duongve/NetaYume-Lumina-Image-2.0
- Lumina-Image 2をFine-tuningしたDanbooruタグと自然言語(日英中)対応のアニメモデル
- 現状完成されたSDXLベースのアニメモデルのほうがいい。
-- 時々非常に美しい絵が出ることから可能性を感じる。かつてのSDXLのように、さらなるFine-tuningで改善の余地あり
- 絵師タグが無いとベースモデルらしく不安定
- 細部、色、ライティングはSDXLより明確に良い
- NSFWになったとたん品質が低下する傾向がある

* predictionの種類
※間違っている可能性あり。間違っていたら修正してください。

ノイズ予測アルゴリズムの種類。
v-predictionだからといって劇的に性能が上がるわけではない。わかりやすい変化はZero Terminal SNRを併用することで色の精度が上がる程度。一方で構図の不安定化の原因となる。
なお、計算式の違いで推論時に本来の予測方法とは別のものに変えても正しく動作しない(学習時のpredictionタイプに合わせる)。
SD3以降のほとんどのモデルはv-predictionより高性能なFlow Matchingを使用する。

** ε-prediction / epsilon-prediction
SD1とSDXLのデフォルト
画像のノイズ部分を予測する。
SNR=0(純粋なノイズ)では機能しない。そのためSNR=0になるよう修正するZero Terminal SNRは使用できない。
*** epsilonの欠陥
学習時、画像にノイズを付与するが、不具合により完全なノイズになるべき状況でわずかに元画像が残るため、間違った学習をしてしまう。
結果、プロンプトを無視して中間的な明るさにしたり関係のないものを生成してしまうことがある。いわゆるハルシネーション(幻覚)？
これはNoise offsetやMultires noiseで緩和できる。

** v-prediction(v-pred)
SD2と一部のSDXLモデル(NovelAI Diffusion V3、NoobAI-XL)が使用
ノイズ除去前と除去後の差分を予測する。
SNR=0(純粋なノイズ)でも機能する。
''Zero Terminal SNRを併用することで''全体が明るい、暗い、または高コントラストの状況でグレー寄りになる問題を解消できる。単色背景やシルエットなどの表現が改善する。
これは実質Zero Terminal SNRのための技術であり、それがなければepsilonと変わらずv-predの利点もなくなる。

*** Zero Terminal SNR(ZTNSR/ZSNR)
学習時、SNR=0(完全なノイズ)であるべき状況で0にならない(ノイズの中にわずかに元画像が残る)欠陥を修正するもの。
全体が明るい、暗い、または高コントラストの状況でグレー寄りになる問題を解消できる。単色背景やシルエットなどの表現が改善する。
低めのGuidance Scaleで運用できる(5以下)。言い換えれば低めでないと彩度が過度に高くなる。
欠点として、彩度やコントラストが極端に高くなったり構図の破綻が増加したりする。
また、LoRA学習においては、不安定性が増大する(特に画風LoRA学習で相性が悪いと再現できずに品質が著しく低下する)。

ZTSNR有効のv-predictionモデルとepsilonモデルをマージするとZTSNRの効果が減少または消失し、「なんちゃってv-pred」になりやすいため注意。

*** よくある勘違い
v-predictionは''高速化する技術ではない''。低ステップ動作を謳うcheckpointは蒸留LoRAをマージしている。
v-predictionそのものは''明暗に強くする効果はない''。それはZero Terminal  SNRの効果。
VはVelocityの頭文字でありν(ニュー)ではない。


** v-predictionモデルの使い方
2024年11月27日時点の情報です。
*** AUTOMATIC1111
webuiのディレクトリ直下で次のコマンドを実行する。
 git checkout -b dev origin/dev 
あるいは、Github DesktopでFile->Add local repositoryで1111のフォルダを選択してリポジトリを追加した後、Current branchでorigin/devを選択する。
You have changed...と聞かれたらBring my changesを選択してswitch branchを押す。
あとはいつも通りwebuiを使用する。

*** Forge/ComfyUI
最新版にアプデする。

*** 1111/Forgeでノイズ製造機/真っ黒になる、またはmainブランチの1111で使う
次のリンクからsd_xl_v.yamlをwebui/models/Stable-diffusionにDLする。
https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/dev/configs/sd_xl_v.yaml
DLしたyamlファイルを使用するcheckpointと同じ名称に変更する。

必要に応じて、Settings>Sampler parametersにあるNoise schedule for sampling(sd_noise_schedule)をZero Terminal SNRにする。

* インペイント専用モデル

inpaint.ckptは普通のcheckpointとは違うらしい。

* ファイル形式

拡張子で判別する。

** .ckpt

- checkpointの略。
- チクポチではない。が、チクポチが出るかどうかはこいつ次第。
- ロード時にPythonのコードを実行できるため安全とは言い切れない
-- まれにウィルスソフトが誤検知を起こして騒ぎになっていたりする
- 狭義のモデルではないファイル(VAEとか)にもこの拡張子が使われることがあるので注意

** safetensors
- Hugging Face提唱の形式
- ロード時にPythonのコードを実行できないので安全
- 読み込みも速くなる(はず)

* 精度

あくまで数値の精度であって、見た目の綺麗さや絵の細かさに直結するわけではない。

** fp32(単精度浮動小数点数)
- floating pointの略。A1111系でfullとかいう言葉が出てきたら大抵これのこと。コンピュータの世界ではfloatと呼ぶ場合はこれ。
- 1ビットの符号と8ビットの指数部と23ビットの仮数部
- 現行のハードウェアはすべて対応
- 高精度だが計算量が多い
- 生成AIでは重い割にfp16とあまりかわらない

** fp16/float16(半精度浮動小数点数)
- 生成AIで主流の精度
- floating pointの略。halfとかいう言葉が出てきたら大抵これのこと。
- 1ビットの符号と5ビットの指数部と10ビットの仮数部
- 処理が速い
- 容量が半分で済む
- IntelとNVIDIAのGPUで対応している(AMDはRDNA3以降で対応？)
- ダイナミックレンジが狭いためオーバーフローによるNaN演算を起こしやすい
- Torch2.6以降であればCPUでも動作する

** bf16/bfloat16
- 1ビットの符号と8ビットの指数部と7ビットの仮数部
- fp32と指数部が一緒になる（ダイナミックレンジが広い代わりに仮数部が減ってるので小数点以下の表現力はfp16より少し落ちる）
- IntelとNVIDIA(Ampere以降)のGPUで対応している(AMDはRDNA3以降で対応？)

** fp8(float8_e4m3fn)
- 1ビットの符号と4ビットの指数部と3ビットの仮数部(ほかにもある)
- fp16の半分のメモリ使用量
- だが精度も落ちる
-- fp8 scaledならfp16相当の精度
- Torchが自動でキャストするので非対応のGPUでも動作する。
- NVIDIAのRTX40以降であればそのまま動いて速くなる(TensorRTが必要?)
-- しかしほとんどのソフトがfp8演算に非対応でfp16にアップキャストするので若干遅くなる。

** NormalFloat4(LinearNF4)
- 量子化をすることで精度低下を抑える？

** fp4
- NVIDIAのBlackwellアーキテクチャで利用可能
-- TensorRTが必要？
- fp8の半分、fp16の1/4の計算量
- 精度が低い
-- 4ビット量子化のほうがマシだとか

[+] 古い情報
SDXLはpruned,fp16がデフォルトです。左の状態でSD1なら2.13GB(1.98GiB)、SDXLなら6.94GB(6.46GiB)前後になる。
** ema(SD1)

- Exponential Moving Average(指数移動平均)の略
- モデルのトレーニング中に、トレーニングしたパラメータの移動平均を維持すると有益らしい([[TensorFlow API>https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage]])

- どっちが学習向きなの？
- 容量は変わらんやつがあるけどなんなん？
- LoRA学習ではなくてもいい

*** emaあり
- モデルにトレーニング中のemaが付いている

*** emaなし(no-ema)

- 容量が小さくなるの？

** prune(SD1)

- 生成に不要なデータを削除することで容量を削減すること
- ファイル名に pruned をつけて一緒に配布されていることがある
- 学習時には prune してないファイルがいいの？ 大差ないの？
- LoRA学習ではprunedでもいい
- SDXL以降は元から不要なデータが含まれていないので関係なし。
[END]

* VAE

- 数が少ないので横着せずにドロップダウンから選択したほうがいい
-- Quicksetting listにsd_vaeを追加する
- Checkpointに内蔵されているものを上書きする
- SDXL BaseのVAEはfp16ではオーバーフローによるNaN演算で動作しないので、sdxl-vae-fp16-fixを使う

* モデル作成

モデルファイルを得られる方法

- Full Fine-tuning
- DreamboothやLoRAのマージ
- checkpoint同士のマージ

** マージ
- [[Checkpoint Merger>ローカルのマージ]]
- [[層別マージ>階層マージ]]

[+] 古い情報(2022)
AUTOMATIC1111 WebUIの標準Checkpoint Mergerにバグがあり、Add Differenceを使うとモデルが壊れていた。
n番目のトークンが無視されたり効果が弱まったりする。nは不定(1や76以外もありうる)
これは2023-01-15に修正された。(層別マージニキがプルリク送ってくれた)
それ以前に作られたマージモデルはほぼバグの影響を受けていると思われるので要修正。

詳しい説明と修正方法はこちら。
https://note.com/bbcmc/n/n12c05bf109cc
[END]

* モデル配布

** 配布方法

- [[HuggingFace]] Modelsがおすすめ

** 配布ファイル
- 学習・推論共にfp16, pruned, safetensorsでよい。
- ファイル名には日本語やスペースが無いほうが文字化けやパスの区切りの誤判定によるトラブルが減る。



